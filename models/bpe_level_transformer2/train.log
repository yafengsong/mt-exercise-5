2023-06-03 01:24:40,168 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -                           cfg.name : bpe_level_transformer2
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -                     cfg.data.train : data/train
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -                      cfg.data.test : data/test
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -                  cfg.data.src.lang : it
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : shared_models/joint-vocab.txt
2023-06-03 01:24:40,169 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 3000
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data/bpe.codes.3000
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : shared_models/joint-vocab.txt
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 3000
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data/bpe.codes.3000
2023-06-03 01:24:40,170 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2023-06-03 01:24:40,171 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_level_transformer2
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -              cfg.training.use_cuda : False
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2023-06-03 01:24:40,172 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2023-06-03 01:24:40,173 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2023-06-03 01:24:40,174 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2023-06-03 01:24:40,174 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2023-06-03 01:24:40,174 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2023-06-03 01:24:40,174 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2023-06-03 01:24:40,175 - INFO - joeynmt.data - Building tokenizer...
2023-06-03 01:24:40,185 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2023-06-03 01:24:40,185 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2023-06-03 01:24:40,185 - INFO - joeynmt.data - Loading train set...
2023-06-03 01:24:40,357 - INFO - joeynmt.data - Building vocabulary...
2023-06-03 01:24:40,583 - INFO - joeynmt.data - Loading dev set...
2023-06-03 01:24:40,584 - INFO - joeynmt.data - Loading test set...
2023-06-03 01:24:40,587 - INFO - joeynmt.data - Data loaded.
2023-06-03 01:24:40,587 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2023-06-03 01:24:40,587 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2023-06-03 01:24:40,587 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2023-06-03 01:24:40,587 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ or@@ e: ar@@ re@@ st@@ i@@ amo i@@ l r@@ i@@ sc@@ al@@ d@@ amento g@@ lob@@ ale
	[TRG] A@@ l G@@ or@@ e: A@@ ver@@ ting the clim@@ ate cri@@ si@@ s
2023-06-03 01:24:40,587 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) of (7) a (8) and (9) in
2023-06-03 01:24:40,587 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) of (7) a (8) and (9) in
2023-06-03 01:24:40,587 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4552
2023-06-03 01:24:40,587 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4552
2023-06-03 01:24:40,589 - INFO - joeynmt.model - Building an encoder-decoder model...
2023-06-03 01:24:40,654 - INFO - joeynmt.model - Enc-dec model built.
2023-06-03 01:24:40,657 - INFO - joeynmt.model - Total params: 4064512
2023-06-03 01:24:40,657 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2023-06-03 01:24:40,658 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4552),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4552),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2023-06-03 01:24:40,658 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2023-06-03 01:24:40,659 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2023-06-03 01:24:40,659 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2023-06-03 01:24:40,659 - INFO - joeynmt.training - EPOCH 1
2023-06-03 01:25:20,906 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.315606, Batch Acc: 0.043246, Tokens per Sec:     1775, Lr: 0.000300
2023-06-03 01:26:01,816 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     4.131802, Batch Acc: 0.062918, Tokens per Sec:     1673, Lr: 0.000300
2023-06-03 01:26:45,679 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.791128, Batch Acc: 0.079203, Tokens per Sec:     1587, Lr: 0.000300
2023-06-03 01:27:28,676 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.792711, Batch Acc: 0.085450, Tokens per Sec:     1611, Lr: 0.000300
2023-06-03 01:28:13,525 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.701557, Batch Acc: 0.088087, Tokens per Sec:     1560, Lr: 0.000300
2023-06-03 01:28:13,525 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 01:34:05,453 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.80, ppl:  44.73, acc:   0.08, generation: 351.8720[sec], evaluation: 0.0000[sec]
2023-06-03 01:34:05,454 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 01:34:05,770 - INFO - joeynmt.training - Example #0
2023-06-03 01:34:05,771 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 01:34:05,771 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 01:34:05,771 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'ut', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2023-06-03 01:34:05,771 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 01:34:05,771 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 01:34:05,771 - INFO - joeynmt.training - 	Hypothesis: And I ut the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2023-06-03 01:34:05,771 - INFO - joeynmt.training - Example #1
2023-06-03 01:34:05,771 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 01:34:05,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 01:34:05,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'ut', 'you', 'ut', 'I', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'have', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 're@@', 'm', 'the', 'the', 'the', 'the', 'the', 'the', 're@@', 'm', 'you', 'ut', 'you', 'ut']
2023-06-03 01:34:05,772 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 01:34:05,772 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 01:34:05,772 - INFO - joeynmt.training - 	Hypothesis: And I ut you ut I ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you have the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the rem the the the the the the rem you ut you ut
2023-06-03 01:34:05,772 - INFO - joeynmt.training - Example #2
2023-06-03 01:34:05,772 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 01:34:05,772 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 01:34:05,772 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'you', 'ut', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2023-06-03 01:34:05,773 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 01:34:05,773 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 01:34:05,773 - INFO - joeynmt.training - 	Hypothesis: And you ut the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2023-06-03 01:34:05,773 - INFO - joeynmt.training - Example #3
2023-06-03 01:34:05,773 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 01:34:05,773 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 01:34:05,773 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 're@@', 'm', 'to', 'have', 'r@@', '.', '</s>']
2023-06-03 01:34:05,774 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 01:34:05,774 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 01:34:05,774 - INFO - joeynmt.training - 	Hypothesis: And you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut a a a a a a a a a a a a a a a a a a a a a a the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the rem to have r.
2023-06-03 01:34:05,774 - INFO - joeynmt.training - Example #4
2023-06-03 01:34:05,774 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 01:34:05,774 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 01:34:05,774 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'you', 'ut', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']
2023-06-03 01:34:05,775 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 01:34:05,775 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 01:34:05,775 - INFO - joeynmt.training - 	Hypothesis: And you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut you ut the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the
2023-06-03 01:34:55,240 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.651640, Batch Acc: 0.093037, Tokens per Sec:     1442, Lr: 0.000300
2023-06-03 01:35:44,360 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.729570, Batch Acc: 0.098778, Tokens per Sec:     1432, Lr: 0.000300
2023-06-03 01:36:34,597 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.554649, Batch Acc: 0.101667, Tokens per Sec:     1393, Lr: 0.000300
2023-06-03 01:37:24,993 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.518976, Batch Acc: 0.108044, Tokens per Sec:     1405, Lr: 0.000300
2023-06-03 01:38:12,463 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.638800, Batch Acc: 0.115876, Tokens per Sec:     1454, Lr: 0.000300
2023-06-03 01:38:12,464 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 01:43:48,913 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.62, ppl:  37.33, acc:   0.12, generation: 336.4007[sec], evaluation: 0.0000[sec]
2023-06-03 01:43:48,914 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 01:43:49,158 - INFO - joeynmt.training - Example #0
2023-06-03 01:43:49,158 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 01:43:49,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 01:43:49,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'can', 'be', 'a', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'world', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'same', 'of', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'and', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'little', 'of', 'the', 'world', 'and', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'little', 'of', 'the', 'world', 'of', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'same', 'le', 'of', 'the', 'world', 'and', 'the', 'first', 'very', 'little', 'f@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'p', 'the', 'cause', 'I']
2023-06-03 01:43:49,159 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 01:43:49,159 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 01:43:49,159 - INFO - joeynmt.training - 	Hypothesis: And I can be a little little little little little little little little little little little little little little little of the same of the same of the world of the same of the same of the same of the same of the world of the world of the world of the world of the world of the same of the world of the world of the world and the same of the same of the little of the world and the world of the world of the world and the world and the little of the world of the world and the world and the same le of the world and the first very little firirip the cause I
2023-06-03 01:43:49,159 - INFO - joeynmt.training - Example #1
2023-06-03 01:43:49,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 01:43:49,160 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 01:43:49,160 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'can', 'be', 'a', 'little', 'of', 'the', 'same', 'of', 'the', 'same', 'little', 'of', 'the', 'same', 'little', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'little', 'of', 'the', 'little', 'of', 'the', 'same', 'of', 'the', 'same', 'of', 'the', 'little', 'of', 'the', 'same', 'of', 'the', 'little', 'of', 'the', 'same', 'r@@', '.', '</s>']
2023-06-03 01:43:49,160 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 01:43:49,160 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 01:43:49,160 - INFO - joeynmt.training - 	Hypothesis: And I can be a little of the same of the same little of the same little of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the same of the little of the little of the same of the same of the little of the same of the little of the same r.
2023-06-03 01:43:49,160 - INFO - joeynmt.training - Example #2
2023-06-03 01:43:49,161 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 01:43:49,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 01:43:49,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'was', 'is', 'a', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 's@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'i@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'r@@', 'i@@', 'r@@', '.', '</s>']
2023-06-03 01:43:49,161 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 01:43:49,161 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 01:43:49,161 - INFO - joeynmt.training - 	Hypothesis: And I was is a little little little little little little little little little little little little little little little little little little little little little little little little little little little siririririririririririririririririririririiiriririririririiiiiiiirir.
2023-06-03 01:43:49,161 - INFO - joeynmt.training - Example #3
2023-06-03 01:43:49,161 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 01:43:49,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 01:43:49,162 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'was', 'a', 'op@@', 'le', 'of', 'the', 'is', 'a', 'op@@', 'le', 'of', 'the', 'is', 'a', 'op@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', '.', '</s>']
2023-06-03 01:43:49,162 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 01:43:49,162 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 01:43:49,162 - INFO - joeynmt.training - 	Hypothesis: And I was a ople of the is a ople of the is a opririririririririririr.
2023-06-03 01:43:49,162 - INFO - joeynmt.training - Example #4
2023-06-03 01:43:49,162 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 01:43:49,162 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 01:43:49,163 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'can', 'be', 'a', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'd@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'i@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'i@@', 'i@@', 'r@@', 'i@@', 'r@@', 'i@@', 'r@@']
2023-06-03 01:43:49,163 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 01:43:49,163 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 01:43:49,163 - INFO - joeynmt.training - 	Hypothesis: And I can be a little little little little little little little little little little little little little little little little little little diririririririririririririririririririririririririririririririririiiiiiiiiririiiririririririririiiririr
2023-06-03 01:44:35,316 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.619807, Batch Acc: 0.118052, Tokens per Sec:     1478, Lr: 0.000300
2023-06-03 01:45:21,216 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.524360, Batch Acc: 0.126551, Tokens per Sec:     1540, Lr: 0.000300
2023-06-03 01:46:07,076 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.520404, Batch Acc: 0.134211, Tokens per Sec:     1535, Lr: 0.000300
2023-06-03 01:46:51,624 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.333826, Batch Acc: 0.139681, Tokens per Sec:     1602, Lr: 0.000300
2023-06-03 01:47:37,722 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     3.390507, Batch Acc: 0.145026, Tokens per Sec:     1530, Lr: 0.000300
2023-06-03 01:47:37,722 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 01:52:56,325 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.45, ppl:  31.50, acc:   0.15, generation: 318.5657[sec], evaluation: 0.0000[sec]
2023-06-03 01:52:56,331 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 01:52:56,554 - INFO - joeynmt.training - Example #0
2023-06-03 01:52:56,554 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 01:52:56,554 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 01:52:56,554 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'have', 'a', 'lot', 'of', 'the', 'first', 'is', 'a', 'lot', 'of', 'the', 'first', 're@@', 'ally', 'to', 'the', 'first', 'to', 'the', 'first', 're@@', 'ally', 'a', 'lot', 'of', 'the', 'first', 'to', 'the', 'first', 're@@', 'ally', 'a', 'little', 're@@', 'ally', 'a', 'little', 'lot', 'of', 'the', 'first', 're@@', 'ally', 're@@', 'ally', 'a', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'h@@', 'i@@', 'i@@', 'i@@', 'i@@', 'd@@', 'i@@', 'y@@', ',', '</s>']
2023-06-03 01:52:56,555 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 01:52:56,555 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 01:52:56,555 - INFO - joeynmt.training - 	Hypothesis: And I have a lot of the first is a lot of the first really to the first to the first really a lot of the first to the first really a little really a little lot of the first really really a very very very very very very very very very very very very very very very very very very very very very very very hiiiidiy,
2023-06-03 01:52:56,555 - INFO - joeynmt.training - Example #1
2023-06-03 01:52:56,555 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 01:52:56,555 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 01:52:56,555 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'can', 'be', 'a', 'lot', 'of', 'the', 'first', 'is', 'a', 'lot', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'is', 'a', 'lot', 'of', 'the', 'first', 'is', 'a', 'lot', 'of', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'is', 'a', 'lot', 'of', 'the', 'first', 'of', 'the', 'h@@', '.', '</s>']
2023-06-03 01:52:56,556 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 01:52:56,556 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 01:52:56,556 - INFO - joeynmt.training - 	Hypothesis: And I can be a lot of the first is a lot of the first of the first of the first is a lot of the first is a lot of the first of the first of the first is a lot of the first of the h.
2023-06-03 01:52:56,556 - INFO - joeynmt.training - Example #2
2023-06-03 01:52:56,556 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 01:52:56,556 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 01:52:56,556 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'can', 'be', 'a', 'lot', 'of', 'the', 'first', 're@@', 'ally', 're@@', 'ally', 'a', 'lot', 'of', 'the', 'first', 're@@', 'ally', 'a', 'lot', 'of', 'the', 'first', 're@@', 'ally', 'a', 'lot', 'of', 'the', 'first', 're@@', 'ally', 're@@', 'ally', 'a', 'lot', 'of', 'the', 'first', 're@@', 'ally', 're@@', 'ally', 'a', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 're@@', 'ally', 'of', 'the', 'same', 're@@', 'ally', 'of', 'the', 'h@@', 'i@@', 'd@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@']
2023-06-03 01:52:56,557 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 01:52:56,557 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 01:52:56,557 - INFO - joeynmt.training - 	Hypothesis: And I can be a lot of the first really really a lot of the first really a lot of the first really a lot of the first really really a lot of the first really really a very very very very very very very very very very very very very very very very very very very very very very very very very very very really of the same really of the hidiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii
2023-06-03 01:52:56,557 - INFO - joeynmt.training - Example #3
2023-06-03 01:52:56,557 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 01:52:56,557 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 01:52:56,557 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'first', 'is', 'a', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'h@@', '.', '</s>']
2023-06-03 01:52:56,557 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 01:52:56,557 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 01:52:56,557 - INFO - joeynmt.training - 	Hypothesis: And the first is a little little little little little little little little h.
2023-06-03 01:52:56,558 - INFO - joeynmt.training - Example #4
2023-06-03 01:52:56,558 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 01:52:56,558 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 01:52:56,558 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'first', 'lot', 'of', 'the', 'first', 'of', 'the', 'first', 'lot', 'of', 'the', 'first', 'of', 'the', 'first', 'lot', 'of', 'the', 'first', 'lot', 'of', 'the', 'first', 'lot', 'of', 'the', 'first', 're@@', 'ally', 're@@', 'ally', 'a', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'h@@', '.', '</s>']
2023-06-03 01:52:56,558 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 01:52:56,558 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 01:52:56,558 - INFO - joeynmt.training - 	Hypothesis: And the first lot of the first of the first lot of the first of the first lot of the first lot of the first lot of the first really really a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very h.
2023-06-03 01:53:42,284 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     3.274542, Batch Acc: 0.151339, Tokens per Sec:     1526, Lr: 0.000300
2023-06-03 01:54:28,448 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     3.371555, Batch Acc: 0.160126, Tokens per Sec:     1517, Lr: 0.000300
2023-06-03 01:55:14,758 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     3.294580, Batch Acc: 0.169472, Tokens per Sec:     1513, Lr: 0.000300
2023-06-03 01:56:00,975 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     3.215129, Batch Acc: 0.181164, Tokens per Sec:     1528, Lr: 0.000300
2023-06-03 01:56:45,114 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     3.170888, Batch Acc: 0.192467, Tokens per Sec:     1584, Lr: 0.000300
2023-06-03 01:56:45,114 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 02:01:53,151 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.21, ppl:  24.87, acc:   0.19, generation: 307.8987[sec], evaluation: 0.0000[sec]
2023-06-03 02:01:53,152 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 02:01:53,382 - INFO - joeynmt.training - Example #0
2023-06-03 02:01:53,382 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 02:01:53,382 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 02:01:53,382 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'that', 'the', 'first', 'thing', 'that', 'are', 'the', 'first', 'thing', 'that', 'are', 'the', 'other', 'pe@@', 'op@@', 'le', 'and', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'and', 'the', 'world', 'of', 'the', 'world', 'of', 'the', 'world', 'and', 'the', 'world', 'of', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'other', 'pe@@', 'op@@', 'le', 'of', 'the', 'other', 'pe@@', 'op@@', 'le', 'of', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'other', 'pe@@', 'op@@', 'le', 'and', 'the', 'other', 'pe@@', 'op@@', 'le', 'of', 'the', 'other', 'pe@@', 'op@@', 'le', 'of', 'the', 're@@', 'ally', 'have', 'to', 'be', 'the', 're@@', 'ally', 're@@', 'ally', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are', 'are']
2023-06-03 02:01:53,383 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 02:01:53,383 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 02:01:53,383 - INFO - joeynmt.training - 	Hypothesis: The first thing that the first thing that are the first thing that are the other people and the world of the world of the world and the world and the world and the world and the world of the world of the world and the world of the world of the world and the world of the world and the world and the world and the other people of the other people of the world and the world and the other people and the other people of the other people of the really have to be the really really are are are are are are are are are are are
2023-06-03 02:01:53,383 - INFO - joeynmt.training - Example #1
2023-06-03 02:01:53,383 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 02:01:53,383 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 02:01:53,383 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'that', 'the', 'first', 'thing', 'is', 'the', 'first', 'thing', 'is', 'the', 'first', 'thing', 'that', 'is', 'the', 'world', 'and', 'the', 'world', 'and', 'the', 'other', 'pe@@', 'op@@', 'le', 'and', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', '</s>']
2023-06-03 02:01:53,384 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 02:01:53,384 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 02:01:53,384 - INFO - joeynmt.training - 	Hypothesis: The first thing that the first thing is the first thing is the first thing that is the world and the world and the other people and the really really really really really really really really really really really really really really really really really really really really really really really really really really really really really
2023-06-03 02:01:53,384 - INFO - joeynmt.training - Example #2
2023-06-03 02:01:53,384 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 02:01:53,384 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 02:01:53,384 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'is', 'the', 'first', 'thing', 'is', 'the', 'first', 'thing', 'is', 'the', 'first', 'thing', 'that', 'is', 'the', 'first', 'thing', 'is', 'the', 'same', 'pe@@', 'op@@', 'le', 'and', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'to', 'be', 'to', 'be', 'the', 'world', '</s>']
2023-06-03 02:01:53,385 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 02:01:53,385 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 02:01:53,385 - INFO - joeynmt.training - 	Hypothesis: The first thing is the first thing is the first thing is the first thing that is the first thing is the same people and the really really really really really really to be to be the world
2023-06-03 02:01:53,385 - INFO - joeynmt.training - Example #3
2023-06-03 02:01:53,385 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 02:01:53,385 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 02:01:53,385 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'first', 'thing', 'to', 'be', 'the', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', '.', '</s>']
2023-06-03 02:01:53,386 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 02:01:53,386 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 02:01:53,386 - INFO - joeynmt.training - 	Hypothesis: And the first thing to be the ssssssssssssssss.
2023-06-03 02:01:53,386 - INFO - joeynmt.training - Example #4
2023-06-03 02:01:53,386 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 02:01:53,386 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 02:01:53,386 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'to', 'be', 'the', 'first', 'of', 'the', 'first', 'of', 'the', 'first', 'pe@@', 'op@@', 'le', 'and', 'the', 'first', 'of', 'the', 'other', 'pe@@', 'op@@', 'le', 'and', 'the', 'world', 'and', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'to', 'the', 's@@', 's@@', '.', '</s>']
2023-06-03 02:01:53,386 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 02:01:53,386 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 02:01:53,387 - INFO - joeynmt.training - 	Hypothesis: The first thing to be the first of the first of the first people and the first of the other people and the world and the really really really really really really to the ss.
2023-06-03 02:02:37,858 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     3.228561, Batch Acc: 0.201086, Tokens per Sec:     1570, Lr: 0.000300
2023-06-03 02:03:22,269 - INFO - joeynmt.training - Epoch   1, Step:     2200, Batch Loss:     2.996204, Batch Acc: 0.208762, Tokens per Sec:     1609, Lr: 0.000300
2023-06-03 02:04:05,600 - INFO - joeynmt.training - Epoch   1, Step:     2300, Batch Loss:     3.130246, Batch Acc: 0.217554, Tokens per Sec:     1632, Lr: 0.000300
2023-06-03 02:04:49,857 - INFO - joeynmt.training - Epoch   1, Step:     2400, Batch Loss:     2.926176, Batch Acc: 0.222510, Tokens per Sec:     1595, Lr: 0.000300
2023-06-03 02:05:33,187 - INFO - joeynmt.training - Epoch   1, Step:     2500, Batch Loss:     2.938835, Batch Acc: 0.225266, Tokens per Sec:     1586, Lr: 0.000300
2023-06-03 02:05:33,187 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 02:10:36,901 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.01, ppl:  20.36, acc:   0.22, generation: 303.6881[sec], evaluation: 0.0000[sec]
2023-06-03 02:10:36,902 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 02:10:37,143 - INFO - joeynmt.training - Example #0
2023-06-03 02:10:37,144 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 02:10:37,144 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 02:10:37,144 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'I', 'was', 'a', 'lot', 'of', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 's@@', 's@@', 'av@@', 'ing', 'of', 'the', 'most', 'pe@@', 'op@@', 'le', 'to', 'the', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 's@@', 'av@@', 'av@@', 'av@@', 'av@@', 'av@@', 'av@@', 'av@@', 'av@@', 'av@@', 'av@@', 'ing', 'of', 'the', 'y@@', 'ear@@', '-@@', 'y@@', 'ear@@', '.', '</s>']
2023-06-03 02:10:37,144 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 02:10:37,144 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 02:10:37,145 - INFO - joeynmt.training - 	Hypothesis: The first thing I was a lot of the most people to the most people to the most people to the most people to the most people to the most people to the most people to the most people to the most people to the ssaving of the most people to the sssssssssavavavavavavavavavaving of the year-year.
2023-06-03 02:10:37,145 - INFO - joeynmt.training - Example #1
2023-06-03 02:10:37,145 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 02:10:37,145 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 02:10:37,145 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'is', 'the', 'most', 'of', 'the', 'most', 'of', 'the', 'most', 'of', 'the', 'most', 'of', 'the', 'most', 'of', 'the', 'same', 'thing', 'is', 'the', 'same', 'is', 'the', 's@@', 'un@@', 'd.', '</s>']
2023-06-03 02:10:37,145 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 02:10:37,145 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 02:10:37,145 - INFO - joeynmt.training - 	Hypothesis: The first thing is the most of the most of the most of the most of the most of the same thing is the same is the sund.
2023-06-03 02:10:37,146 - INFO - joeynmt.training - Example #2
2023-06-03 02:10:37,146 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 02:10:37,146 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 02:10:37,146 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'most', 'of', 'the', 'most', 'thing', 'is', 'a', 'lot', 'of', 'a', 'lot', 'of', 'the', 'most', 'thing', 'is', 'a', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'very', 'c@@', 'ul@@', 'ul@@', 'ul@@', 'es.', '</s>']
2023-06-03 02:10:37,146 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 02:10:37,146 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 02:10:37,146 - INFO - joeynmt.training - 	Hypothesis: The most of the most thing is a lot of a lot of the most thing is a very very very very very very very very very very very very very very very very very very very very very very very very very very very cululules.
2023-06-03 02:10:37,146 - INFO - joeynmt.training - Example #3
2023-06-03 02:10:37,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 02:10:37,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 02:10:37,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'the', 's@@', 'em@@', 'em@@', 'em@@', 'em@@', 'em@@', 'em@@', 'em@@', 'em@@', 'ot@@', '.', '</s>']
2023-06-03 02:10:37,147 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 02:10:37,147 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 02:10:37,147 - INFO - joeynmt.training - 	Hypothesis: You can see the sememememememememot.
2023-06-03 02:10:37,147 - INFO - joeynmt.training - Example #4
2023-06-03 02:10:37,147 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 02:10:37,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 02:10:37,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'most', 'most', 'pe@@', 'op@@', 'le', 'in', 'the', 'most', 'most', 'pe@@', 'op@@', 'le', 'in', 'the', 'most', 'most', 'most', 'of', 'the', 'most', 'most', 'pe@@', 'op@@', 'le', 'in', 'the', 'most', 'of', 'the', 'most', 'pe@@', 'op@@', 'le', 'in', 'the', 'most', 'of', 'the', 'most', 'pe@@', 'op@@', 'le', 'in', 'the', 'most', 'pe@@', 'op@@', 'le', 'in', 'the', 'most', 'pe@@', 'op@@', 'le', 'in', 'the', 'most', 'years.', '</s>']
2023-06-03 02:10:37,148 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 02:10:37,148 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 02:10:37,148 - INFO - joeynmt.training - 	Hypothesis: The most most people in the most most people in the most most most of the most most people in the most of the most people in the most of the most people in the most people in the most people in the most years.
2023-06-03 02:11:21,907 - INFO - joeynmt.training - Epoch   1, Step:     2600, Batch Loss:     2.995578, Batch Acc: 0.228562, Tokens per Sec:     1564, Lr: 0.000300
2023-06-03 02:12:07,698 - INFO - joeynmt.training - Epoch   1, Step:     2700, Batch Loss:     2.948015, Batch Acc: 0.233779, Tokens per Sec:     1541, Lr: 0.000300
2023-06-03 02:12:51,872 - INFO - joeynmt.training - Epoch   1, Step:     2800, Batch Loss:     2.829452, Batch Acc: 0.238252, Tokens per Sec:     1549, Lr: 0.000300
2023-06-03 02:13:35,457 - INFO - joeynmt.training - Epoch   1, Step:     2900, Batch Loss:     2.896491, Batch Acc: 0.241618, Tokens per Sec:     1614, Lr: 0.000300
2023-06-03 02:14:19,273 - INFO - joeynmt.training - Epoch   1, Step:     3000, Batch Loss:     2.739742, Batch Acc: 0.245744, Tokens per Sec:     1598, Lr: 0.000300
2023-06-03 02:14:19,274 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 02:19:21,721 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.89, ppl:  18.01, acc:   0.24, generation: 302.4197[sec], evaluation: 0.0000[sec]
2023-06-03 02:19:21,725 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 02:19:21,998 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/500.ckpt
2023-06-03 02:19:22,000 - INFO - joeynmt.training - Example #0
2023-06-03 02:19:22,000 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 02:19:22,001 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 02:19:22,001 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'first', 'I', 'thin@@', 'k', 'I', 'had', 'to', 'be', 'the', 'most', 'of', 'the', 'most', 'of', 'the', 'last', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '2', 'million', 'million', 'million', 'million', 'million', 'million', 'million', 'million', 'million', 'million', 'million', 'million', 'million', 'years', 'ago,', 'the', 'last', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', ',@@', '00@@', ',000', 'years', 'ago,', '</s>']
2023-06-03 02:19:22,001 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 02:19:22,001 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 02:19:22,001 - INFO - joeynmt.training - 	Hypothesis: The first first I think I had to be the most of the most of the last 11111112 million million million million million million million million million million million million million years ago, the last 1111111111111111111111111111111,00,000 years ago,
2023-06-03 02:19:22,001 - INFO - joeynmt.training - Example #1
2023-06-03 02:19:22,001 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 02:19:22,001 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 02:19:22,002 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'that', 'the', 'most', 'ex@@', 'pl@@', 'eas@@', 'y', 'of', 'the', 're@@', 'sul@@', 't', 'of', 'the', 're@@', 'sul@@', 't', 'of', 'the', 're@@', 'sul@@', 't', 'of', 'the', 're@@', 'sul@@', 't', 'of', 'the', 'same', 'thing.', '</s>']
2023-06-03 02:19:22,002 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 02:19:22,002 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 02:19:22,002 - INFO - joeynmt.training - 	Hypothesis: The first thing that the most expleasy of the result of the result of the result of the result of the same thing.
2023-06-03 02:19:22,002 - INFO - joeynmt.training - Example #2
2023-06-03 02:19:22,002 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 02:19:22,002 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 02:19:22,002 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'one', 'of', 'the', 're@@', 'cor@@', 'd', 'is', 'that', 'the', 're@@', 'cor@@', 'al', 'is', 'the', 're@@', 'sul@@', 't', 'of', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'the', 'world.', '</s>']
2023-06-03 02:19:22,003 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 02:19:22,003 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 02:19:22,003 - INFO - joeynmt.training - 	Hypothesis: The one of the record is that the recoral is the result of the really really really really really the world.
2023-06-03 02:19:22,003 - INFO - joeynmt.training - Example #3
2023-06-03 02:19:22,003 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 02:19:22,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 02:19:22,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'then', 'you', 'can', 'see', 'the', 'ro@@', 'om', 'and', 'the', 'ro@@', 'om', 'of', 'the', 'ro@@', 'om', '</s>']
2023-06-03 02:19:22,004 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 02:19:22,004 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 02:19:22,004 - INFO - joeynmt.training - 	Hypothesis: And then you can see the room and the room of the room
2023-06-03 02:19:22,004 - INFO - joeynmt.training - Example #4
2023-06-03 02:19:22,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 02:19:22,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 02:19:22,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'is', 'the', 'most', 'important', 'of', 'the', 'most', 'of', 'the', 'last', 'last', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '2', 'years.', '</s>']
2023-06-03 02:19:22,005 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 02:19:22,005 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 02:19:22,005 - INFO - joeynmt.training - 	Hypothesis: The first is the most important of the most of the last last 1111112 years.
2023-06-03 02:20:05,710 - INFO - joeynmt.training - Epoch   1, Step:     3100, Batch Loss:     2.658324, Batch Acc: 0.252549, Tokens per Sec:     1650, Lr: 0.000300
2023-06-03 02:20:50,834 - INFO - joeynmt.training - Epoch   1, Step:     3200, Batch Loss:     2.788144, Batch Acc: 0.256755, Tokens per Sec:     1524, Lr: 0.000300
2023-06-03 02:21:35,340 - INFO - joeynmt.training - Epoch   1, Step:     3300, Batch Loss:     2.633490, Batch Acc: 0.262521, Tokens per Sec:     1589, Lr: 0.000300
2023-06-03 02:22:19,881 - INFO - joeynmt.training - Epoch   1, Step:     3400, Batch Loss:     2.637438, Batch Acc: 0.268706, Tokens per Sec:     1549, Lr: 0.000300
2023-06-03 02:23:03,962 - INFO - joeynmt.training - Epoch   1, Step:     3500, Batch Loss:     2.769611, Batch Acc: 0.274155, Tokens per Sec:     1621, Lr: 0.000300
2023-06-03 02:23:03,962 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 02:27:52,403 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.77, ppl:  15.91, acc:   0.26, generation: 288.4162[sec], evaluation: 0.0000[sec]
2023-06-03 02:27:52,406 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 02:27:52,653 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/1000.ckpt
2023-06-03 02:27:52,656 - INFO - joeynmt.training - Example #0
2023-06-03 02:27:52,656 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 02:27:52,656 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 02:27:52,656 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'was', 'the', 'first', 'time', 'I', 'was', 'the', 'de@@', 'ep', 'to', 'the', 'way', 'to', 'the', 'f@@', 'ar', 'of', 'the', 'first', 'one', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'the', 'United', 'St@@', 'at@@', 'es,', 'the', 'last', 'million', 'years', 'of', 'the', 'last', 'million', 'years', 'of', 'the', 'United', 'St@@', 'at@@', 'at@@', 'at@@', 'at@@', 'ics', 'of', 'the', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '1@@', '2', 'percent', 'of', 'the', '<unk>', 'ou@@', 'th', 'of', 'the', 'United', 'St@@', 'at@@', 'es.', '</s>']
2023-06-03 02:27:52,657 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 02:27:52,657 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 02:27:52,657 - INFO - joeynmt.training - 	Hypothesis: I was the first time I was the deep to the way to the far of the first one of the United States, the United States, the last million years of the last million years of the United Statatatatics of the 11111112 percent of the <unk> outh of the United States.
2023-06-03 02:27:52,657 - INFO - joeynmt.training - Example #1
2023-06-03 02:27:52,657 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 02:27:52,657 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 02:27:52,657 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'that', 'this', 'is', 'the', 'same', 'way', 'to', 'the', 'same', 'thing', 'be@@', 'cause', 'the', 'same', 'thing', 'be@@', 'cause', 'the', 'same', 'time', 'the', 'same', 'same', 'time', 'the', 's@@', 'av@@', 'el@@', 'y.', '</s>']
2023-06-03 02:27:52,658 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 02:27:52,658 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 02:27:52,658 - INFO - joeynmt.training - 	Hypothesis: The first thing that this is the same way to the same thing because the same thing because the same time the same same time the savely.
2023-06-03 02:27:52,658 - INFO - joeynmt.training - Example #2
2023-06-03 02:27:52,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 02:27:52,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 02:27:52,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'one', 'of', 'the', 'same', 'ide@@', 'a', 'is', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'the', 're@@', 'ally', 'is', 'the', 're@@', 'ally', 'the', 're@@', 'ally', 're@@', 'ally', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'the', 'ex@@', 'amp@@', 'le', 'of', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'is', 'the', 'world.', '</s>']
2023-06-03 02:27:52,659 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 02:27:52,659 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 02:27:52,659 - INFO - joeynmt.training - 	Hypothesis: The one of the same idea is a little bit of a little bit of the really is the really the really really the really really really the example of the really really really is the world.
2023-06-03 02:27:52,659 - INFO - joeynmt.training - Example #3
2023-06-03 02:27:52,659 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 02:27:52,659 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 02:27:52,659 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'you', 'see', 'the', 'f@@', 'ar', 'and', 'the', 's@@', 'un@@', 'd@@', 'i@@', 'ed', 'the', 's@@', 'am@@', '.', '</s>']
2023-06-03 02:27:52,660 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 02:27:52,660 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 02:27:52,660 - INFO - joeynmt.training - 	Hypothesis: And you see the far and the sundied the sam.
2023-06-03 02:27:52,660 - INFO - joeynmt.training - Example #4
2023-06-03 02:27:52,660 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 02:27:52,660 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 02:27:52,660 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'to', 'be', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'the', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'last', 'years.', '</s>']
2023-06-03 02:27:52,661 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 02:27:52,661 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 02:27:52,661 - INFO - joeynmt.training - 	Hypothesis: The first thing to be a little bit of a little bit of the last last last last last last last last last last last last last last last last years.
2023-06-03 02:28:37,158 - INFO - joeynmt.training - Epoch   1, Step:     3600, Batch Loss:     2.660207, Batch Acc: 0.280037, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 02:29:20,856 - INFO - joeynmt.training - Epoch   1, Step:     3700, Batch Loss:     2.722097, Batch Acc: 0.287190, Tokens per Sec:     1624, Lr: 0.000300
2023-06-03 02:30:03,285 - INFO - joeynmt.training - Epoch   1: total training loss 12388.84
2023-06-03 02:30:03,285 - INFO - joeynmt.training - EPOCH 2
2023-06-03 02:30:04,130 - INFO - joeynmt.training - Epoch   2, Step:     3800, Batch Loss:     2.594797, Batch Acc: 0.287671, Tokens per Sec:     1557, Lr: 0.000300
2023-06-03 02:30:47,983 - INFO - joeynmt.training - Epoch   2, Step:     3900, Batch Loss:     2.385247, Batch Acc: 0.301112, Tokens per Sec:     1577, Lr: 0.000300
2023-06-03 02:31:31,739 - INFO - joeynmt.training - Epoch   2, Step:     4000, Batch Loss:     2.548948, Batch Acc: 0.304161, Tokens per Sec:     1615, Lr: 0.000300
2023-06-03 02:31:31,740 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 02:36:12,605 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.67, ppl:  14.50, acc:   0.28, generation: 280.8441[sec], evaluation: 0.0000[sec]
2023-06-03 02:36:12,605 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 02:36:12,862 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/1500.ckpt
2023-06-03 02:36:12,864 - INFO - joeynmt.training - Example #0
2023-06-03 02:36:12,865 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 02:36:12,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 02:36:12,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'one', 'of', 'th@@', 'ese', 'gu@@', 'ys', 'of', 'th@@', 'ese', 'd@@', 'i@@', 'st@@', 'ate', 'for', 'the', 'b@@', 're@@', 'a@@', 'ke', 'of', 'the', 'b@@', 're@@', 'a@@', 'ke', 'of', 'the', 'f@@', 'lo@@', 'ts', 'of', 'the', 'U.@@', 'S.', 'was', 'a', 'lot', 'of', 'the', 'U.@@', 'S.', 'was', '1@@', '7@@', '0@@', ',000', 'years', 'ag@@', 'o.', '</s>']
2023-06-03 02:36:12,865 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 02:36:12,865 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 02:36:12,866 - INFO - joeynmt.training - 	Hypothesis: The one of these guys of these distate for the breake of the breake of the flots of the U.S. was a lot of the U.S. was 170,000 years ago.
2023-06-03 02:36:12,866 - INFO - joeynmt.training - Example #1
2023-06-03 02:36:12,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 02:36:12,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 02:36:12,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'the', 'f@@', 'lo@@', 'ts', 'of', 'the', 're@@', 're@@', 'ally', 'is', 'be@@', 'cause', 'the', 're@@', 'sul@@', 't', 'of', 'the', 'b@@', 're@@', 'a@@', 'ke', 'of', 'the', 'b@@', 're@@', 'a@@', 'ke', 'of', 'the', 'b@@', 're@@', 'a@@', 'way.', '</s>']
2023-06-03 02:36:12,866 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 02:36:12,866 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 02:36:12,866 - INFO - joeynmt.training - 	Hypothesis: But the flots of the rereally is because the result of the breake of the breake of the breaway.
2023-06-03 02:36:12,867 - INFO - joeynmt.training - Example #2
2023-06-03 02:36:12,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 02:36:12,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 02:36:12,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'mo@@', 'tiv@@', 'ate', 'of', 'the', 'ro@@', 'om', 'is', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'the', 're@@', 'sul@@', 't', 'of', 'the', 'de@@', 'velo@@', 'p@@', 'h', 'of', 'the', 'de@@', 'st', 'of', 'the', 'de@@', 'pen@@', 'gu@@', 'gu@@', 'y.', '</s>']
2023-06-03 02:36:12,867 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 02:36:12,867 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 02:36:12,867 - INFO - joeynmt.training - 	Hypothesis: The motivate of the room is a little bit of a little bit of the result of the developh of the dest of the depenguguy.
2023-06-03 02:36:12,867 - INFO - joeynmt.training - Example #3
2023-06-03 02:36:12,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 02:36:12,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 02:36:12,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'see', 'it', 'in', 'the', 'b@@', 're@@', 'a@@', 'ke', 'and', 'd@@', 'i@@', 'e@@', 'r@@', 'r@@', 'i@@', 'ght.', '</s>']
2023-06-03 02:36:12,868 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 02:36:12,868 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 02:36:12,868 - INFO - joeynmt.training - 	Hypothesis: You see it in the breake and dierright.
2023-06-03 02:36:12,868 - INFO - joeynmt.training - Example #4
2023-06-03 02:36:12,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 02:36:12,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 02:36:12,869 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'second', 'is', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'a', 'little', 'bit', 'of', 'the', 'last', 'last', 'y@@', 'ear@@', '.', '</s>']
2023-06-03 02:36:12,869 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 02:36:12,869 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 02:36:12,869 - INFO - joeynmt.training - 	Hypothesis: The second is a little bit of a little bit of a little bit of the last last year.
2023-06-03 02:36:57,066 - INFO - joeynmt.training - Epoch   2, Step:     4100, Batch Loss:     2.607836, Batch Acc: 0.306565, Tokens per Sec:     1619, Lr: 0.000300
2023-06-03 02:37:40,780 - INFO - joeynmt.training - Epoch   2, Step:     4200, Batch Loss:     2.579884, Batch Acc: 0.310930, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 02:38:23,707 - INFO - joeynmt.training - Epoch   2, Step:     4300, Batch Loss:     2.562445, Batch Acc: 0.315888, Tokens per Sec:     1647, Lr: 0.000300
2023-06-03 02:39:08,158 - INFO - joeynmt.training - Epoch   2, Step:     4400, Batch Loss:     2.476848, Batch Acc: 0.322894, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 02:39:52,368 - INFO - joeynmt.training - Epoch   2, Step:     4500, Batch Loss:     2.554727, Batch Acc: 0.329327, Tokens per Sec:     1596, Lr: 0.000300
2023-06-03 02:39:52,369 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 02:44:28,755 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.57, ppl:  13.12, acc:   0.31, generation: 276.3664[sec], evaluation: 0.0000[sec]
2023-06-03 02:44:28,760 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 02:44:29,007 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/2000.ckpt
2023-06-03 02:44:29,010 - INFO - joeynmt.training - Example #0
2023-06-03 02:44:29,010 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 02:44:29,010 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 02:44:29,010 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'I', 'went', 'to', 'th@@', 'ese', 'two', 'two', 'years', 'to', 'get', 'th@@', 'ese', 'two', 'years', 'of', 'the', 'last', 'year', 'of', 'the', 't@@', 'un@@', 'ds', 'that', 'the', 'United', 'St@@', 'ates', 'that', 'a', 'lot', 'of', 'the', 'United', 'St@@', 'ates', 'has', 'been', 'a', 'year', 'in', 'the', 'United', 'St@@', 'ates', 'the', '6@@', '0@@', 's,', 'the', '6@@', '0', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'the', '6@@', '0@@', 's.', '</s>']
2023-06-03 02:44:29,010 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 02:44:29,011 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 02:44:29,011 - INFO - joeynmt.training - 	Hypothesis: The first I went to these two two years to get these two years of the last year of the tunds that the United States that a lot of the United States has been a year in the United States the 60s, the 60 percent of the United States the 60s.
2023-06-03 02:44:29,011 - INFO - joeynmt.training - Example #1
2023-06-03 02:44:29,011 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 02:44:29,011 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 02:44:29,011 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'the', 're@@', 'sul@@', 't', 'this', 'is', 'the', 're@@', 're@@', 'ally', 'the', 'problem', 'is', 'the', 'problem', 'be@@', 'cause', 'it', 'is', 'the', 'problem', 'be@@', 'cause', 'the', 's@@', 'un@@', 'c@@', 'an@@', 'no@@', 'un@@', 'd.', '</s>']
2023-06-03 02:44:29,011 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 02:44:29,011 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 02:44:29,012 - INFO - joeynmt.training - 	Hypothesis: All the result this is the rereally the problem is the problem because it is the problem because the suncannound.
2023-06-03 02:44:29,012 - INFO - joeynmt.training - Example #2
2023-06-03 02:44:29,012 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 02:44:29,012 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 02:44:29,012 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'kind', 'of', 'a', 'kind', 'of', 're@@', 'ally', 'a', 'sense', 'of', 'a', 'sense', 'of', 'the', 'sense', 'of', 'the', 'system', 'of', 'the', 'system', 'system', 'of', 'the', 'system', 'system', 'of', 'the', 'system', 'system', 'of', 'the', 'system', 'system', 'of', 'the', 'system', 'system', '</s>']
2023-06-03 02:44:29,012 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 02:44:29,012 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 02:44:29,012 - INFO - joeynmt.training - 	Hypothesis: The kind of a kind of really a sense of a sense of the sense of the system of the system system of the system system of the system system of the system system of the system system
2023-06-03 02:44:29,013 - INFO - joeynmt.training - Example #3
2023-06-03 02:44:29,013 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 02:44:29,013 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 02:44:29,013 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'see', 'it', 'in', 'the', 'f@@', 'lo@@', 'ts', 'of', 'the', 'r@@', 'i@@', 'ght.', '</s>']
2023-06-03 02:44:29,013 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 02:44:29,013 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 02:44:29,013 - INFO - joeynmt.training - 	Hypothesis: You can see it in the flots of the right.
2023-06-03 02:44:29,013 - INFO - joeynmt.training - Example #4
2023-06-03 02:44:29,014 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 02:44:29,014 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 02:44:29,014 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 's@@', 'mar@@', 't', 'of', 'the', 'most', 'f@@', 'as@@', 'king', 'a', 'f@@', 'lo@@', 'ts', 'of', 'the', 'last', '2@@', '2@@', '2@@', '2@@', '2@@', '2@@', '2@@', '2@@', '2@@', '5', 'years.', '</s>']
2023-06-03 02:44:29,014 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 02:44:29,014 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 02:44:29,014 - INFO - joeynmt.training - 	Hypothesis: The smart of the most fasking a flots of the last 2222222225 years.
2023-06-03 02:45:12,758 - INFO - joeynmt.training - Epoch   2, Step:     4600, Batch Loss:     2.356399, Batch Acc: 0.331652, Tokens per Sec:     1613, Lr: 0.000300
2023-06-03 02:45:57,085 - INFO - joeynmt.training - Epoch   2, Step:     4700, Batch Loss:     2.565822, Batch Acc: 0.335354, Tokens per Sec:     1567, Lr: 0.000300
2023-06-03 02:46:41,353 - INFO - joeynmt.training - Epoch   2, Step:     4800, Batch Loss:     2.476497, Batch Acc: 0.340606, Tokens per Sec:     1602, Lr: 0.000300
2023-06-03 02:47:25,132 - INFO - joeynmt.training - Epoch   2, Step:     4900, Batch Loss:     2.504048, Batch Acc: 0.349211, Tokens per Sec:     1576, Lr: 0.000300
2023-06-03 02:48:08,322 - INFO - joeynmt.training - Epoch   2, Step:     5000, Batch Loss:     2.438700, Batch Acc: 0.351620, Tokens per Sec:     1641, Lr: 0.000300
2023-06-03 02:48:08,322 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 02:51:06,031 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.49, ppl:  12.00, acc:   0.33, generation: 177.6923[sec], evaluation: 0.0000[sec]
2023-06-03 02:51:06,032 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 02:51:06,291 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/2500.ckpt
2023-06-03 02:51:06,294 - INFO - joeynmt.training - Example #0
2023-06-03 02:51:06,294 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 02:51:06,294 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 02:51:06,294 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'they', 'went', 'to', 'the', 'most', 'of', 'th@@', 'ese', 'd@@', 'i@@', 'stri@@', 'but@@', 'ed', 'to', 'the', 's@@', 'un@@', 'd@@', 'i@@', 'st@@', 'ed', 'to', 'the', 't@@', 'un@@', 'ch', 'of', 'the', 'mi@@', 'dd@@', 'le', 'of', 'the', 'United', 'St@@', 'ates', 'has', 'been', 'a', '2@@', '5', 'million', 'years', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'at@@', 'es.', '</s>']
2023-06-03 02:51:06,295 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 02:51:06,295 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 02:51:06,295 - INFO - joeynmt.training - 	Hypothesis: And they went to the most of these distributed to the sundisted to the tunch of the middle of the United States has been a 25 million years of the United States of the United States.
2023-06-03 02:51:06,295 - INFO - joeynmt.training - Example #1
2023-06-03 02:51:06,295 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 02:51:06,295 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 02:51:06,295 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 't@@', 'un@@', 'e', 'of', 'the', 's@@', 'un@@', 'ch@@', 'ed', 'the', 'big', 'problem', 'be@@', 'cause', 'the', 'big', 'problem', 'be@@', 'cause', 'it', "doesn't", 're@@', 'ally', 'have', 'the', 'd@@', 'i@@', 'd.', '</s>']
2023-06-03 02:51:06,296 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 02:51:06,296 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 02:51:06,296 - INFO - joeynmt.training - 	Hypothesis: And the tune of the sunched the big problem because the big problem because it doesn't really have the did.
2023-06-03 02:51:06,296 - INFO - joeynmt.training - Example #2
2023-06-03 02:51:06,296 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 02:51:06,296 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 02:51:06,296 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ph@@', 'ot@@', 'ot@@', 'es', 'of', 'the', 's@@', 'am@@', ',', 'you', 'know,', 'in', 'a', 'sen@@', 'se,', 'the', 's@@', 'ite', 'of', 'the', 's@@', 'un@@', 'al', 'system@@', '.', '</s>']
2023-06-03 02:51:06,297 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 02:51:06,297 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 02:51:06,297 - INFO - joeynmt.training - 	Hypothesis: The phototes of the sam, you know, in a sense, the site of the sunal system.
2023-06-03 02:51:06,297 - INFO - joeynmt.training - Example #3
2023-06-03 02:51:06,297 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 02:51:06,297 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 02:51:06,297 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'it@@', "'s", 'go@@', 'ing', 'to', 'go', 'and', 'd@@', 'i@@', 'st@@', 'and', 'and', 'r@@', 'i@@', 'ght.', '</s>']
2023-06-03 02:51:06,297 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 02:51:06,297 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 02:51:06,298 - INFO - joeynmt.training - 	Hypothesis: And it's going to go and distand and right.
2023-06-03 02:51:06,298 - INFO - joeynmt.training - Example #4
2023-06-03 02:51:06,298 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 02:51:06,298 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 02:51:06,298 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'of', 'the', 'f@@', 'ast@@', '.', '</s>']
2023-06-03 02:51:06,298 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 02:51:06,298 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 02:51:06,298 - INFO - joeynmt.training - 	Hypothesis: The next of the fast.
2023-06-03 02:51:49,804 - INFO - joeynmt.training - Epoch   2, Step:     5100, Batch Loss:     2.292858, Batch Acc: 0.352808, Tokens per Sec:     1573, Lr: 0.000300
2023-06-03 02:52:33,916 - INFO - joeynmt.training - Epoch   2, Step:     5200, Batch Loss:     2.250517, Batch Acc: 0.357740, Tokens per Sec:     1636, Lr: 0.000300
2023-06-03 02:53:17,464 - INFO - joeynmt.training - Epoch   2, Step:     5300, Batch Loss:     2.265809, Batch Acc: 0.369712, Tokens per Sec:     1630, Lr: 0.000300
2023-06-03 02:54:01,373 - INFO - joeynmt.training - Epoch   2, Step:     5400, Batch Loss:     2.436349, Batch Acc: 0.370156, Tokens per Sec:     1607, Lr: 0.000300
2023-06-03 02:54:44,775 - INFO - joeynmt.training - Epoch   2, Step:     5500, Batch Loss:     2.144077, Batch Acc: 0.381435, Tokens per Sec:     1627, Lr: 0.000300
2023-06-03 02:54:44,775 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 02:58:23,613 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.39, ppl:  10.95, acc:   0.35, generation: 218.8205[sec], evaluation: 0.0000[sec]
2023-06-03 02:58:23,614 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 02:58:23,855 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/3000.ckpt
2023-06-03 02:58:23,858 - INFO - joeynmt.training - Example #0
2023-06-03 02:58:23,858 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 02:58:23,858 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 02:58:23,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'they', 'have', 'to', 'show', 'th@@', 'ese', 'de@@', 'tec@@', 't', 'to', 'de@@', 'tec@@', 't', 'the', 'mo@@', 'un@@', 'c@@', 'le', 'that', 'the', 'sc@@', 'ale', 'of', 'the', 't@@', 'ac@@', 'tion', 'of', 'the', 'last', 'y@@', 'ear@@', 's', 'of', 'the', "'@@", '4@@', '8', 'million', 'years', 'of', 'the', "'@@", '7@@', ',', '1@@', '8', 'percent', 'of', 'the', "'@@", '9@@', '0@@', 's', '--', '</s>']
2023-06-03 02:58:23,859 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 02:58:23,859 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 02:58:23,859 - INFO - joeynmt.training - 	Hypothesis: And they have to show these detect to detect the mouncle that the scale of the taction of the last years of the '48 million years of the '7, 18 percent of the '90s --
2023-06-03 02:58:23,859 - INFO - joeynmt.training - Example #1
2023-06-03 02:58:23,859 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 02:58:23,859 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 02:58:23,859 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'the', 't@@', 'ab@@', 'le,', 'the', 'problem', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'have', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'have', 'the', 'd@@', 'i@@', 'vers@@', 'ity', 'of', 'the', 'd@@', 'i@@', 'vers@@', 'ity', 'of', 'the', 'ac@@', 'tion.', '</s>']
2023-06-03 02:58:23,860 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 02:58:23,860 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 02:58:23,860 - INFO - joeynmt.training - 	Hypothesis: All the table, the problem of the problem because it doesn't have the problem because it doesn't have the diversity of the diversity of the action.
2023-06-03 02:58:23,860 - INFO - joeynmt.training - Example #2
2023-06-03 02:58:23,860 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 02:58:23,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 02:58:23,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'p@@', 'sy@@', 'sy@@', 'm@@', 'met@@', 'al', 'is', 'a', 'sen@@', 'se,', 'in', 'a', 'sense', 'of', 'the', 'cli@@', 'p@@', ',', 'the', 'cli@@', 'p@@', '.', '</s>']
2023-06-03 02:58:23,861 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 02:58:23,861 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 02:58:23,861 - INFO - joeynmt.training - 	Hypothesis: The psysymmetal is a sense, in a sense of the clip, the clip.
2023-06-03 02:58:23,861 - INFO - joeynmt.training - Example #3
2023-06-03 02:58:23,861 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 02:58:23,861 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 02:58:23,861 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'it', 'went', 'to', 'the', 's@@', 'and@@', '-@@', 'and', 'd@@', 'i@@', 'e', 're@@', 'ally', 're@@', 'fl@@', 'ec@@', 't.', '</s>']
2023-06-03 02:58:23,861 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 02:58:23,862 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 02:58:23,862 - INFO - joeynmt.training - 	Hypothesis: And it went to the sand-and die really reflect.
2023-06-03 02:58:23,862 - INFO - joeynmt.training - Example #4
2023-06-03 02:58:23,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 02:58:23,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 02:58:23,862 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'ep', 'to', 'be', 'a', 'f@@', 'oun@@', 'd@@', 'ation', 'to', 'be', 'a', 'f@@', 'lo@@', 'se', 'of', 'the', 'last', '2@@', '2@@', '5', 'years.', '</s>']
2023-06-03 02:58:23,862 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 02:58:23,862 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 02:58:23,862 - INFO - joeynmt.training - 	Hypothesis: The next deep to be a foundation to be a flose of the last 225 years.
2023-06-03 02:59:08,634 - INFO - joeynmt.training - Epoch   2, Step:     5600, Batch Loss:     2.220466, Batch Acc: 0.378790, Tokens per Sec:     1545, Lr: 0.000300
2023-06-03 02:59:52,019 - INFO - joeynmt.training - Epoch   2, Step:     5700, Batch Loss:     2.249872, Batch Acc: 0.392554, Tokens per Sec:     1616, Lr: 0.000300
2023-06-03 03:00:35,632 - INFO - joeynmt.training - Epoch   2, Step:     5800, Batch Loss:     2.215137, Batch Acc: 0.393500, Tokens per Sec:     1613, Lr: 0.000300
2023-06-03 03:01:19,860 - INFO - joeynmt.training - Epoch   2, Step:     5900, Batch Loss:     2.148543, Batch Acc: 0.398460, Tokens per Sec:     1571, Lr: 0.000300
2023-06-03 03:02:03,179 - INFO - joeynmt.training - Epoch   2, Step:     6000, Batch Loss:     2.039848, Batch Acc: 0.403706, Tokens per Sec:     1630, Lr: 0.000300
2023-06-03 03:02:03,179 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 03:05:23,291 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.31, ppl:  10.09, acc:   0.37, generation: 200.0950[sec], evaluation: 0.0000[sec]
2023-06-03 03:05:23,295 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 03:05:23,539 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/3500.ckpt
2023-06-03 03:05:23,542 - INFO - joeynmt.training - Example #0
2023-06-03 03:05:23,542 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 03:05:23,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 03:05:23,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'year', 'I', 'show', 'you', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'to', 'the', 'right', 'right', 'to', 'the', 'h@@', 'ot@@', ',', 'which', 'is', 'that', 'the', 'd@@', 'i@@', 'vers@@', 'ity', 'of', 'the', 'd@@', 'i@@', 'vers@@', 'ity', 'has', 'been', 'been', 'the', 'si@@', 'x@@', '-@@', 'dimen@@', 'sion@@', 's', 'of', 'the', 'U.@@', 'S.', '</s>']
2023-06-03 03:05:23,542 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 03:05:23,543 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 03:05:23,543 - INFO - joeynmt.training - 	Hypothesis: The year year I show you these diasive to the right right to the hot, which is that the diversity of the diversity has been been the six-dimensions of the U.S.
2023-06-03 03:05:23,543 - INFO - joeynmt.training - Example #1
2023-06-03 03:05:23,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 03:05:23,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 03:05:23,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'of', 'this', 're@@', 'ally', 'val@@', 'u@@', 'es', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'big', 'problem', 'be@@', 'cause', 'you', "don't", 'have', 'the', 'r@@', 'i@@', 'gh@@', 'bor@@', 'n', 'of', 'the', 'ar@@', 'oun@@', 'd.', '</s>']
2023-06-03 03:05:23,543 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 03:05:23,543 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 03:05:23,544 - INFO - joeynmt.training - 	Hypothesis: All of this really values the gravity of the big problem because you don't have the righborn of the around.
2023-06-03 03:05:23,544 - INFO - joeynmt.training - Example #2
2023-06-03 03:05:23,544 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 03:05:23,544 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 03:05:23,544 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'ous', 'is', 'a', 'sense', 'of', 'the', 'cli@@', 'mat@@', 'h', 'of', 'the', 'cli@@', 'mat@@', 'e.', '</s>']
2023-06-03 03:05:23,544 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 03:05:23,544 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 03:05:23,544 - INFO - joeynmt.training - 	Hypothesis: The glacial glacious is a sense of the climath of the climate.
2023-06-03 03:05:23,545 - INFO - joeynmt.training - Example #3
2023-06-03 03:05:23,545 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 03:05:23,545 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 03:05:23,545 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'go', 'and', 'go', 'and', 's@@', 'ave', 'and', 're@@', 'duc@@', 'ed', 'and', 're@@', 'duc@@', 'ed', 'the', 'r@@', 'i@@', 'se', 'of', 'th@@', 'is.', '</s>']
2023-06-03 03:05:23,545 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 03:05:23,545 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 03:05:23,545 - INFO - joeynmt.training - 	Hypothesis: You go and go and save and reduced and reduced the rise of this.
2023-06-03 03:05:23,545 - INFO - joeynmt.training - Example #4
2023-06-03 03:05:23,545 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 03:05:23,546 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 03:05:23,546 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'm@@', 'en@@', 'der', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'very', 'qu@@', 'ick@@', 'ly', 'in', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 03:05:23,546 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 03:05:23,546 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 03:05:23,546 - INFO - joeynmt.training - 	Hypothesis: The next demender is going to be a very quickly in the last 25 years.
2023-06-03 03:06:07,493 - INFO - joeynmt.training - Epoch   2, Step:     6100, Batch Loss:     2.128458, Batch Acc: 0.407836, Tokens per Sec:     1575, Lr: 0.000300
2023-06-03 03:06:51,804 - INFO - joeynmt.training - Epoch   2, Step:     6200, Batch Loss:     2.193820, Batch Acc: 0.410405, Tokens per Sec:     1591, Lr: 0.000300
2023-06-03 03:07:36,454 - INFO - joeynmt.training - Epoch   2, Step:     6300, Batch Loss:     2.185775, Batch Acc: 0.416891, Tokens per Sec:     1556, Lr: 0.000300
2023-06-03 03:08:21,231 - INFO - joeynmt.training - Epoch   2, Step:     6400, Batch Loss:     2.162091, Batch Acc: 0.412186, Tokens per Sec:     1578, Lr: 0.000300
2023-06-03 03:09:04,738 - INFO - joeynmt.training - Epoch   2, Step:     6500, Batch Loss:     2.010220, Batch Acc: 0.418155, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 03:09:04,738 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 03:12:03,857 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.25, ppl:   9.47, acc:   0.39, generation: 179.1028[sec], evaluation: 0.0000[sec]
2023-06-03 03:12:03,858 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 03:12:04,093 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/4000.ckpt
2023-06-03 03:12:04,096 - INFO - joeynmt.training - Example #0
2023-06-03 03:12:04,097 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 03:12:04,097 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 03:12:04,097 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'to', 'show', 'that', 'the', 'mo@@', 'ther@@', 's', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'p@@', 'sy@@', 'ch@@', 'olog@@', 'y@@', ',', 'which', 'has', 'been', 'the', '4@@', '8', 'percent', 'of', 'years', 'of', 'the', 'United', 'St@@', 'ates', 'has', 'been', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '0@@', 's.', '</s>']
2023-06-03 03:12:04,097 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 03:12:04,097 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 03:12:04,098 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diasive to show that the mothers that the glacial psychology, which has been the 48 percent of years of the United States has been the 48 percent of the 48 percent of the 48 percent of the 40s.
2023-06-03 03:12:04,098 - INFO - joeynmt.training - Example #1
2023-06-03 03:12:04,098 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 03:12:04,098 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 03:12:04,098 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E@@', 'ver@@', 'y@@', 'body', 'is', 'the', 'great', 'val@@', 'u@@', 'es', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'big', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'p@@', 'ac@@', 'k.', '</s>']
2023-06-03 03:12:04,098 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 03:12:04,098 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 03:12:04,098 - INFO - joeynmt.training - 	Hypothesis: Everybody is the great values because it doesn't show the big problem because it doesn't show the pack.
2023-06-03 03:12:04,099 - INFO - joeynmt.training - Example #2
2023-06-03 03:12:04,099 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 03:12:04,099 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 03:12:04,099 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'clim@@', 'ate', 'system@@', ',', 'the', 'cu@@', 'st@@', 'om@@', 'e', 'system@@', '.', '</s>']
2023-06-03 03:12:04,099 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 03:12:04,099 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 03:12:04,099 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is in a sense of the climate system, the custome system.
2023-06-03 03:12:04,099 - INFO - joeynmt.training - Example #3
2023-06-03 03:12:04,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 03:12:04,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 03:12:04,100 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'the', 're@@', 'ally', 're@@', 'duc@@', 'ed', 'to', 'the', 'r@@', 'i@@', 'ght.', '</s>']
2023-06-03 03:12:04,100 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 03:12:04,100 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 03:12:04,100 - INFO - joeynmt.training - 	Hypothesis: And the expanding and the really reduced to the right.
2023-06-03 03:12:04,100 - INFO - joeynmt.training - Example #4
2023-06-03 03:12:04,100 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 03:12:04,100 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 03:12:04,101 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'gre@@', 'e', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'de@@', 'sc@@', 're@@', 'en', 'of', 'the', '2@@', '5', 'years.', '</s>']
2023-06-03 03:12:04,101 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 03:12:04,101 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 03:12:04,101 - INFO - joeynmt.training - 	Hypothesis: The next degree is going to be a quickly descreen of the 25 years.
2023-06-03 03:12:50,006 - INFO - joeynmt.training - Epoch   2, Step:     6600, Batch Loss:     1.989261, Batch Acc: 0.427372, Tokens per Sec:     1538, Lr: 0.000300
2023-06-03 03:13:34,981 - INFO - joeynmt.training - Epoch   2, Step:     6700, Batch Loss:     2.201151, Batch Acc: 0.431564, Tokens per Sec:     1565, Lr: 0.000300
2023-06-03 03:14:19,467 - INFO - joeynmt.training - Epoch   2, Step:     6800, Batch Loss:     2.025259, Batch Acc: 0.435782, Tokens per Sec:     1573, Lr: 0.000300
2023-06-03 03:15:04,122 - INFO - joeynmt.training - Epoch   2, Step:     6900, Batch Loss:     2.108969, Batch Acc: 0.429536, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 03:15:50,331 - INFO - joeynmt.training - Epoch   2, Step:     7000, Batch Loss:     2.268223, Batch Acc: 0.437817, Tokens per Sec:     1528, Lr: 0.000300
2023-06-03 03:15:50,332 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 03:19:47,895 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.18, ppl:   8.86, acc:   0.40, generation: 237.5419[sec], evaluation: 0.0000[sec]
2023-06-03 03:19:47,900 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 03:19:48,154 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/4500.ckpt
2023-06-03 03:19:48,157 - INFO - joeynmt.training - Example #0
2023-06-03 03:19:48,157 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 03:19:48,157 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 03:19:48,157 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'for', 'the', 'most', 'ex@@', 'ac@@', 'tly', 'the', 's@@', 'un@@', 'ch@@', 'ing', 'of', 'the', 's@@', 'ac@@', 'i@@', 'al', 'ex@@', 'cit@@', 'ed', 'to', 'almost', 'three', 'million', 'years', 'of', 'years', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'U.@@', 'S.', '</s>']
2023-06-03 03:19:48,158 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 03:19:48,158 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 03:19:48,158 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diasive for the most exactly the sunching of the sacial excited to almost three million years of years of the United States of the U.S.
2023-06-03 03:19:48,158 - INFO - joeynmt.training - Example #1
2023-06-03 03:19:48,158 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 03:19:48,158 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 03:19:48,158 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 's@@', 'un@@', 'c@@', 'ab@@', 'ab@@', 'solut@@', 'e', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'de@@', 'p@@', 'lo@@', 'y@@', 'or', 'the', 'd@@', 'i@@', 'gg@@', 'ing', 'of', 'the', 'gh@@', 'bor@@', 'n@@', '.', '</s>']
2023-06-03 03:19:48,159 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 03:19:48,159 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 03:19:48,159 - INFO - joeynmt.training - 	Hypothesis: And this is the suncababsolute of the problem because it doesn't show the deployor the digging of the ghborn.
2023-06-03 03:19:48,159 - INFO - joeynmt.training - Example #2
2023-06-03 03:19:48,159 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 03:19:48,159 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 03:19:48,159 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', '--', 'in', 'a', 'certa@@', 'in', 'in', 'a', 'certa@@', 'in', 'the', 'global', 'system@@', '.', '</s>']
2023-06-03 03:19:48,159 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 03:19:48,160 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 03:19:48,160 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is -- in a certain in a certain the global system.
2023-06-03 03:19:48,160 - INFO - joeynmt.training - Example #3
2023-06-03 03:19:48,160 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 03:19:48,160 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 03:19:48,160 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'the', 's@@', 'kill@@', 's', 'and', 'the', 're@@', 'st', 'of', 'th@@', 'is.', '</s>']
2023-06-03 03:19:48,160 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 03:19:48,160 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 03:19:48,161 - INFO - joeynmt.training - 	Hypothesis: And the expanding and the skills and the rest of this.
2023-06-03 03:19:48,161 - INFO - joeynmt.training - Example #4
2023-06-03 03:19:48,161 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 03:19:48,161 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 03:19:48,161 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'gre@@', 'e', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'f@@', 'ast@@', ',', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 03:19:48,161 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 03:19:48,161 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 03:19:48,161 - INFO - joeynmt.training - 	Hypothesis: The next degree is going to be a fast, the last 25 years.
2023-06-03 03:20:35,920 - INFO - joeynmt.training - Epoch   2, Step:     7100, Batch Loss:     1.967289, Batch Acc: 0.445005, Tokens per Sec:     1469, Lr: 0.000300
2023-06-03 03:21:21,273 - INFO - joeynmt.training - Epoch   2, Step:     7200, Batch Loss:     1.997603, Batch Acc: 0.442311, Tokens per Sec:     1545, Lr: 0.000300
2023-06-03 03:22:06,027 - INFO - joeynmt.training - Epoch   2, Step:     7300, Batch Loss:     2.126924, Batch Acc: 0.441951, Tokens per Sec:     1540, Lr: 0.000300
2023-06-03 03:22:50,911 - INFO - joeynmt.training - Epoch   2, Step:     7400, Batch Loss:     2.092035, Batch Acc: 0.442121, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 03:23:35,913 - INFO - joeynmt.training - Epoch   2, Step:     7500, Batch Loss:     2.016867, Batch Acc: 0.444141, Tokens per Sec:     1562, Lr: 0.000300
2023-06-03 03:23:35,914 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 03:27:25,867 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.14, ppl:   8.47, acc:   0.42, generation: 229.9362[sec], evaluation: 0.0000[sec]
2023-06-03 03:27:25,871 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 03:27:26,087 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/5000.ckpt
2023-06-03 03:27:26,089 - INFO - joeynmt.training - Example #0
2023-06-03 03:27:26,090 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 03:27:26,090 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 03:27:26,090 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'to', 'show', 'that', 'the', 'mi@@', 'str@@', 'ate', 'of', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'and', 'almost', 'three', 'million', 'years', 'ago,', 'that', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', '<unk>', 'r@@', 'i@@', 'st@@', 'y@@', ',', '<unk>', 'r@@', 'i@@', 'st@@', '-@@', '4@@', '0@@', 's.', '</s>']
2023-06-03 03:27:26,090 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 03:27:26,090 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 03:27:26,090 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diasive to show that the mistrate of glacial glacial and almost three million years ago, that for almost three million years of the United States, <unk> risty, <unk> rist-40s.
2023-06-03 03:27:26,091 - INFO - joeynmt.training - Example #1
2023-06-03 03:27:26,091 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 03:27:26,091 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 03:27:26,091 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'of', 'this', 'se@@', 'ot@@', 'al', 'be@@', 'cause', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'the', 'most', 'of', 'the', 'gh@@', 'ac@@', 'coun@@', 'tr@@', 'y.', '</s>']
2023-06-03 03:27:26,091 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 03:27:26,091 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 03:27:26,091 - INFO - joeynmt.training - 	Hypothesis: All of this seotal because of the gravity of the problem because the most of the ghaccountry.
2023-06-03 03:27:26,091 - INFO - joeynmt.training - Example #2
2023-06-03 03:27:26,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 03:27:26,092 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 03:27:26,092 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'al@@', 'al@@', 'c@@', 'ul@@', 'ar@@', 'y@@', ',', 'in', 'a', 'certa@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 03:27:26,092 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 03:27:26,092 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 03:27:26,092 - INFO - joeynmt.training - 	Hypothesis: The alalculary, in a certain sense, the heart of the climate system.
2023-06-03 03:27:26,092 - INFO - joeynmt.training - Example #3
2023-06-03 03:27:26,092 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 03:27:26,093 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 03:27:26,093 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'it', 'ex@@', 'p@@', 'and', 'the', 's@@', 'ite', 'and', 'you', 're@@', 'ally', 'get', 'to', 'the', 'out@@', 'side', 'of', 'th@@', 'is.', '</s>']
2023-06-03 03:27:26,093 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 03:27:26,093 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 03:27:26,093 - INFO - joeynmt.training - 	Hypothesis: And it expand the site and you really get to the outside of this.
2023-06-03 03:27:26,093 - INFO - joeynmt.training - Example #4
2023-06-03 03:27:26,093 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 03:27:26,093 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 03:27:26,093 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'ep', 'will', 'be', 'a', 'lot', 'of', 'f@@', 'ast@@', 'ed', 'to', 'be', 'a', 'lot', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 03:27:26,094 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 03:27:26,094 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 03:27:26,094 - INFO - joeynmt.training - 	Hypothesis: The next deep will be a lot of fasted to be a lot of the last 25 years.
2023-06-03 03:28:07,834 - INFO - joeynmt.training - Epoch   2: total training loss 8536.34
2023-06-03 03:28:07,835 - INFO - joeynmt.training - EPOCH 3
2023-06-03 03:28:10,782 - INFO - joeynmt.training - Epoch   3, Step:     7600, Batch Loss:     1.926518, Batch Acc: 0.465766, Tokens per Sec:     1482, Lr: 0.000300
2023-06-03 03:28:55,014 - INFO - joeynmt.training - Epoch   3, Step:     7700, Batch Loss:     1.945441, Batch Acc: 0.470206, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 03:29:39,566 - INFO - joeynmt.training - Epoch   3, Step:     7800, Batch Loss:     1.904761, Batch Acc: 0.468115, Tokens per Sec:     1586, Lr: 0.000300
2023-06-03 03:30:24,459 - INFO - joeynmt.training - Epoch   3, Step:     7900, Batch Loss:     1.740204, Batch Acc: 0.466115, Tokens per Sec:     1601, Lr: 0.000300
2023-06-03 03:31:10,900 - INFO - joeynmt.training - Epoch   3, Step:     8000, Batch Loss:     2.060214, Batch Acc: 0.464335, Tokens per Sec:     1543, Lr: 0.000300
2023-06-03 03:31:10,900 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 03:34:36,067 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.10, ppl:   8.17, acc:   0.43, generation: 205.1486[sec], evaluation: 0.0000[sec]
2023-06-03 03:34:36,068 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 03:34:36,296 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/5500.ckpt
2023-06-03 03:34:36,299 - INFO - joeynmt.training - Example #0
2023-06-03 03:34:36,299 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 03:34:36,299 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 03:34:36,299 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'to', 'show', 'that', 'the', 'd@@', 'i@@', 'str@@', 'ate', 'of', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'd@@', 'i@@', 'al', 'd@@', 'i@@', 'al', 'years', 'of', 'the', 'si@@', 'ze', 'of', 'the', 'se@@', 'a', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '0@@', 's.', '</s>']
2023-06-03 03:34:36,300 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 03:34:36,300 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 03:34:36,300 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diasive to show that the distrate of the glacial dial dial years of the size of the sea 48 percent of the 48 percent of the 48 percent of the 48 percent of the 40s.
2023-06-03 03:34:36,300 - INFO - joeynmt.training - Example #1
2023-06-03 03:34:36,300 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 03:34:36,300 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 03:34:36,300 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'of', 'this', 's@@', 'w@@', 'im@@', 'm@@', 'ing', 'the', 'great', 'problem', 'be@@', 'cause', 'of', 'the', 'problem', 'be@@', 'cause', 'of', 'the', 'problem', 'be@@', 'cause', 'of', 'the', 'd@@', 'i@@', 'sh@@', '.', '</s>']
2023-06-03 03:34:36,301 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 03:34:36,301 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 03:34:36,301 - INFO - joeynmt.training - 	Hypothesis: All of this swimming the great problem because of the problem because of the problem because of the dish.
2023-06-03 03:34:36,301 - INFO - joeynmt.training - Example #2
2023-06-03 03:34:36,301 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 03:34:36,301 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 03:34:36,301 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'y@@', ',', 'in', 'a', 'sense', 'of', 'the', 'cli@@', 'mat@@', 'ic', 'system@@', ',', 'the', 'hear@@', 't', 'system@@', '.', '</s>']
2023-06-03 03:34:36,301 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 03:34:36,302 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 03:34:36,302 - INFO - joeynmt.training - 	Hypothesis: The glacial arty, in a sense of the climatic system, the heart system.
2023-06-03 03:34:36,302 - INFO - joeynmt.training - Example #3
2023-06-03 03:34:36,302 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 03:34:36,302 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 03:34:36,302 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'it', 'was', 'go@@', 'ing', 'to', 'be', 'in', 'th@@', 'is.', '</s>']
2023-06-03 03:34:36,302 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 03:34:36,302 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 03:34:36,303 - INFO - joeynmt.training - 	Hypothesis: It is expanding and it was going to be in this.
2023-06-03 03:34:36,303 - INFO - joeynmt.training - Example #4
2023-06-03 03:34:36,303 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 03:34:36,303 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 03:34:36,303 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'pro@@', 'si@@', 'on', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'qu@@', 'ick@@', 'ly', 'd@@', 'i@@', 'st@@', 'ed', 'by', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 03:34:36,303 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 03:34:36,303 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 03:34:36,303 - INFO - joeynmt.training - 	Hypothesis: The prosion is going to be a quickly quickly disted by the last 25 years.
2023-06-03 03:35:20,692 - INFO - joeynmt.training - Epoch   3, Step:     8100, Batch Loss:     1.934259, Batch Acc: 0.467550, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 03:36:04,796 - INFO - joeynmt.training - Epoch   3, Step:     8200, Batch Loss:     2.064882, Batch Acc: 0.472443, Tokens per Sec:     1566, Lr: 0.000300
2023-06-03 03:36:48,360 - INFO - joeynmt.training - Epoch   3, Step:     8300, Batch Loss:     1.866440, Batch Acc: 0.473769, Tokens per Sec:     1600, Lr: 0.000300
2023-06-03 03:37:32,707 - INFO - joeynmt.training - Epoch   3, Step:     8400, Batch Loss:     1.689559, Batch Acc: 0.475749, Tokens per Sec:     1614, Lr: 0.000300
2023-06-03 03:38:16,492 - INFO - joeynmt.training - Epoch   3, Step:     8500, Batch Loss:     1.863562, Batch Acc: 0.473763, Tokens per Sec:     1664, Lr: 0.000300
2023-06-03 03:38:16,492 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 03:41:32,027 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.08, ppl:   7.97, acc:   0.43, generation: 195.5186[sec], evaluation: 0.0000[sec]
2023-06-03 03:41:32,028 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 03:41:32,250 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/6000.ckpt
2023-06-03 03:41:32,253 - INFO - joeynmt.training - Example #0
2023-06-03 03:41:32,254 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 03:41:32,254 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 03:41:32,254 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'for', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'cal@@', 'cal@@', 'y', 'of', 'gl@@', 'ac@@', 'i@@', 'al', 'tw@@', 'ice', 'in', 'the', 'si@@', 'z@@', 'z@@', 'es,', 'for', 'almost', 'three', 'million', 'years', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 03:41:32,254 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 03:41:32,254 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 03:41:32,254 - INFO - joeynmt.training - 	Hypothesis: The year year I showed these diasive for the glacial calcalcaly of glacial twice in the sizzes, for almost three million years of the 48 percent of the 48 percent of the 40.
2023-06-03 03:41:32,254 - INFO - joeynmt.training - Example #1
2023-06-03 03:41:32,254 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 03:41:32,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 03:41:32,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'wh@@', 'ol@@', 'e', 'thing', 'is', 'the', 'great', 'problem', 'be@@', 'cause', 'it', 'is', 'not', 'the', 'big', 'problem', 'be@@', 'cause', 'it', 'is', 'not', 'show@@', 'ing', 'it', 'it', 'is', 'the', '<unk>', 'ou@@', 'gh@@', '.', '</s>']
2023-06-03 03:41:32,255 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 03:41:32,255 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 03:41:32,255 - INFO - joeynmt.training - 	Hypothesis: The whole thing is the great problem because it is not the big problem because it is not showing it it is the <unk> ough.
2023-06-03 03:41:32,255 - INFO - joeynmt.training - Example #2
2023-06-03 03:41:32,255 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 03:41:32,255 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 03:41:32,255 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'cal@@', 'm', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'e', 'of', 'the', 'clim@@', 'ate', 'system@@', ',', 'the', 'hear@@', 't', 'of', 'global', 'ma@@', 'j@@', 'or', 'global', 'ma@@', 'j@@', 'or', 'global', 'ma@@', 'j@@', 'or', 'global', 'ma@@', 'j@@', 'or', 'the', 'lob@@', 'al', 'system@@', '.', '</s>']
2023-06-03 03:41:32,256 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 03:41:32,256 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 03:41:32,256 - INFO - joeynmt.training - 	Hypothesis: The calm glacial glacial artice of the climate system, the heart of global major global major global major global major the lobal system.
2023-06-03 03:41:32,256 - INFO - joeynmt.training - Example #3
2023-06-03 03:41:32,256 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 03:41:32,256 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 03:41:32,256 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'e@@', 'r@@', 'and@@', 'e@@', 'r@@', 'ing', 'and', 'you', 're@@', 'ally', 'get', 'out', 'of', 'th@@', 'is.', '</s>']
2023-06-03 03:41:32,257 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 03:41:32,257 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 03:41:32,257 - INFO - joeynmt.training - 	Hypothesis: Herandering and you really get out of this.
2023-06-03 03:41:32,257 - INFO - joeynmt.training - Example #4
2023-06-03 03:41:32,257 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 03:41:32,257 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 03:41:32,257 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'ep', 'de@@', 'velo@@', 'p', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'f@@', 'ast@@', 'er', 'than', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2023-06-03 03:41:32,258 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 03:41:32,258 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 03:41:32,258 - INFO - joeynmt.training - 	Hypothesis: The next deep develop is going to be a quickly faster than 25 years old.
2023-06-03 03:42:16,996 - INFO - joeynmt.training - Epoch   3, Step:     8600, Batch Loss:     1.972070, Batch Acc: 0.474855, Tokens per Sec:     1563, Lr: 0.000300
2023-06-03 03:43:00,226 - INFO - joeynmt.training - Epoch   3, Step:     8700, Batch Loss:     1.883607, Batch Acc: 0.472053, Tokens per Sec:     1626, Lr: 0.000300
2023-06-03 03:43:44,356 - INFO - joeynmt.training - Epoch   3, Step:     8800, Batch Loss:     1.947726, Batch Acc: 0.478004, Tokens per Sec:     1575, Lr: 0.000300
2023-06-03 03:44:27,364 - INFO - joeynmt.training - Epoch   3, Step:     8900, Batch Loss:     1.946405, Batch Acc: 0.479232, Tokens per Sec:     1628, Lr: 0.000300
2023-06-03 03:45:11,780 - INFO - joeynmt.training - Epoch   3, Step:     9000, Batch Loss:     1.842058, Batch Acc: 0.477644, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 03:45:11,780 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 03:49:00,596 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.05, ppl:   7.74, acc:   0.44, generation: 228.7953[sec], evaluation: 0.0000[sec]
2023-06-03 03:49:00,598 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 03:49:00,825 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/6500.ckpt
2023-06-03 03:49:00,828 - INFO - joeynmt.training - Example #0
2023-06-03 03:49:00,829 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 03:49:00,829 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 03:49:00,829 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'to', 'show', 'that', 'the', 'd@@', 'i@@', 'al', 'al@@', 'ong', 'the', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'the', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'ally', 're@@', 'st@@', 'a@@', 'y@@', ',', '</s>']
2023-06-03 03:49:00,829 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 03:49:00,829 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 03:49:00,829 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diasive to show that the dial along the artical artica, which is about three million years had the United States of the United States, the United States, it's really restay,
2023-06-03 03:49:00,829 - INFO - joeynmt.training - Example #1
2023-06-03 03:49:00,829 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 03:49:00,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 03:49:00,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['E@@', 'ver@@', 'y@@', 'thing', 'is', 'ex@@', 'ac@@', 'tly', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'd@@', 'i@@', 'e', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'is@@', 'sue', 'of', 'the', 'd@@', 'i@@', 'd.', '</s>']
2023-06-03 03:49:00,830 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 03:49:00,830 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 03:49:00,830 - INFO - joeynmt.training - 	Hypothesis: Everything is exactly the gravity of the problem because it doesn't show the die of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice issue of the did.
2023-06-03 03:49:00,830 - INFO - joeynmt.training - Example #2
2023-06-03 03:49:00,830 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 03:49:00,830 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 03:49:00,830 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ical', 'ar@@', 't@@', 'ical', 'cu@@', 'or', 'is', 'in', 'a', 'way,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'the', 'clim@@', 'ate', 'system', 'in', 'a', 'way,', '</s>']
2023-06-03 03:49:00,831 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 03:49:00,831 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 03:49:00,831 - INFO - joeynmt.training - 	Hypothesis: The glacial artical artical cuor is in a way, the heart of the climate system of the climate system of the climate system of the climate system of the climate system of the climate system of the climate system in a way,
2023-06-03 03:49:00,831 - INFO - joeynmt.training - Example #3
2023-06-03 03:49:00,831 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 03:49:00,831 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 03:49:00,831 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'it', 'tur@@', 'ns', 'out', 'of', 'the', 'and', 'you', 'w@@', 'ould', 'be', 'the', 'out@@', 'side', 'of', 'th@@', 'is.', '</s>']
2023-06-03 03:49:00,832 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 03:49:00,832 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 03:49:00,832 - INFO - joeynmt.training - 	Hypothesis: And it turns out of the and you would be the outside of this.
2023-06-03 03:49:00,832 - INFO - joeynmt.training - Example #4
2023-06-03 03:49:00,832 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 03:49:00,832 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 03:49:00,832 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'tec@@', 'tion', 'will', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'f@@', 'ast@@', 'ed', 'by', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 03:49:00,833 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 03:49:00,833 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 03:49:00,833 - INFO - joeynmt.training - 	Hypothesis: The next detection will be a quickly fasted by the last 25 years.
2023-06-03 03:49:44,486 - INFO - joeynmt.training - Epoch   3, Step:     9100, Batch Loss:     1.800755, Batch Acc: 0.478633, Tokens per Sec:     1581, Lr: 0.000300
2023-06-03 03:50:28,184 - INFO - joeynmt.training - Epoch   3, Step:     9200, Batch Loss:     1.748245, Batch Acc: 0.477215, Tokens per Sec:     1630, Lr: 0.000300
2023-06-03 03:51:11,999 - INFO - joeynmt.training - Epoch   3, Step:     9300, Batch Loss:     1.944378, Batch Acc: 0.484825, Tokens per Sec:     1585, Lr: 0.000300
2023-06-03 03:51:55,661 - INFO - joeynmt.training - Epoch   3, Step:     9400, Batch Loss:     1.875850, Batch Acc: 0.484908, Tokens per Sec:     1634, Lr: 0.000300
2023-06-03 03:52:39,349 - INFO - joeynmt.training - Epoch   3, Step:     9500, Batch Loss:     1.761498, Batch Acc: 0.485805, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 03:52:39,349 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 03:56:49,638 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.02, ppl:   7.56, acc:   0.44, generation: 250.2714[sec], evaluation: 0.0000[sec]
2023-06-03 03:56:49,642 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 03:56:49,886 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/7000.ckpt
2023-06-03 03:56:49,889 - INFO - joeynmt.training - Example #0
2023-06-03 03:56:49,889 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 03:56:49,890 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 03:56:49,890 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'de@@', 'sp@@', 'ite', 'of', 'the', 'w@@', 'all', 'of', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'd@@', 'i@@', 'st@@', 'ing', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'almost', 'three', 'million', 'years', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'in', 'the', 'United', 'St@@', 'at@@', 'es,', 'which', 'has', 'been', 'the', 're@@', 'st', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'ally', 're@@', 'ally', 're@@', 'st@@', 'a@@', 'king', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 03:56:49,890 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 03:56:49,890 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 03:56:49,890 - INFO - joeynmt.training - 	Hypothesis: The year I showed these despite of the wall of the glacial disting artica, which is almost three million years of the United States, in the United States, which has been the rest of the United States, it's really really restaking the 40.
2023-06-03 03:56:49,890 - INFO - joeynmt.training - Example #1
2023-06-03 03:56:49,890 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 03:56:49,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 03:56:49,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'this', 'is', 'the', 'wh@@', 'ol@@', 'e', 'thing', 'about', 'the', 'great', 'problem', 'of', 'the', 'wh@@', 'ol@@', 'e', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'sp@@', 'ite', 'the', 'sp@@', 'ite', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'is@@', 'sue', 'of', 'the', 'wh@@', 'ol@@', 'e', 'is@@', 'sue', 'of', 'the', 'wh@@', 'ol@@', 'e', 'is@@', 'm.', '</s>']
2023-06-03 03:56:49,891 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 03:56:49,891 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 03:56:49,891 - INFO - joeynmt.training - 	Hypothesis: All this is the whole thing about the great problem of the whole problem because it doesn't show the spite the spite of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice issue of the whole issue of the whole ism.
2023-06-03 03:56:49,891 - INFO - joeynmt.training - Example #2
2023-06-03 03:56:49,891 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 03:56:49,891 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 03:56:49,891 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'hear@@', 't', 'of', 'the', 'global', 'system', 'of', 'global', 'global', 'system', 'of', 'global', 'global', 'global', 'system', 'of', 'global', 'global', 'global', 'system@@', '.', '</s>']
2023-06-03 03:56:49,892 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 03:56:49,892 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 03:56:49,892 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is in a sense of the heart of the global system of global global system of global global global system of global global global system.
2023-06-03 03:56:49,892 - INFO - joeynmt.training - Example #3
2023-06-03 03:56:49,892 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 03:56:49,892 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 03:56:49,892 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'ex@@', 'p@@', 'and@@', 'ing', 'from', 'the', 'win@@', 'do@@', 'w', 'and', 'you', 'w@@', 'ro@@', 'te', 'and', 'you', 'get', 're@@', 'ally', 're@@', 'ally', 're@@', 'move', '</s>']
2023-06-03 03:56:49,893 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 03:56:49,893 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 03:56:49,893 - INFO - joeynmt.training - 	Hypothesis: It's expanding from the window and you wrote and you get really really remove
2023-06-03 03:56:49,893 - INFO - joeynmt.training - Example #4
2023-06-03 03:56:49,893 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 03:56:49,893 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 03:56:49,893 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'pro@@', 'mi@@', 'se', 'will', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'de@@', 'ep', 'in', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 03:56:49,893 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 03:56:49,894 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 03:56:49,894 - INFO - joeynmt.training - 	Hypothesis: The next promise will be a quickly deep in the last 25 years.
2023-06-03 03:57:34,633 - INFO - joeynmt.training - Epoch   3, Step:     9600, Batch Loss:     1.663494, Batch Acc: 0.485047, Tokens per Sec:     1577, Lr: 0.000300
2023-06-03 03:58:17,946 - INFO - joeynmt.training - Epoch   3, Step:     9700, Batch Loss:     1.750630, Batch Acc: 0.481445, Tokens per Sec:     1619, Lr: 0.000300
2023-06-03 03:59:02,110 - INFO - joeynmt.training - Epoch   3, Step:     9800, Batch Loss:     1.739108, Batch Acc: 0.490911, Tokens per Sec:     1576, Lr: 0.000300
2023-06-03 03:59:45,406 - INFO - joeynmt.training - Epoch   3, Step:     9900, Batch Loss:     1.832712, Batch Acc: 0.485876, Tokens per Sec:     1616, Lr: 0.000300
2023-06-03 04:00:30,050 - INFO - joeynmt.training - Epoch   3, Step:    10000, Batch Loss:     2.007116, Batch Acc: 0.487445, Tokens per Sec:     1599, Lr: 0.000300
2023-06-03 04:00:30,050 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:03:43,103 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.00, ppl:   7.39, acc:   0.45, generation: 193.0370[sec], evaluation: 0.0000[sec]
2023-06-03 04:03:43,104 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:03:43,346 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/7500.ckpt
2023-06-03 04:03:43,348 - INFO - joeynmt.training - Example #0
2023-06-03 04:03:43,348 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:03:43,348 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:03:43,348 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'ving', 'th@@', 'ese', 'd@@', 'i@@', 'ving', 'that', 'the', 'd@@', 'i@@', 'st@@', 'ant', 'ch@@', 'em@@', 'i@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'about', 'about', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', '<unk>', 're@@', 'st@@', 'a@@', 'y@@', ',', '<unk>', 're@@', 'st@@', 'a@@', 'y@@', ',', '<unk>', 're@@', 'st@@', 'a@@', 'ur@@', 't@@', 'y', 'of', 'the', '4@@', '0@@', '-@@', 'percent', 'percent', 'of', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 04:03:43,349 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:03:43,349 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:03:43,349 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diving these diving that the distant chemial artica, which is about about about 48 percent of the United States, <unk> restay, <unk> restay, <unk> restaurty of the 40-percent percent of the 40.
2023-06-03 04:03:43,349 - INFO - joeynmt.training - Example #1
2023-06-03 04:03:43,349 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:03:43,349 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:03:43,349 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'wh@@', 'ol@@', 'e', 'thing', 'about', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'sp@@', 'ite', 'the', 'sp@@', 'here', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'wh@@', 'ol@@', 'e', 'is@@', 'su@@', 'e.', '</s>']
2023-06-03 04:03:43,350 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:03:43,350 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:03:43,350 - INFO - joeynmt.training - 	Hypothesis: And this is the whole thing about the gravity of the problem because it doesn't show the spite the sphere of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the whole issue.
2023-06-03 04:03:43,350 - INFO - joeynmt.training - Example #2
2023-06-03 04:03:43,350 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:03:43,350 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:03:43,350 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'y@@', ',', 'in', 'a', 'sense', 'of', 'the', 'hear@@', 't', 'system', 'of', 'the', 'clim@@', 'ate', 'system', 'of', 'global', 'cli@@', 'mat@@', 'e,', '</s>']
2023-06-03 04:03:43,351 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:03:43,351 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:03:43,351 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial arty, in a sense of the heart system of the climate system of global climate,
2023-06-03 04:03:43,351 - INFO - joeynmt.training - Example #3
2023-06-03 04:03:43,351 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:03:43,351 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:03:43,351 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'go@@', 'ing', 'to', 'be', 'in', 'the', 's@@', 'ou@@', 'ther@@', 'n', 'and', 'the', 're@@', 'st', 'of', 'the', 'se@@', 'c@@', 'tor@@', '.', '</s>']
2023-06-03 04:03:43,351 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:03:43,351 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:03:43,352 - INFO - joeynmt.training - 	Hypothesis: It is going to be in the southern and the rest of the sector.
2023-06-03 04:03:43,352 - INFO - joeynmt.training - Example #4
2023-06-03 04:03:43,352 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:03:43,352 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:03:43,352 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'de@@', 'e@@', 'ply', 'by', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 04:03:43,352 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:03:43,352 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:03:43,352 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a quickly deeply by the last 25 years.
2023-06-03 04:04:26,920 - INFO - joeynmt.training - Epoch   3, Step:    10100, Batch Loss:     1.943383, Batch Acc: 0.488794, Tokens per Sec:     1596, Lr: 0.000300
2023-06-03 04:05:10,041 - INFO - joeynmt.training - Epoch   3, Step:    10200, Batch Loss:     1.832038, Batch Acc: 0.488290, Tokens per Sec:     1615, Lr: 0.000300
2023-06-03 04:05:52,604 - INFO - joeynmt.training - Epoch   3, Step:    10300, Batch Loss:     1.912174, Batch Acc: 0.491132, Tokens per Sec:     1644, Lr: 0.000300
2023-06-03 04:06:36,377 - INFO - joeynmt.training - Epoch   3, Step:    10400, Batch Loss:     1.853639, Batch Acc: 0.492349, Tokens per Sec:     1586, Lr: 0.000300
2023-06-03 04:07:20,487 - INFO - joeynmt.training - Epoch   3, Step:    10500, Batch Loss:     1.770921, Batch Acc: 0.497771, Tokens per Sec:     1587, Lr: 0.000300
2023-06-03 04:07:20,487 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:11:04,507 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.97, ppl:   7.20, acc:   0.46, generation: 224.0003[sec], evaluation: 0.0000[sec]
2023-06-03 04:11:04,508 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:11:04,750 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/8000.ckpt
2023-06-03 04:11:04,753 - INFO - joeynmt.training - Example #0
2023-06-03 04:11:04,753 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:11:04,753 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:11:04,753 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'ght@@', 'ly', 'de@@', 'sp@@', 'ite', 'that', 'the', 'd@@', 'i@@', 'ves', 'that', 'the', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'to', 'almost', 'three', 'three', 'million', 'years', 'of', 'the', 'nex@@', 't', 'four', 'to', 'the', 'si@@', 'ze', '4@@', '8', 'percent', 'of', 'the', '4@@', '0@@', '-@@', 'year', 'year', 'percent@@', '.', '</s>']
2023-06-03 04:11:04,753 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:11:04,754 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:11:04,754 - INFO - joeynmt.training - 	Hypothesis: And I showed these slightly despite that the dives that the artica, which is about three million years to almost three three million years of the next four to the size 48 percent of the 40-year year percent.
2023-06-03 04:11:04,754 - INFO - joeynmt.training - Example #1
2023-06-03 04:11:04,754 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:11:04,754 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:11:04,754 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'a', 'wh@@', 'ol@@', 'e', 'thing', 'be@@', 'cause', 'the', 'gr@@', 'av@@', 'ity', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'b@@', 're@@', 'ak@@', 'e.', '</s>']
2023-06-03 04:11:04,754 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:11:04,754 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:11:04,755 - INFO - joeynmt.training - 	Hypothesis: And this is a whole thing because the gravity because it doesn't show the breake.
2023-06-03 04:11:04,755 - INFO - joeynmt.training - Example #2
2023-06-03 04:11:04,755 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:11:04,755 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:11:04,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'l', 'is', 'in', 'a', 'certa@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'system', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 04:11:04,755 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:11:04,755 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:11:04,755 - INFO - joeynmt.training - 	Hypothesis: The glacial glacil is in a certain sense, the heart system of the climate system.
2023-06-03 04:11:04,755 - INFO - joeynmt.training - Example #3
2023-06-03 04:11:04,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:11:04,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:11:04,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 'ing', 'from', 'the', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 're@@', 'ally', 'ex@@', 'p@@', 'and@@', 'ing', 'to', 'be', 'ex@@', 'p@@', 'and@@', 'ing', 'to', 'be', 'ex@@', 'p@@', 'and@@', 'ing', '</s>']
2023-06-03 04:11:04,756 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:11:04,756 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:11:04,756 - INFO - joeynmt.training - 	Hypothesis: You expanding from the expanding and really expanding to be expanding to be expanding
2023-06-03 04:11:04,756 - INFO - joeynmt.training - Example #4
2023-06-03 04:11:04,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:11:04,757 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:11:04,757 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'qu@@', 'ick@@', 'ly', 'over', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 04:11:04,757 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:11:04,757 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:11:04,757 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly quickly over the last 25 years.
2023-06-03 04:11:49,150 - INFO - joeynmt.training - Epoch   3, Step:    10600, Batch Loss:     1.673981, Batch Acc: 0.495415, Tokens per Sec:     1575, Lr: 0.000300
2023-06-03 04:12:32,643 - INFO - joeynmt.training - Epoch   3, Step:    10700, Batch Loss:     1.615301, Batch Acc: 0.500729, Tokens per Sec:     1608, Lr: 0.000300
2023-06-03 04:13:16,690 - INFO - joeynmt.training - Epoch   3, Step:    10800, Batch Loss:     1.885232, Batch Acc: 0.496222, Tokens per Sec:     1604, Lr: 0.000300
2023-06-03 04:13:59,265 - INFO - joeynmt.training - Epoch   3, Step:    10900, Batch Loss:     1.896996, Batch Acc: 0.498479, Tokens per Sec:     1653, Lr: 0.000300
2023-06-03 04:14:43,272 - INFO - joeynmt.training - Epoch   3, Step:    11000, Batch Loss:     1.761309, Batch Acc: 0.495239, Tokens per Sec:     1618, Lr: 0.000300
2023-06-03 04:14:43,273 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:18:12,537 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.96, ppl:   7.07, acc:   0.46, generation: 209.2486[sec], evaluation: 0.0000[sec]
2023-06-03 04:18:12,538 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:18:12,790 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/8500.ckpt
2023-06-03 04:18:12,792 - INFO - joeynmt.training - Example #0
2023-06-03 04:18:12,793 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:18:12,793 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:18:12,793 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'de@@', 'mon@@', 'str@@', 'ate', 'of', 'the', 'd@@', 'i@@', 've', 'd@@', 'i@@', 've', 'd@@', 'i@@', 've', 'that', 'the', 'mi@@', 'dd@@', 'le', 'of', 'the', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'mark@@', 'able', 'to', '4@@', '0@@', '.', '</s>']
2023-06-03 04:18:12,793 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:18:12,793 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:18:12,794 - INFO - joeynmt.training - 	Hypothesis: The year I showed these demonstrate of the dive dive dive that the middle of the three million years had the size of the United States of the United States, it's remarkable to 40.
2023-06-03 04:18:12,794 - INFO - joeynmt.training - Example #1
2023-06-03 04:18:12,794 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:18:12,794 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:18:12,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'is@@', "n't", 'ex@@', 'hi@@', 'p@@', '.', '</s>']
2023-06-03 04:18:12,794 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:18:12,794 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:18:12,794 - INFO - joeynmt.training - 	Hypothesis: And the gravity of the gravity of the problem because it doesn't show the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice isn't exhip.
2023-06-03 04:18:12,794 - INFO - joeynmt.training - Example #2
2023-06-03 04:18:12,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:18:12,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:18:12,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'certa@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 04:18:12,795 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:18:12,795 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:18:12,795 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is in a certain sense, the heart of the climate system.
2023-06-03 04:18:12,795 - INFO - joeynmt.training - Example #3
2023-06-03 04:18:12,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:18:12,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:18:12,796 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'get', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'you', 'w@@', 'ould', 're@@', 'duc@@', 'e', 'the', 're@@', 'st', 'of', 'th@@', 'is.', '</s>']
2023-06-03 04:18:12,796 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:18:12,796 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:18:12,796 - INFO - joeynmt.training - 	Hypothesis: You get expanding and you would reduce the rest of this.
2023-06-03 04:18:12,796 - INFO - joeynmt.training - Example #4
2023-06-03 04:18:12,796 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:18:12,796 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:18:12,797 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'velo@@', 'p', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'f@@', 'ast@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2023-06-03 04:18:12,797 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:18:12,797 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:18:12,797 - INFO - joeynmt.training - 	Hypothesis: The next develop is going to be a quickly fasted on the last 25 years old.
2023-06-03 04:18:56,878 - INFO - joeynmt.training - Epoch   3, Step:    11100, Batch Loss:     1.828664, Batch Acc: 0.499785, Tokens per Sec:     1626, Lr: 0.000300
2023-06-03 04:19:39,990 - INFO - joeynmt.training - Epoch   3, Step:    11200, Batch Loss:     1.696211, Batch Acc: 0.501197, Tokens per Sec:     1619, Lr: 0.000300
2023-06-03 04:20:25,024 - INFO - joeynmt.training - Epoch   3, Step:    11300, Batch Loss:     1.864564, Batch Acc: 0.502948, Tokens per Sec:     1563, Lr: 0.000300
2023-06-03 04:21:01,588 - INFO - joeynmt.training - Epoch   3: total training loss 7019.23
2023-06-03 04:21:01,588 - INFO - joeynmt.training - EPOCH 4
2023-06-03 04:21:08,267 - INFO - joeynmt.training - Epoch   4, Step:    11400, Batch Loss:     1.578268, Batch Acc: 0.523716, Tokens per Sec:     1522, Lr: 0.000300
2023-06-03 04:21:52,730 - INFO - joeynmt.training - Epoch   4, Step:    11500, Batch Loss:     1.814847, Batch Acc: 0.517700, Tokens per Sec:     1619, Lr: 0.000300
2023-06-03 04:21:52,731 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:24:58,768 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.95, acc:   0.47, generation: 186.0202[sec], evaluation: 0.0000[sec]
2023-06-03 04:24:58,769 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:24:59,000 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/9000.ckpt
2023-06-03 04:24:59,002 - INFO - joeynmt.training - Example #0
2023-06-03 04:24:59,003 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:24:59,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:24:59,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'de@@', 'mon@@', 'str@@', 'ated', 'th@@', 'ese', 'd@@', 'i@@', 'str@@', 'ating', 'that', 'the', 'cal@@', 'ot@@', 't', 'of', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'of', '4@@', '0@@', '.', '</s>']
2023-06-03 04:24:59,003 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:24:59,003 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:24:59,003 - INFO - joeynmt.training - 	Hypothesis: The year I showed these demonstrated these distrating that the calott of glacial artica, which is about three million years had the size of 48 percent of the United States of 40.
2023-06-03 04:24:59,003 - INFO - joeynmt.training - Example #1
2023-06-03 04:24:59,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:24:59,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:24:59,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'problem', 'of', 'the', 'gl@@', 'ac@@', 'i@@', 'er.', '</s>']
2023-06-03 04:24:59,004 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:24:59,004 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:24:59,004 - INFO - joeynmt.training - 	Hypothesis: The gravity of the gravity of the problem because it doesn't show the problem of the glacier.
2023-06-03 04:24:59,004 - INFO - joeynmt.training - Example #2
2023-06-03 04:24:59,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:24:59,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:24:59,005 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'certa@@', 'in', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 04:24:59,005 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:24:59,005 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:24:59,005 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is in a certain sense, the heart of the climate system.
2023-06-03 04:24:59,005 - INFO - joeynmt.training - Example #3
2023-06-03 04:24:59,005 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:24:59,005 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:24:59,005 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and@@', 'de', 'and', 'it', 'tur@@', 'ns', 'out', 'to', 'the', 'h@@', 'and', 'it', 'u@@', 'p', 'to', 'the', 'h@@', 'or@@', 'se', 'of', 'it.', '</s>']
2023-06-03 04:24:59,006 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:24:59,006 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:24:59,006 - INFO - joeynmt.training - 	Hypothesis: You expandde and it turns out to the hand it up to the horse of it.
2023-06-03 04:24:59,006 - INFO - joeynmt.training - Example #4
2023-06-03 04:24:59,006 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:24:59,006 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:24:59,006 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'velo@@', 'p', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'full@@', 'y', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', '2@@', '5', 'years.', '</s>']
2023-06-03 04:24:59,007 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:24:59,007 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:24:59,007 - INFO - joeynmt.training - 	Hypothesis: The next develop is going to be a quickly carefully is going to be a quickly 25 years.
2023-06-03 04:25:41,793 - INFO - joeynmt.training - Epoch   4, Step:    11600, Batch Loss:     1.581498, Batch Acc: 0.516924, Tokens per Sec:     1627, Lr: 0.000300
2023-06-03 04:26:25,494 - INFO - joeynmt.training - Epoch   4, Step:    11700, Batch Loss:     1.564220, Batch Acc: 0.523184, Tokens per Sec:     1607, Lr: 0.000300
2023-06-03 04:27:08,463 - INFO - joeynmt.training - Epoch   4, Step:    11800, Batch Loss:     1.734578, Batch Acc: 0.516005, Tokens per Sec:     1650, Lr: 0.000300
2023-06-03 04:27:53,576 - INFO - joeynmt.training - Epoch   4, Step:    11900, Batch Loss:     1.654975, Batch Acc: 0.525151, Tokens per Sec:     1584, Lr: 0.000300
2023-06-03 04:28:37,069 - INFO - joeynmt.training - Epoch   4, Step:    12000, Batch Loss:     1.584239, Batch Acc: 0.520010, Tokens per Sec:     1604, Lr: 0.000300
2023-06-03 04:28:37,069 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:31:27,199 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.94, ppl:   6.93, acc:   0.47, generation: 170.1139[sec], evaluation: 0.0000[sec]
2023-06-03 04:31:27,199 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:31:27,435 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/9500.ckpt
2023-06-03 04:31:27,438 - INFO - joeynmt.training - Example #0
2023-06-03 04:31:27,438 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:31:27,438 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:31:27,438 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'd@@', 'i@@', 've', 'd@@', 'i@@', 've', 'that', 'the', 'ar@@', 'tic@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'about', 'three', 'million', 'years', 'of', 'the', '4@@', '0@@', '-@@', 'dimen@@', 'sion@@', 'al', 'dimen@@', 'sion@@', 's', 'of', 'the', '4@@', '0@@', '-@@', 'y@@', 'ear@@', '-@@', 'old', 'si@@', 'ze', 'of', '4@@', '0@@', 's.', '</s>']
2023-06-03 04:31:27,439 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:31:27,439 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:31:27,439 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide diasive dive dive that the artical artica, which is about about three million years of the 40-dimensional dimensions of the 40-year-old size of 40s.
2023-06-03 04:31:27,439 - INFO - joeynmt.training - Example #1
2023-06-03 04:31:27,439 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:31:27,439 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:31:27,439 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 're@@', 'st', 'of', 'this', 's@@', 'ite', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'it', 'the', 'ice', 'of', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'of', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'ice', 'ice', 'of', 'the', 'ice', 'is@@', 'sue', 'of', 'the', 'ice', 'of', 'the', 'ice', 'is@@', 'sue', 'of', 'the', 'problem', 'be@@', 'cause', 'it', 'is', 'the', 'problem', 'be@@', 'cause', 'it', 'is', 'the', 'is@@', "n't", 'the', 'ice', 'of', 'the', 'ice', 'is@@', "n't", 'the', 'ice', 'as', 'a', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ice', 'is@@', 'is@@', 'is@@', 'is@@', 'n@@', 'ic@@', 'e.', '</s>']
2023-06-03 04:31:27,440 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:31:27,440 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:31:27,440 - INFO - joeynmt.training - 	Hypothesis: And the rest of this site is the gravity of the problem because it doesn't show it the ice of the ice of ice ice ice ice ice ice ice of the ice of ice ice ice ice ice of the ice issue of the ice of the ice issue of the problem because it is the problem because it is the isn't the ice of the ice isn't the ice as a gravity of the ice isisisisnice.
2023-06-03 04:31:27,440 - INFO - joeynmt.training - Example #2
2023-06-03 04:31:27,440 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:31:27,440 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:31:27,440 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ical', 'cu@@', 'st@@', 'om@@', 'at@@', 'el@@', 'y@@', ',', 'the', 'hear@@', 't', 'system', 'of', 'global', 'system@@', '.', '</s>']
2023-06-03 04:31:27,441 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:31:27,441 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:31:27,441 - INFO - joeynmt.training - 	Hypothesis: The art glacial glacial artical customately, the heart system of global system.
2023-06-03 04:31:27,441 - INFO - joeynmt.training - Example #3
2023-06-03 04:31:27,441 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:31:27,441 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:31:27,441 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'was', 's@@', 'and@@', 'ing', 'from', 'the', 'h@@', 'and', 'it', 're@@', 'ally', 're@@', 'duc@@', 'ed', 'by', 'th@@', 'is.', '</s>']
2023-06-03 04:31:27,441 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:31:27,442 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:31:27,442 - INFO - joeynmt.training - 	Hypothesis: It was sanding from the hand it really reduced by this.
2023-06-03 04:31:27,442 - INFO - joeynmt.training - Example #4
2023-06-03 04:31:27,442 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:31:27,442 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:31:27,442 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'vic@@', 'e,', 'it@@', "'s", 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'r@@', 'y', 'of', 'the', 'last', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2023-06-03 04:31:27,442 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:31:27,442 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:31:27,442 - INFO - joeynmt.training - 	Hypothesis: The next device, it's going to be a quickly carery of the last 25 years old.
2023-06-03 04:32:10,187 - INFO - joeynmt.training - Epoch   4, Step:    12100, Batch Loss:     1.668276, Batch Acc: 0.519214, Tokens per Sec:     1613, Lr: 0.000300
2023-06-03 04:32:54,539 - INFO - joeynmt.training - Epoch   4, Step:    12200, Batch Loss:     1.714006, Batch Acc: 0.515470, Tokens per Sec:     1594, Lr: 0.000300
2023-06-03 04:33:37,722 - INFO - joeynmt.training - Epoch   4, Step:    12300, Batch Loss:     1.652370, Batch Acc: 0.514795, Tokens per Sec:     1610, Lr: 0.000300
2023-06-03 04:34:21,607 - INFO - joeynmt.training - Epoch   4, Step:    12400, Batch Loss:     1.660504, Batch Acc: 0.515034, Tokens per Sec:     1604, Lr: 0.000300
2023-06-03 04:35:04,743 - INFO - joeynmt.training - Epoch   4, Step:    12500, Batch Loss:     1.460178, Batch Acc: 0.519570, Tokens per Sec:     1613, Lr: 0.000300
2023-06-03 04:35:04,743 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:37:39,691 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.92, ppl:   6.84, acc:   0.47, generation: 154.9317[sec], evaluation: 0.0000[sec]
2023-06-03 04:37:39,692 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:37:39,941 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/10000.ckpt
2023-06-03 04:37:39,943 - INFO - joeynmt.training - Example #0
2023-06-03 04:37:39,944 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:37:39,944 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:37:39,944 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'de@@', 'vic@@', 'es', 'to', 'show', 'th@@', 'ese', 'sli@@', 'de', '--', 'that', 'the', 'c@@', 'al@@', 'c@@', 'ul@@', 'ate', 'c@@', 'y@@', 'c@@', 'le', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '0@@', '-@@', 'y@@', 'ear@@', '-@@', 'old', 'si@@', 'ze', 'of', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 04:37:39,944 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:37:39,944 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:37:39,944 - INFO - joeynmt.training - 	Hypothesis: The year I showed these devices to show these slide -- that the calculate cycle for almost three million years had the size of the 40-year-old size of the 40.
2023-06-03 04:37:39,944 - INFO - joeynmt.training - Example #1
2023-06-03 04:37:39,945 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:37:39,945 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:37:39,945 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'why', 'not', 'show@@', 'ing', 'it', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'sp@@', 'ite', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'is@@', 'su@@', 'e.', '</s>']
2023-06-03 04:37:39,945 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:37:39,945 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:37:39,945 - INFO - joeynmt.training - 	Hypothesis: And this is the gravity of the gravity of why not showing it because it doesn't show the spite of the ice of the ice of the ice of the ice of the ice of the ice issue.
2023-06-03 04:37:39,945 - INFO - joeynmt.training - Example #2
2023-06-03 04:37:39,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:37:39,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:37:39,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'to', 'be', 'in', 'a', 'sense', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 04:37:39,946 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:37:39,946 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:37:39,946 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is to be in a sense of the climate system.
2023-06-03 04:37:39,946 - INFO - joeynmt.training - Example #3
2023-06-03 04:37:39,946 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:37:39,946 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:37:39,946 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'know,', 'the', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'you', 're@@', 'ally', 're@@', 'turn', 'to', 'the', 'se@@', 'at@@', '.', '</s>']
2023-06-03 04:37:39,947 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:37:39,947 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:37:39,947 - INFO - joeynmt.training - 	Hypothesis: You know, the expanding and you really return to the seat.
2023-06-03 04:37:39,947 - INFO - joeynmt.training - Example #4
2023-06-03 04:37:39,947 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:37:39,947 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:37:39,947 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', '2@@', '5', 'years.', '</s>']
2023-06-03 04:37:39,948 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:37:39,948 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:37:39,948 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly going to be a quickly 25 years.
2023-06-03 04:38:23,998 - INFO - joeynmt.training - Epoch   4, Step:    12600, Batch Loss:     1.873849, Batch Acc: 0.518958, Tokens per Sec:     1608, Lr: 0.000300
2023-06-03 04:39:06,324 - INFO - joeynmt.training - Epoch   4, Step:    12700, Batch Loss:     1.481561, Batch Acc: 0.520771, Tokens per Sec:     1640, Lr: 0.000300
2023-06-03 04:39:50,778 - INFO - joeynmt.training - Epoch   4, Step:    12800, Batch Loss:     1.589558, Batch Acc: 0.521008, Tokens per Sec:     1556, Lr: 0.000300
2023-06-03 04:40:34,016 - INFO - joeynmt.training - Epoch   4, Step:    12900, Batch Loss:     1.758287, Batch Acc: 0.519614, Tokens per Sec:     1621, Lr: 0.000300
2023-06-03 04:41:17,175 - INFO - joeynmt.training - Epoch   4, Step:    13000, Batch Loss:     1.853890, Batch Acc: 0.519337, Tokens per Sec:     1604, Lr: 0.000300
2023-06-03 04:41:17,175 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:44:02,592 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.90, ppl:   6.69, acc:   0.47, generation: 165.4006[sec], evaluation: 0.0000[sec]
2023-06-03 04:44:02,593 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:44:02,818 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/10500.ckpt
2023-06-03 04:44:02,820 - INFO - joeynmt.training - Example #0
2023-06-03 04:44:02,821 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:44:02,821 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:44:02,821 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'of', 'sli@@', 'de', 'for', 'the', 'mi@@', 'dd@@', 'le', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'been', 're@@', 'si@@', 'li@@', 'zed', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'z@@', 'y@@', ',', 'the', 'U.@@', 'S.', 'contin@@', 'ent@@', 's,', 'it@@', "'s", 're@@', 'st@@', 'ra@@', 'i@@', 'ght', 'of', '4@@', '0@@', '.', '</s>']
2023-06-03 04:44:02,821 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:44:02,821 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:44:02,822 - INFO - joeynmt.training - 	Hypothesis: And I showed these slide of slide for the middle glacial artica, which is about three million years had been resilized for almost three million years had the sizy, the U.S. continents, it's restraight of 40.
2023-06-03 04:44:02,822 - INFO - joeynmt.training - Example #1
2023-06-03 04:44:02,822 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:44:02,822 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:44:02,822 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'is@@', 'sue', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 's@@', 'le@@', 'ep@@', '.', '</s>']
2023-06-03 04:44:02,822 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:44:02,822 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:44:02,822 - INFO - joeynmt.training - 	Hypothesis: And this is the gravity of the gravity issue because it doesn't show the sleep.
2023-06-03 04:44:02,823 - INFO - joeynmt.training - Example #2
2023-06-03 04:44:02,823 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:44:02,823 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:44:02,823 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['<unk>', 'e@@', 'r@@', 'ci@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 04:44:02,823 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:44:02,823 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:44:02,823 - INFO - joeynmt.training - 	Hypothesis: <unk> ercial glacial is in a sense of the heart of the climate system.
2023-06-03 04:44:02,823 - INFO - joeynmt.training - Example #3
2023-06-03 04:44:02,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:44:02,824 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:44:02,824 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'you', 'w@@', 'ould', 'be', 'ab@@', 'out.', '</s>']
2023-06-03 04:44:02,824 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:44:02,824 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:44:02,824 - INFO - joeynmt.training - 	Hypothesis: And the expanding and you would be about.
2023-06-03 04:44:02,824 - INFO - joeynmt.training - Example #4
2023-06-03 04:44:02,824 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:44:02,825 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:44:02,825 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'er', 'was', 'a', 'qu@@', 'ick@@', 'ly', '2@@', '5', 'years.', '</s>']
2023-06-03 04:44:02,825 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:44:02,825 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:44:02,825 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick career was a quickly 25 years.
2023-06-03 04:44:46,669 - INFO - joeynmt.training - Epoch   4, Step:    13100, Batch Loss:     1.476852, Batch Acc: 0.521148, Tokens per Sec:     1643, Lr: 0.000300
2023-06-03 04:45:30,574 - INFO - joeynmt.training - Epoch   4, Step:    13200, Batch Loss:     1.521274, Batch Acc: 0.521791, Tokens per Sec:     1610, Lr: 0.000300
2023-06-03 04:46:14,439 - INFO - joeynmt.training - Epoch   4, Step:    13300, Batch Loss:     1.697278, Batch Acc: 0.519453, Tokens per Sec:     1623, Lr: 0.000300
2023-06-03 04:46:57,786 - INFO - joeynmt.training - Epoch   4, Step:    13400, Batch Loss:     1.665174, Batch Acc: 0.520510, Tokens per Sec:     1580, Lr: 0.000300
2023-06-03 04:47:41,883 - INFO - joeynmt.training - Epoch   4, Step:    13500, Batch Loss:     1.852888, Batch Acc: 0.521670, Tokens per Sec:     1600, Lr: 0.000300
2023-06-03 04:47:41,883 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:51:15,341 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.61, acc:   0.48, generation: 213.4390[sec], evaluation: 0.0000[sec]
2023-06-03 04:51:15,342 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:51:15,584 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/11000.ckpt
2023-06-03 04:51:15,587 - INFO - joeynmt.training - Example #0
2023-06-03 04:51:15,587 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:51:15,587 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:51:15,587 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'of', 'sli@@', 'de', 'for', 'the', 'mi@@', 'dd@@', 'le', 'gl@@', 'ac@@', 'i@@', 'l', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'three', 'million', 'years', 'of', 'the', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'three', 'million', 'years', 'of', 'the', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', 'which', 'is', 're@@', 'ally', 're@@', 'st@@', 'a@@', 'king', 'for', '4@@', '0@@', 's.', '</s>']
2023-06-03 04:51:15,588 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:51:15,588 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:51:15,588 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide of slide for the middle glacil artica, which for three million years of the artica, which for three million years of the United States continental continental continental continents, which is really restaking for 40s.
2023-06-03 04:51:15,588 - INFO - joeynmt.training - Example #1
2023-06-03 04:51:15,588 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:51:15,588 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:51:15,588 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'a', 's@@', 'lo@@', 'ad', 'of', 'the', 'gr@@', 'av@@', 'ity', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'ex@@', 'hi@@', 'p@@', '.', '</s>']
2023-06-03 04:51:15,589 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:51:15,589 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:51:15,589 - INFO - joeynmt.training - 	Hypothesis: And this is a sload of the gravity problem because it doesn't show the ice exhip.
2023-06-03 04:51:15,589 - INFO - joeynmt.training - Example #2
2023-06-03 04:51:15,589 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:51:15,589 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:51:15,589 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'l', 'gl@@', 'ac@@', 'i@@', 'l', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'system@@', '.', '</s>']
2023-06-03 04:51:15,590 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:51:15,590 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:51:15,590 - INFO - joeynmt.training - 	Hypothesis: The glacil glacil is, in a sense, the heart of the global system.
2023-06-03 04:51:15,590 - INFO - joeynmt.training - Example #3
2023-06-03 04:51:15,590 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:51:15,590 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:51:15,590 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'ex@@', 'p@@', 'and@@', 's', 'from', 'the', 'w@@', 'all@@', 's', 'and', 'you', 'w@@', 'ould', 'be', 'the', 'h@@', 'at@@', 'e.', '</s>']
2023-06-03 04:51:15,591 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:51:15,591 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:51:15,591 - INFO - joeynmt.training - 	Hypothesis: It's expands from the walls and you would be the hate.
2023-06-03 04:51:15,591 - INFO - joeynmt.training - Example #4
2023-06-03 04:51:15,591 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:51:15,591 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:51:15,591 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'ful', 'car@@', 'e@@', 'ful', 'car@@', 'e@@', 'ful', 'for', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 04:51:15,592 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:51:15,592 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:51:15,592 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quickly careful careful careful for the last 25 years.
2023-06-03 04:51:59,308 - INFO - joeynmt.training - Epoch   4, Step:    13600, Batch Loss:     1.496798, Batch Acc: 0.522453, Tokens per Sec:     1626, Lr: 0.000300
2023-06-03 04:52:43,834 - INFO - joeynmt.training - Epoch   4, Step:    13700, Batch Loss:     1.788007, Batch Acc: 0.524903, Tokens per Sec:     1570, Lr: 0.000300
2023-06-03 04:53:28,203 - INFO - joeynmt.training - Epoch   4, Step:    13800, Batch Loss:     1.809237, Batch Acc: 0.522987, Tokens per Sec:     1591, Lr: 0.000300
2023-06-03 04:54:11,729 - INFO - joeynmt.training - Epoch   4, Step:    13900, Batch Loss:     1.675706, Batch Acc: 0.520490, Tokens per Sec:     1592, Lr: 0.000300
2023-06-03 04:54:55,273 - INFO - joeynmt.training - Epoch   4, Step:    14000, Batch Loss:     1.684525, Batch Acc: 0.525601, Tokens per Sec:     1663, Lr: 0.000300
2023-06-03 04:54:55,273 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 04:57:56,612 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.89, ppl:   6.59, acc:   0.48, generation: 181.3213[sec], evaluation: 0.0000[sec]
2023-06-03 04:57:56,612 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 04:57:56,861 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/11500.ckpt
2023-06-03 04:57:56,864 - INFO - joeynmt.training - Example #0
2023-06-03 04:57:56,864 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 04:57:56,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 04:57:56,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'c@@', 'al@@', 'c@@', 'ul@@', 'ar@@', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'of', 'the', '4@@', '0@@', '-@@', 'y@@', 'ear@@', '-@@', 'old', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'cor@@', 'd@@', '-@@', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2023-06-03 04:57:56,865 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 04:57:56,865 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 04:57:56,865 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides for demonstrate to show that the glacial calcular, which is about three million years of the 40-year-old United States, it's record-40-percent.
2023-06-03 04:57:56,865 - INFO - joeynmt.training - Example #1
2023-06-03 04:57:56,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 04:57:56,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 04:57:56,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'a', 'wh@@', 'ol@@', 'e', 'thing', 'be@@', 'cause', 'it', 'is', 'not', 'the', 'big', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'it', 'ex@@', 'ist@@', 's', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'gl@@', 'ac@@', 'i@@', 'er.', '</s>']
2023-06-03 04:57:56,866 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 04:57:56,866 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 04:57:56,866 - INFO - joeynmt.training - 	Hypothesis: And this is a whole thing because it is not the big problem because it doesn't show it exists the ice of the ice of the ice of the ice of the glacier.
2023-06-03 04:57:56,866 - INFO - joeynmt.training - Example #2
2023-06-03 04:57:56,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 04:57:56,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 04:57:56,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is,', 'in', 'a', 'sense', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 04:57:56,867 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 04:57:56,867 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 04:57:56,867 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is, in a sense of the climate system.
2023-06-03 04:57:56,867 - INFO - joeynmt.training - Example #3
2023-06-03 04:57:56,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 04:57:56,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 04:57:56,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'was', 'go@@', 'ing', 'to', 'be', 'the', 'h@@', 'it@@', 'ting', 'and', 'you', 'w@@', 'ould', 'be', 'ext@@', 'rem@@', 'ely', 're@@', 'ally', 'thin@@', 'k', 'of', 'the', 'est@@', 'at@@', '.', '</s>']
2023-06-03 04:57:56,867 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 04:57:56,868 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 04:57:56,868 - INFO - joeynmt.training - 	Hypothesis: It was going to be the hitting and you would be extremely really think of the estat.
2023-06-03 04:57:56,868 - INFO - joeynmt.training - Example #4
2023-06-03 04:57:56,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 04:57:56,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 04:57:56,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'er', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 04:57:56,868 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 04:57:56,868 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 04:57:56,868 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly career of the last 25 years.
2023-06-03 04:58:40,941 - INFO - joeynmt.training - Epoch   4, Step:    14100, Batch Loss:     1.701372, Batch Acc: 0.527377, Tokens per Sec:     1598, Lr: 0.000300
2023-06-03 04:59:25,184 - INFO - joeynmt.training - Epoch   4, Step:    14200, Batch Loss:     1.656787, Batch Acc: 0.519660, Tokens per Sec:     1570, Lr: 0.000300
2023-06-03 05:00:07,763 - INFO - joeynmt.training - Epoch   4, Step:    14300, Batch Loss:     1.519768, Batch Acc: 0.525823, Tokens per Sec:     1663, Lr: 0.000300
2023-06-03 05:00:51,229 - INFO - joeynmt.training - Epoch   4, Step:    14400, Batch Loss:     1.684179, Batch Acc: 0.529990, Tokens per Sec:     1605, Lr: 0.000300
2023-06-03 05:01:34,180 - INFO - joeynmt.training - Epoch   4, Step:    14500, Batch Loss:     1.675534, Batch Acc: 0.529675, Tokens per Sec:     1586, Lr: 0.000300
2023-06-03 05:01:34,180 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 05:04:55,939 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.88, ppl:   6.57, acc:   0.48, generation: 201.7421[sec], evaluation: 0.0000[sec]
2023-06-03 05:04:55,939 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 05:04:56,161 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/12000.ckpt
2023-06-03 05:04:56,164 - INFO - joeynmt.training - Example #0
2023-06-03 05:04:56,164 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 05:04:56,164 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 05:04:56,164 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'th@@', 'ese', 'sli@@', 'de', 'that', 'the', 'mi@@', 'dd@@', 'le', 'of', 'the', 'bot@@', 'to@@', 'm', 'of', 'the', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'a', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'al', 'contin@@', 'ent@@', 's,', 'it@@', "'s", 're@@', 'ally', 're@@', 'cor@@', 'd', 'of', 'the', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2023-06-03 05:04:56,165 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 05:04:56,165 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 05:04:56,165 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide these slide that the middle of the bottom of the artica, which is a three million years had the size of the 48 United States continal continents, it's really record of the 40-percent.
2023-06-03 05:04:56,165 - INFO - joeynmt.training - Example #1
2023-06-03 05:04:56,165 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 05:04:56,165 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 05:04:56,165 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'a', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'spac@@', 'e.', '</s>']
2023-06-03 05:04:56,166 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 05:04:56,166 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 05:04:56,166 - INFO - joeynmt.training - 	Hypothesis: But this is a gravity of the gravity of the problem because it doesn't show the space.
2023-06-03 05:04:56,166 - INFO - joeynmt.training - Example #2
2023-06-03 05:04:56,166 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 05:04:56,166 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 05:04:56,166 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 05:04:56,167 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 05:04:56,167 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 05:04:56,167 - INFO - joeynmt.training - 	Hypothesis: The artical glacial is in a sense, the heart of global climate system.
2023-06-03 05:04:56,167 - INFO - joeynmt.training - Example #3
2023-06-03 05:04:56,167 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 05:04:56,167 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 05:04:56,167 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'it', 'ex@@', 'p@@', 'and@@', 'ing', 'from', 'the', 'h@@', 'and', 'you', 'w@@', 'ould', 'come', 'to', 'the', 'h@@', 'and', 'it', 'on', 'the', 'h@@', 'air@@', '.', '</s>']
2023-06-03 05:04:56,167 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 05:04:56,167 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 05:04:56,167 - INFO - joeynmt.training - 	Hypothesis: And it expanding from the hand you would come to the hand it on the hair.
2023-06-03 05:04:56,168 - INFO - joeynmt.training - Example #4
2023-06-03 05:04:56,168 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 05:04:56,168 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 05:04:56,168 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'f@@', 'ast@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 05:04:56,168 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 05:04:56,168 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 05:04:56,168 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly faster on the last 25 years.
2023-06-03 05:05:41,138 - INFO - joeynmt.training - Epoch   4, Step:    14600, Batch Loss:     1.796651, Batch Acc: 0.525632, Tokens per Sec:     1552, Lr: 0.000300
2023-06-03 05:06:23,913 - INFO - joeynmt.training - Epoch   4, Step:    14700, Batch Loss:     1.628554, Batch Acc: 0.525429, Tokens per Sec:     1646, Lr: 0.000300
2023-06-03 05:07:08,395 - INFO - joeynmt.training - Epoch   4, Step:    14800, Batch Loss:     1.575377, Batch Acc: 0.528581, Tokens per Sec:     1563, Lr: 0.000300
2023-06-03 05:07:52,812 - INFO - joeynmt.training - Epoch   4, Step:    14900, Batch Loss:     1.825998, Batch Acc: 0.525410, Tokens per Sec:     1622, Lr: 0.000300
2023-06-03 05:08:36,869 - INFO - joeynmt.training - Epoch   4, Step:    15000, Batch Loss:     1.753178, Batch Acc: 0.531113, Tokens per Sec:     1580, Lr: 0.000300
2023-06-03 05:08:36,869 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 05:12:03,846 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.86, ppl:   6.42, acc:   0.48, generation: 206.9569[sec], evaluation: 0.0000[sec]
2023-06-03 05:12:03,848 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 05:12:04,098 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/12500.ckpt
2023-06-03 05:12:04,101 - INFO - joeynmt.training - Example #0
2023-06-03 05:12:04,101 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 05:12:04,101 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 05:12:04,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'de@@', 'vic@@', 'es', 'of', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ot@@', 'h', '--', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'se@@', 'a', 'to', 'three', 'million', 'years', 'of', 'the', 'United', 'St@@', 'ates', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 05:12:04,102 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 05:12:04,102 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 05:12:04,102 - INFO - joeynmt.training - 	Hypothesis: The year I showed these devices of demonstrate that the glacial calototh -- that the glacial sea to three million years of the United States has been the size of the 40.
2023-06-03 05:12:04,102 - INFO - joeynmt.training - Example #1
2023-06-03 05:12:04,102 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 05:12:04,102 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 05:12:04,102 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'ex@@', 'hi@@', 'p@@', '.', '</s>']
2023-06-03 05:12:04,103 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 05:12:04,103 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 05:12:04,103 - INFO - joeynmt.training - 	Hypothesis: But the gravity of the gravity of the problem because it doesn't exhip.
2023-06-03 05:12:04,103 - INFO - joeynmt.training - Example #2
2023-06-03 05:12:04,103 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 05:12:04,103 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 05:12:04,103 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'ot@@', 'h', 'is', '--', 'in', 'a', 'sense', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 05:12:04,104 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 05:12:04,104 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 05:12:04,104 - INFO - joeynmt.training - 	Hypothesis: The glacial heoth is -- in a sense of the global climate system.
2023-06-03 05:12:04,104 - INFO - joeynmt.training - Example #3
2023-06-03 05:12:04,104 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 05:12:04,104 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 05:12:04,104 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'the', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'qui@@', 'red', 'of', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 05:12:04,104 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 05:12:04,105 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 05:12:04,105 - INFO - joeynmt.training - 	Hypothesis: It is the expanding and the really really required of the summer.
2023-06-03 05:12:04,105 - INFO - joeynmt.training - Example #4
2023-06-03 05:12:04,105 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 05:12:04,105 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 05:12:04,105 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'pro@@', 'mi@@', 'se', 'pro@@', 'si@@', 'm', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'car@@', 'r@@', 'i@@', 'ed', 'pa@@', 'per@@', 's', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 05:12:04,105 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 05:12:04,105 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 05:12:04,105 - INFO - joeynmt.training - 	Hypothesis: The promise prosim is going to be a fast-carried papers of the last 25 years.
2023-06-03 05:12:48,986 - INFO - joeynmt.training - Epoch   4, Step:    15100, Batch Loss:     1.798176, Batch Acc: 0.526218, Tokens per Sec:     1558, Lr: 0.000300
2023-06-03 05:13:24,502 - INFO - joeynmt.training - Epoch   4: total training loss 6439.35
2023-06-03 05:13:24,502 - INFO - joeynmt.training - EPOCH 5
2023-06-03 05:13:32,520 - INFO - joeynmt.training - Epoch   5, Step:    15200, Batch Loss:     1.560281, Batch Acc: 0.546120, Tokens per Sec:     1569, Lr: 0.000300
2023-06-03 05:14:16,526 - INFO - joeynmt.training - Epoch   5, Step:    15300, Batch Loss:     1.544058, Batch Acc: 0.549983, Tokens per Sec:     1601, Lr: 0.000300
2023-06-03 05:14:59,857 - INFO - joeynmt.training - Epoch   5, Step:    15400, Batch Loss:     1.601061, Batch Acc: 0.554475, Tokens per Sec:     1625, Lr: 0.000300
2023-06-03 05:15:44,625 - INFO - joeynmt.training - Epoch   5, Step:    15500, Batch Loss:     1.609790, Batch Acc: 0.550511, Tokens per Sec:     1561, Lr: 0.000300
2023-06-03 05:15:44,625 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 05:19:03,762 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.87, ppl:   6.46, acc:   0.48, generation: 199.1204[sec], evaluation: 0.0000[sec]
2023-06-03 05:19:03,999 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/13000.ckpt
2023-06-03 05:19:04,001 - INFO - joeynmt.training - Example #0
2023-06-03 05:19:04,001 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 05:19:04,001 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 05:19:04,001 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 've', 'd@@', 'i@@', 've', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ter', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'a', 'gl@@', 'ac@@', 'i@@', 'al', 'war@@', 'm@@', 'ing', 'the', 'mi@@', 'dd@@', 'le', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', 'U.@@', 'S.', 'contin@@', 'ent@@', 's,', 'you', 'know,', 'it@@', "'s", 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'st@@', 'a@@', 'ur@@', 't', 'of', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 05:19:04,002 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 05:19:04,002 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 05:19:04,002 - INFO - joeynmt.training - 	Hypothesis: The year I showed these dive dive that the glacial calotter artica, which is a glacial warming the middle of the 48 percent of the U.S. continents, you know, it's really really really really really restaurt of the 40.
2023-06-03 05:19:04,002 - INFO - joeynmt.training - Example #1
2023-06-03 05:19:04,002 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 05:19:04,002 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 05:19:04,002 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 's@@', 'l@@', 'ate', 'be@@', 'cause', 'it', "doesn't", 'have', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'ex@@', 'hi@@', 'b@@', 'it@@', 'ion.', '</s>']
2023-06-03 05:19:04,003 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 05:19:04,003 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 05:19:04,003 - INFO - joeynmt.training - 	Hypothesis: And this slate because it doesn't have the gravity of the problem because it doesn't show the ice of the ice exhibition.
2023-06-03 05:19:04,003 - INFO - joeynmt.training - Example #2
2023-06-03 05:19:04,003 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 05:19:04,003 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 05:19:04,003 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'clim@@', 'ate', 'cu@@', 'st@@', 'om@@', 'p@@', 'ing', 'system', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 05:19:04,004 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 05:19:04,004 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 05:19:04,004 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is in a sense of the climate customping system of global climate system.
2023-06-03 05:19:04,004 - INFO - joeynmt.training - Example #3
2023-06-03 05:19:04,004 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 05:19:04,004 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 05:19:04,004 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'know,', 'the', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'you', 'w@@', 'ould', 'be', 'ext@@', 'rem@@', 'ely', 's@@', 'lo@@', 't.', '</s>']
2023-06-03 05:19:04,004 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 05:19:04,004 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 05:19:04,005 - INFO - joeynmt.training - 	Hypothesis: You know, the expanding and you would be extremely slot.
2023-06-03 05:19:04,005 - INFO - joeynmt.training - Example #4
2023-06-03 05:19:04,005 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 05:19:04,005 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 05:19:04,005 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'bo@@', 'ard', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 05:19:04,005 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 05:19:04,005 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 05:19:04,005 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly careboard of the last 25 years.
2023-06-03 05:19:47,618 - INFO - joeynmt.training - Epoch   5, Step:    15600, Batch Loss:     1.626546, Batch Acc: 0.549431, Tokens per Sec:     1620, Lr: 0.000300
2023-06-03 05:20:34,005 - INFO - joeynmt.training - Epoch   5, Step:    15700, Batch Loss:     1.485726, Batch Acc: 0.544721, Tokens per Sec:     1510, Lr: 0.000300
2023-06-03 05:21:17,497 - INFO - joeynmt.training - Epoch   5, Step:    15800, Batch Loss:     1.559247, Batch Acc: 0.546704, Tokens per Sec:     1622, Lr: 0.000300
2023-06-03 05:22:01,766 - INFO - joeynmt.training - Epoch   5, Step:    15900, Batch Loss:     1.512171, Batch Acc: 0.550157, Tokens per Sec:     1603, Lr: 0.000300
2023-06-03 05:22:47,035 - INFO - joeynmt.training - Epoch   5, Step:    16000, Batch Loss:     1.746133, Batch Acc: 0.546927, Tokens per Sec:     1561, Lr: 0.000300
2023-06-03 05:22:47,035 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 05:26:05,350 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.85, ppl:   6.35, acc:   0.49, generation: 198.2988[sec], evaluation: 0.0000[sec]
2023-06-03 05:26:05,351 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 05:26:05,572 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/13500.ckpt
2023-06-03 05:26:05,575 - INFO - joeynmt.training - Example #0
2023-06-03 05:26:05,576 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 05:26:05,576 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 05:26:05,576 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'de@@', 'mon@@', 'str@@', 'ate', 'd@@', 'i@@', 've', 'for', 'd@@', 'i@@', 'mo@@', 'tion', 'for', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'd@@', 'i@@', 'st@@', 'ant', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', 'it@@', "'s", 're@@', 'ally', 're@@', 'st@@', 'a@@', 'ur@@', 'ing', 'the', '4@@', '0@@', 's.', '</s>']
2023-06-03 05:26:05,576 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 05:26:05,576 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 05:26:05,576 - INFO - joeynmt.training - 	Hypothesis: The year I showed these demonstrate dive for dimotion for glacial glacial distant years had the size of 48 percent of the United States continental continents, it's really restauring the 40s.
2023-06-03 05:26:05,576 - INFO - joeynmt.training - Example #1
2023-06-03 05:26:05,577 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 05:26:05,577 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 05:26:05,577 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'is@@', 'sue', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 05:26:05,577 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 05:26:05,577 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 05:26:05,577 - INFO - joeynmt.training - 	Hypothesis: However, the gravity of the issue because it doesn't show the ice because it doesn't show the ice of the ice of the ice of the ice of the ice of the ice.
2023-06-03 05:26:05,578 - INFO - joeynmt.training - Example #2
2023-06-03 05:26:05,578 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 05:26:05,578 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 05:26:05,578 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'c@@', 'tic@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'ar@@', 'c@@', 'tic@@', 'ul@@', 'ate', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 05:26:05,578 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 05:26:05,578 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 05:26:05,578 - INFO - joeynmt.training - 	Hypothesis: The arctical glacial is in a sense of the arcticulate climate system.
2023-06-03 05:26:05,578 - INFO - joeynmt.training - Example #3
2023-06-03 05:26:05,579 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 05:26:05,579 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 05:26:05,579 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'from', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 05:26:05,579 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 05:26:05,579 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 05:26:05,579 - INFO - joeynmt.training - 	Hypothesis: It expands from the summer.
2023-06-03 05:26:05,579 - INFO - joeynmt.training - Example #4
2023-06-03 05:26:05,580 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 05:26:05,580 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 05:26:05,580 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 05:26:05,580 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 05:26:05,580 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 05:26:05,580 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick career on the last 25 years of the last 25 years.
2023-06-03 05:26:49,108 - INFO - joeynmt.training - Epoch   5, Step:    16100, Batch Loss:     1.798276, Batch Acc: 0.543409, Tokens per Sec:     1617, Lr: 0.000300
2023-06-03 05:27:32,903 - INFO - joeynmt.training - Epoch   5, Step:    16200, Batch Loss:     1.558770, Batch Acc: 0.543649, Tokens per Sec:     1605, Lr: 0.000300
2023-06-03 05:28:17,855 - INFO - joeynmt.training - Epoch   5, Step:    16300, Batch Loss:     1.628567, Batch Acc: 0.545841, Tokens per Sec:     1574, Lr: 0.000300
2023-06-03 05:29:01,955 - INFO - joeynmt.training - Epoch   5, Step:    16400, Batch Loss:     1.739658, Batch Acc: 0.544403, Tokens per Sec:     1572, Lr: 0.000300
2023-06-03 05:29:44,754 - INFO - joeynmt.training - Epoch   5, Step:    16500, Batch Loss:     1.562356, Batch Acc: 0.542451, Tokens per Sec:     1626, Lr: 0.000300
2023-06-03 05:29:44,754 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 05:32:52,964 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.84, ppl:   6.29, acc:   0.49, generation: 188.1933[sec], evaluation: 0.0000[sec]
2023-06-03 05:32:52,964 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 05:32:53,195 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/14000.ckpt
2023-06-03 05:32:53,198 - INFO - joeynmt.training - Example #0
2023-06-03 05:32:53,198 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 05:32:53,198 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 05:32:53,198 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'for', 'a', 'sli@@', 'de', 'for', 'd@@', 'i@@', 'ving', 'that', 'the', 'cal@@', 'ot@@', 'ti@@', 'c', 'ar@@', 'tic@@', 'a,', 'which', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'in', 'the', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', 'and', 'the', 'bot@@', 'to@@', 'm', 'of', '4@@', '0@@', '.', '</s>']
2023-06-03 05:32:53,199 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 05:32:53,199 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 05:32:53,199 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides for a slide for diving that the calottic artica, which for almost three million years had the size 48 United States continental continental continental continental in the U.S. continental continents, and the bottom of 40.
2023-06-03 05:32:53,199 - INFO - joeynmt.training - Example #1
2023-06-03 05:32:53,199 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 05:32:53,199 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 05:32:53,199 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 's@@', 'low', 'be@@', 'low', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'is@@', 'sue', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'be@@', 'cause', 'it', 'is', 'not', 'the', 'ice', 'be@@', 'cause', 'it', 'is', 'not', 'the', 'ice', 'is', 'the', 'gl@@', 'ac@@', 't.', '</s>']
2023-06-03 05:32:53,200 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 05:32:53,200 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 05:32:53,200 - INFO - joeynmt.training - 	Hypothesis: But this slow below the gravity of the gravity of the ice because it doesn't show the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice issue of the ice of the ice of the ice of the ice because it is not the ice because it is not the ice is the glact.
2023-06-03 05:32:53,200 - INFO - joeynmt.training - Example #2
2023-06-03 05:32:53,200 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 05:32:53,200 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 05:32:53,200 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'ar@@', 'tic@@', 'al', 'cu@@', 'st@@', 'om@@', 'ing', 'system', 'of', 'the', 'global', 'clim@@', 'ate', 'system', 'of', 'global', 'clim@@', 'ate', 'system', 'of', 'global', 'system@@', '.', '</s>']
2023-06-03 05:32:53,201 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 05:32:53,201 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 05:32:53,201 - INFO - joeynmt.training - 	Hypothesis: The art glacial is in a sense of the artical customing system of the global climate system of global climate system of global system.
2023-06-03 05:32:53,201 - INFO - joeynmt.training - Example #3
2023-06-03 05:32:53,201 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 05:32:53,201 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 05:32:53,201 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'and', 'in@@', 'ver@@', 'se', 'and', 'you', 'w@@', 'ould', 'be', 'ext@@', 'rem@@', 'e.', '</s>']
2023-06-03 05:32:53,201 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 05:32:53,202 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 05:32:53,202 - INFO - joeynmt.training - 	Hypothesis: It expands and inverse and you would be extreme.
2023-06-03 05:32:53,202 - INFO - joeynmt.training - Example #4
2023-06-03 05:32:53,202 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 05:32:53,202 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 05:32:53,202 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'full@@', 'y', 'car@@', 'e@@', 'bo@@', 'ard', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 05:32:53,202 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 05:32:53,202 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 05:32:53,202 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly carefully careboard of the last 25 years.
2023-06-03 05:33:37,196 - INFO - joeynmt.training - Epoch   5, Step:    16600, Batch Loss:     1.542940, Batch Acc: 0.543891, Tokens per Sec:     1573, Lr: 0.000300
2023-06-03 05:34:21,029 - INFO - joeynmt.training - Epoch   5, Step:    16700, Batch Loss:     1.656156, Batch Acc: 0.544403, Tokens per Sec:     1630, Lr: 0.000300
2023-06-03 05:35:03,738 - INFO - joeynmt.training - Epoch   5, Step:    16800, Batch Loss:     1.425708, Batch Acc: 0.546358, Tokens per Sec:     1674, Lr: 0.000300
2023-06-03 05:35:47,891 - INFO - joeynmt.training - Epoch   5, Step:    16900, Batch Loss:     1.578660, Batch Acc: 0.541835, Tokens per Sec:     1589, Lr: 0.000300
2023-06-03 05:36:30,546 - INFO - joeynmt.training - Epoch   5, Step:    17000, Batch Loss:     1.497045, Batch Acc: 0.549726, Tokens per Sec:     1636, Lr: 0.000300
2023-06-03 05:36:30,547 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 05:39:36,706 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.25, acc:   0.49, generation: 186.1437[sec], evaluation: 0.0000[sec]
2023-06-03 05:39:36,709 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 05:39:36,928 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/14500.ckpt
2023-06-03 05:39:36,930 - INFO - joeynmt.training - Example #0
2023-06-03 05:39:36,931 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 05:39:36,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 05:39:36,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'c@@', 'al@@', 'c@@', 'ul@@', 'ate', 'c@@', 'al@@', 'c@@', 'ul@@', 'ar@@', 'y@@', ',', 'which', 'is', 'about', 'three', 'million', 'years', 'of', 'years', 'of', 'the', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'it@@', "'s", 're@@', 'st@@', 'ing', 'in', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 05:39:36,931 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 05:39:36,931 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 05:39:36,931 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides for demonstrate that calculate calculary, which is about three million years of years of the 48 percent of the United States continents, it's resting in the 40.
2023-06-03 05:39:36,931 - INFO - joeynmt.training - Example #1
2023-06-03 05:39:36,932 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 05:39:36,932 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 05:39:36,932 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'ex@@', 'hi@@', 'b@@', 'it@@', 'e.', '</s>']
2023-06-03 05:39:36,932 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 05:39:36,932 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 05:39:36,932 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't exhibite.
2023-06-03 05:39:36,932 - INFO - joeynmt.training - Example #2
2023-06-03 05:39:36,932 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 05:39:36,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 05:39:36,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 't@@', 'ical', 'cu@@', 'p@@', '.', '</s>']
2023-06-03 05:39:36,933 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 05:39:36,933 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 05:39:36,933 - INFO - joeynmt.training - 	Hypothesis: The artical glacial artical cup.
2023-06-03 05:39:36,933 - INFO - joeynmt.training - Example #3
2023-06-03 05:39:36,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 05:39:36,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 05:39:36,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'ex@@', 'p@@', 'and@@', 's', 'and', 'it', 'comes', 'to', 'the', 'w@@', 'r@@', 'i@@', 'ght.', '</s>']
2023-06-03 05:39:36,934 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 05:39:36,934 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 05:39:36,934 - INFO - joeynmt.training - 	Hypothesis: It is expands and it comes to the wright.
2023-06-03 05:39:36,934 - INFO - joeynmt.training - Example #4
2023-06-03 05:39:36,934 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 05:39:36,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 05:39:36,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'f@@', 'ast', 'car@@', 'e@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 05:39:36,935 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 05:39:36,935 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 05:39:36,935 - INFO - joeynmt.training - 	Hypothesis: The next slide slide is going to be a quickly fast career on the last 25 years.
2023-06-03 05:40:19,361 - INFO - joeynmt.training - Epoch   5, Step:    17100, Batch Loss:     1.576830, Batch Acc: 0.547006, Tokens per Sec:     1679, Lr: 0.000300
2023-06-03 05:41:02,809 - INFO - joeynmt.training - Epoch   5, Step:    17200, Batch Loss:     1.586731, Batch Acc: 0.542211, Tokens per Sec:     1624, Lr: 0.000300
2023-06-03 05:41:45,346 - INFO - joeynmt.training - Epoch   5, Step:    17300, Batch Loss:     1.464500, Batch Acc: 0.545905, Tokens per Sec:     1661, Lr: 0.000300
2023-06-03 05:42:29,124 - INFO - joeynmt.training - Epoch   5, Step:    17400, Batch Loss:     1.601708, Batch Acc: 0.548878, Tokens per Sec:     1612, Lr: 0.000300
2023-06-03 05:43:11,520 - INFO - joeynmt.training - Epoch   5, Step:    17500, Batch Loss:     1.670711, Batch Acc: 0.546903, Tokens per Sec:     1648, Lr: 0.000300
2023-06-03 05:43:11,521 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 05:46:31,522 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.83, ppl:   6.22, acc:   0.49, generation: 199.9845[sec], evaluation: 0.0000[sec]
2023-06-03 05:46:31,527 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 05:46:31,752 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/15500.ckpt
2023-06-03 05:46:31,755 - INFO - joeynmt.training - Example #0
2023-06-03 05:46:31,755 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 05:46:31,755 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 05:46:31,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'pp@@', 'ed', 'sli@@', 'de', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'e,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'been', 're@@', 'cor@@', 'd,', 'which', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'the', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'which', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', '4@@', '0@@', '.', '</s>']
2023-06-03 05:46:31,756 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 05:46:31,756 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 05:46:31,756 - INFO - joeynmt.training - 	Hypothesis: And I showed these slipped slide for demonstrate to show that the glacial calote, which is about three million years had been record, which is restretched the United States continents, which is restretched 40.
2023-06-03 05:46:31,756 - INFO - joeynmt.training - Example #1
2023-06-03 05:46:31,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 05:46:31,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 05:46:31,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'be@@', 'low', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', 'show@@', 's', 'not', 'the', 'ice', 'ex@@', 'hi@@', 'b@@', 'it@@', 'e.', '</s>']
2023-06-03 05:46:31,756 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 05:46:31,756 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 05:46:31,757 - INFO - joeynmt.training - 	Hypothesis: But this underestimate below the gravity of the problem because it shows not the ice exhibite.
2023-06-03 05:46:31,757 - INFO - joeynmt.training - Example #2
2023-06-03 05:46:31,757 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 05:46:31,757 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 05:46:31,757 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'g@@', 'lob@@', 'al', 'system@@', '.', '</s>']
2023-06-03 05:46:31,757 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 05:46:31,757 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 05:46:31,757 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is, in a sense, the heart of the global system.
2023-06-03 05:46:31,758 - INFO - joeynmt.training - Example #3
2023-06-03 05:46:31,758 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 05:46:31,758 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 05:46:31,758 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'ex@@', 'p@@', 'and@@', 's', 'and', 'you', 'w@@', 'ould', 'thin@@', 'k', 'of', 'ext@@', 'at@@', 'ely', 'ex@@', 'p@@', 'lo@@', 'y@@', 'ing.', '</s>']
2023-06-03 05:46:31,758 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 05:46:31,758 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 05:46:31,758 - INFO - joeynmt.training - 	Hypothesis: And the expands and you would think of extately exploying.
2023-06-03 05:46:31,758 - INFO - joeynmt.training - Example #4
2023-06-03 05:46:31,759 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 05:46:31,759 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 05:46:31,759 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'r@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 05:46:31,759 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 05:46:31,759 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 05:46:31,759 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly carered on the last 25 years.
2023-06-03 05:47:15,099 - INFO - joeynmt.training - Epoch   5, Step:    17600, Batch Loss:     1.593101, Batch Acc: 0.545149, Tokens per Sec:     1613, Lr: 0.000300
2023-06-03 05:47:58,829 - INFO - joeynmt.training - Epoch   5, Step:    17700, Batch Loss:     1.558016, Batch Acc: 0.537429, Tokens per Sec:     1577, Lr: 0.000300
2023-06-03 05:48:41,648 - INFO - joeynmt.training - Epoch   5, Step:    17800, Batch Loss:     1.528228, Batch Acc: 0.542997, Tokens per Sec:     1642, Lr: 0.000300
2023-06-03 05:49:25,264 - INFO - joeynmt.training - Epoch   5, Step:    17900, Batch Loss:     1.651037, Batch Acc: 0.543147, Tokens per Sec:     1617, Lr: 0.000300
2023-06-03 05:50:09,533 - INFO - joeynmt.training - Epoch   5, Step:    18000, Batch Loss:     1.516910, Batch Acc: 0.544259, Tokens per Sec:     1586, Lr: 0.000300
2023-06-03 05:50:09,534 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 05:53:49,495 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.82, ppl:   6.19, acc:   0.49, generation: 219.9449[sec], evaluation: 0.0000[sec]
2023-06-03 05:53:49,496 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 05:53:49,725 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/15000.ckpt
2023-06-03 05:53:49,728 - INFO - joeynmt.training - Example #0
2023-06-03 05:53:49,728 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 05:53:49,728 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 05:53:49,728 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'for', 'de@@', 'mon@@', 'str@@', 'ing', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ta', '--', 'that', 'for', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', '4@@', '8', 'to', 'the', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'is', 're@@', 'a@@', 'king', 'for', '4@@', '0@@', '.', '</s>']
2023-06-03 05:53:49,729 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 05:53:49,729 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 05:53:49,729 - INFO - joeynmt.training - 	Hypothesis: And I showed these slides to show these slides for demonstring that the glacial calotta -- that for three million years had the size 48 dimensions of 48 to the United States continental continental continental is reaking for 40.
2023-06-03 05:53:49,729 - INFO - joeynmt.training - Example #1
2023-06-03 05:53:49,729 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 05:53:49,729 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 05:53:49,729 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 's@@', 'un@@', 't', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 05:53:49,730 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 05:53:49,730 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 05:53:49,730 - INFO - joeynmt.training - 	Hypothesis: But this sunt of the gravity of the problem because it doesn't show the ice of the ice.
2023-06-03 05:53:49,730 - INFO - joeynmt.training - Example #2
2023-06-03 05:53:49,730 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 05:53:49,730 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 05:53:49,730 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 05:53:49,730 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 05:53:49,731 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 05:53:49,731 - INFO - joeynmt.training - 	Hypothesis: The art glacial is in a sense, the heart of the climate system.
2023-06-03 05:53:49,731 - INFO - joeynmt.training - Example #3
2023-06-03 05:53:49,731 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 05:53:49,731 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 05:53:49,731 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 'the', 'in@@', 'ver@@', 'se', 'and', 'you', 'w@@', 'ro@@', 'te', 'and', 'it', 'tur@@', 'ns', 'out', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'it@@', 'ut@@', 's.', '</s>']
2023-06-03 05:53:49,731 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 05:53:49,731 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 05:53:49,731 - INFO - joeynmt.training - 	Hypothesis: You expand the inverse and you wrote and it turns out of the summer and ituts.
2023-06-03 05:53:49,731 - INFO - joeynmt.training - Example #4
2023-06-03 05:53:49,732 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 05:53:49,732 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 05:53:49,732 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'full@@', 'y', 'car@@', 'e@@', 'full@@', 'y', 'for', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 05:53:49,732 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 05:53:49,732 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 05:53:49,732 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a quick carefully carefully for the last 25 years.
2023-06-03 05:54:33,404 - INFO - joeynmt.training - Epoch   5, Step:    18100, Batch Loss:     1.663970, Batch Acc: 0.545643, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 05:55:17,579 - INFO - joeynmt.training - Epoch   5, Step:    18200, Batch Loss:     1.645584, Batch Acc: 0.545261, Tokens per Sec:     1614, Lr: 0.000300
2023-06-03 05:56:00,884 - INFO - joeynmt.training - Epoch   5, Step:    18300, Batch Loss:     1.423273, Batch Acc: 0.546978, Tokens per Sec:     1644, Lr: 0.000300
2023-06-03 05:56:44,144 - INFO - joeynmt.training - Epoch   5, Step:    18400, Batch Loss:     1.713097, Batch Acc: 0.545980, Tokens per Sec:     1643, Lr: 0.000300
2023-06-03 05:57:28,110 - INFO - joeynmt.training - Epoch   5, Step:    18500, Batch Loss:     1.648066, Batch Acc: 0.548980, Tokens per Sec:     1616, Lr: 0.000300
2023-06-03 05:57:28,110 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:00:36,558 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.10, acc:   0.49, generation: 188.4295[sec], evaluation: 0.0000[sec]
2023-06-03 06:00:36,559 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 06:00:36,788 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/16000.ckpt
2023-06-03 06:00:36,791 - INFO - joeynmt.training - Example #0
2023-06-03 06:00:36,791 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:00:36,791 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:00:36,791 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'si@@', 've', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'ad', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ter', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'cor@@', 't@@', 'ed.', '</s>']
2023-06-03 06:00:36,792 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:00:36,792 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:00:36,792 - INFO - joeynmt.training - 	Hypothesis: The year I showed these diasive that the glacial head that the glacial calotter artica, which is almost three million years had the size of 48 the size of 48 United States, it's recorted.
2023-06-03 06:00:36,792 - INFO - joeynmt.training - Example #1
2023-06-03 06:00:36,792 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:00:36,792 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:00:36,792 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 's@@', 'ite', 'be@@', 'low', 'the', 'wh@@', 'ol@@', 'e', 'problem', 'be@@', 'cause', 'it', "doesn't", 'ex@@', 'hi@@', 'b@@', 'ition', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 06:00:36,793 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:00:36,793 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:00:36,793 - INFO - joeynmt.training - 	Hypothesis: But this site below the whole problem because it doesn't exhibition of the ice.
2023-06-03 06:00:36,793 - INFO - joeynmt.training - Example #2
2023-06-03 06:00:36,793 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:00:36,793 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:00:36,793 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ro@@', 'wn', 'is', 'the', 'ar@@', 'c@@', 't', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'pul@@', 's@@', 'si@@', 'on', 'of', 'global', 'system@@', '.', '</s>']
2023-06-03 06:00:36,793 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:00:36,793 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:00:36,794 - INFO - joeynmt.training - 	Hypothesis: The arthrown is the arct is, in a sense, the pulssion of global system.
2023-06-03 06:00:36,794 - INFO - joeynmt.training - Example #3
2023-06-03 06:00:36,794 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:00:36,794 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:00:36,794 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'it', 'ex@@', 'p@@', 'and@@', 's', 'and', 'you', 'w@@', 'ould', 'come', 'u@@', 'p', 'and', 'you', 'w@@', 'ould', 'ex@@', 'p@@', 'lo@@', 'se@@', 'd.', '</s>']
2023-06-03 06:00:36,794 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:00:36,794 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:00:36,794 - INFO - joeynmt.training - 	Hypothesis: And it expands and you would come up and you would explosed.
2023-06-03 06:00:36,794 - INFO - joeynmt.training - Example #4
2023-06-03 06:00:36,795 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:00:36,795 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:00:36,795 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:00:36,795 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:00:36,795 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:00:36,795 - INFO - joeynmt.training - 	Hypothesis: The next devices will be a quick career on the last 25 years.
2023-06-03 06:01:20,408 - INFO - joeynmt.training - Epoch   5, Step:    18600, Batch Loss:     1.720513, Batch Acc: 0.544641, Tokens per Sec:     1566, Lr: 0.000300
2023-06-03 06:02:03,117 - INFO - joeynmt.training - Epoch   5, Step:    18700, Batch Loss:     1.484998, Batch Acc: 0.544900, Tokens per Sec:     1653, Lr: 0.000300
2023-06-03 06:02:46,681 - INFO - joeynmt.training - Epoch   5, Step:    18800, Batch Loss:     1.851121, Batch Acc: 0.545327, Tokens per Sec:     1636, Lr: 0.000300
2023-06-03 06:03:30,685 - INFO - joeynmt.training - Epoch   5, Step:    18900, Batch Loss:     1.565520, Batch Acc: 0.546736, Tokens per Sec:     1593, Lr: 0.000300
2023-06-03 06:04:01,407 - INFO - joeynmt.training - Epoch   5: total training loss 6062.90
2023-06-03 06:04:01,407 - INFO - joeynmt.training - EPOCH 6
2023-06-03 06:04:13,766 - INFO - joeynmt.training - Epoch   6, Step:    19000, Batch Loss:     1.342156, Batch Acc: 0.573949, Tokens per Sec:     1587, Lr: 0.000300
2023-06-03 06:04:13,766 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:07:26,624 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.06, acc:   0.50, generation: 192.8401[sec], evaluation: 0.0000[sec]
2023-06-03 06:07:26,625 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 06:07:26,860 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/16500.ckpt
2023-06-03 06:07:26,863 - INFO - joeynmt.training - Example #0
2023-06-03 06:07:26,864 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:07:26,864 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:07:26,864 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'for', 'the', 'sli@@', 'de', 'of', 'sli@@', 'de', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'ad', 'of', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', '4@@', '8', 'United', 'St@@', 'ates', 'is', 're@@', 'qu@@', 'i@@', 're', 'the', 'si@@', 'ze', 'of', '4@@', '0@@', '.', '</s>']
2023-06-03 06:07:26,864 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:07:26,864 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:07:26,865 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slide for the slide of slide that the glacial head of artica, which is almost three million years had the size of the 48 dimensions of the 48 United States is require the size of 40.
2023-06-03 06:07:26,865 - INFO - joeynmt.training - Example #1
2023-06-03 06:07:26,865 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:07:26,865 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:07:26,865 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'be@@', 'cause', 'it', "doesn't", 'show', 'you', 'the', 'ice', 'of', 'the', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'you', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'you', 'the', 'gl@@', 'ac@@', 'y.', '</s>']
2023-06-03 06:07:26,865 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:07:26,865 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:07:26,865 - INFO - joeynmt.training - 	Hypothesis: But the gravity of the gravity of the gravity because it doesn't show you the ice of the ice because it doesn't show you the ice of the ice of the ice of the ice of the ice because it doesn't show you the glacy.
2023-06-03 06:07:26,866 - INFO - joeynmt.training - Example #2
2023-06-03 06:07:26,866 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:07:26,866 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:07:26,866 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'le', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'global', 'clim@@', 'ate', '</s>']
2023-06-03 06:07:26,866 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:07:26,866 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:07:26,866 - INFO - joeynmt.training - 	Hypothesis: The article glacial is in a sense of the global climate
2023-06-03 06:07:26,866 - INFO - joeynmt.training - Example #3
2023-06-03 06:07:26,867 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:07:26,867 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:07:26,867 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'was', 's@@', 'w@@', 'ing', 'out', 'of', 'the', 'w@@', 'r@@', 'i@@', 'd', 'and', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'ex@@', 'p@@', 'and@@', 'ing', 'and', 'it', 'was', 'go@@', 'ing', 'to', 'be', 's@@', 'w@@', 'ing@@', 's.', '</s>']
2023-06-03 06:07:26,867 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:07:26,867 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:07:26,867 - INFO - joeynmt.training - 	Hypothesis: It was swing out of the wrid and really really really really really expanding and it was going to be swings.
2023-06-03 06:07:26,867 - INFO - joeynmt.training - Example #4
2023-06-03 06:07:26,868 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:07:26,868 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:07:26,868 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'of', 'positi@@', 've', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:07:26,868 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:07:26,868 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:07:26,868 - INFO - joeynmt.training - 	Hypothesis: The next slide of positive will be a quick career on the last 25 years.
2023-06-03 06:08:11,169 - INFO - joeynmt.training - Epoch   6, Step:    19100, Batch Loss:     1.543994, Batch Acc: 0.571618, Tokens per Sec:     1571, Lr: 0.000300
2023-06-03 06:08:56,961 - INFO - joeynmt.training - Epoch   6, Step:    19200, Batch Loss:     1.366136, Batch Acc: 0.573759, Tokens per Sec:     1578, Lr: 0.000300
2023-06-03 06:09:44,219 - INFO - joeynmt.training - Epoch   6, Step:    19300, Batch Loss:     1.434815, Batch Acc: 0.570515, Tokens per Sec:     1469, Lr: 0.000300
2023-06-03 06:10:30,356 - INFO - joeynmt.training - Epoch   6, Step:    19400, Batch Loss:     1.519824, Batch Acc: 0.564282, Tokens per Sec:     1561, Lr: 0.000300
2023-06-03 06:11:14,332 - INFO - joeynmt.training - Epoch   6, Step:    19500, Batch Loss:     1.384598, Batch Acc: 0.572448, Tokens per Sec:     1591, Lr: 0.000300
2023-06-03 06:11:14,332 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:14:54,718 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.81, ppl:   6.08, acc:   0.50, generation: 220.3696[sec], evaluation: 0.0000[sec]
2023-06-03 06:14:54,980 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/17000.ckpt
2023-06-03 06:14:54,984 - INFO - joeynmt.training - Example #0
2023-06-03 06:14:54,984 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:14:54,984 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:14:54,984 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'lo@@', 'ts', 'for', 'almost', 'three', 'million', 'years', 'had', 'been', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', '4@@', '8', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '0@@', 's', 'the', 'United', 'St@@', 'ates', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'in', 'the', 'United', 'St@@', 'at@@', 'es,', '</s>']
2023-06-03 06:14:54,984 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:14:54,985 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:14:54,985 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slide for demonstrate that the glacial lots for almost three million years had been the size of the 48 dimensions of the 48 the size of the 40s the United States is restretched in the United States,
2023-06-03 06:14:54,985 - INFO - joeynmt.training - Example #1
2023-06-03 06:14:54,985 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:14:54,985 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:14:54,985 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'a', 'wh@@', 'ol@@', 'e', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 06:14:54,985 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:14:54,986 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:14:54,986 - INFO - joeynmt.training - 	Hypothesis: But this is a whole problem because it doesn't show the gravity of the ice.
2023-06-03 06:14:54,986 - INFO - joeynmt.training - Example #2
2023-06-03 06:14:54,986 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:14:54,986 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:14:54,986 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'tic@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'global', 'cli@@', 'mat@@', 'e.', '</s>']
2023-06-03 06:14:54,986 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:14:54,986 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:14:54,987 - INFO - joeynmt.training - 	Hypothesis: The artical glacial is in a sense of the global climate.
2023-06-03 06:14:54,987 - INFO - joeynmt.training - Example #3
2023-06-03 06:14:54,987 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:14:54,987 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:14:54,987 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'was', 's@@', 'pre@@', 'ad@@', 's', 'and', 're@@', 'ally', 'ex@@', 'p@@', 'and@@', 'ing', 'out', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'it@@', "'s", 'the', 'ex@@', 'p@@', 'and@@', 's.', '</s>']
2023-06-03 06:14:54,987 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:14:54,987 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:14:54,987 - INFO - joeynmt.training - 	Hypothesis: It was spreads and really expanding out of the summer and the summer of the summer and it's the expands.
2023-06-03 06:14:54,988 - INFO - joeynmt.training - Example #4
2023-06-03 06:14:54,988 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:14:54,988 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:14:54,988 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:14:54,988 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:14:54,988 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:14:54,989 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly career on the last 25 years.
2023-06-03 06:15:40,057 - INFO - joeynmt.training - Epoch   6, Step:    19600, Batch Loss:     1.419916, Batch Acc: 0.562277, Tokens per Sec:     1527, Lr: 0.000300
2023-06-03 06:16:24,288 - INFO - joeynmt.training - Epoch   6, Step:    19700, Batch Loss:     1.479716, Batch Acc: 0.561016, Tokens per Sec:     1594, Lr: 0.000300
2023-06-03 06:17:07,964 - INFO - joeynmt.training - Epoch   6, Step:    19800, Batch Loss:     1.506789, Batch Acc: 0.558782, Tokens per Sec:     1608, Lr: 0.000300
2023-06-03 06:17:52,420 - INFO - joeynmt.training - Epoch   6, Step:    19900, Batch Loss:     1.489318, Batch Acc: 0.560357, Tokens per Sec:     1593, Lr: 0.000300
2023-06-03 06:18:35,794 - INFO - joeynmt.training - Epoch   6, Step:    20000, Batch Loss:     1.443229, Batch Acc: 0.565940, Tokens per Sec:     1595, Lr: 0.000300
2023-06-03 06:18:35,794 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:21:53,432 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.04, acc:   0.50, generation: 197.6202[sec], evaluation: 0.0000[sec]
2023-06-03 06:21:53,433 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 06:21:53,677 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/17500.ckpt
2023-06-03 06:21:53,680 - INFO - joeynmt.training - Example #0
2023-06-03 06:21:53,680 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:21:53,680 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:21:53,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'l', 'gl@@', 'ac@@', 'i@@', 'l', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', '4@@', '8', 'the', 'si@@', 'ze', 'of', '4@@', '0@@', '-@@', 'y@@', 'ear@@', '-@@', 'ol@@', 'ds', '--', 'it@@', "'s", 're@@', 'cor@@', 't@@', 'ed', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2023-06-03 06:21:53,681 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:21:53,681 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:21:53,681 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides for demonstrate that the glacil glacil artica, which is about three million years had the size of 48 dimensions of 48 the size of 40-year-olds -- it's recorted 40-percent.
2023-06-03 06:21:53,681 - INFO - joeynmt.training - Example #1
2023-06-03 06:21:53,681 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:21:53,681 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:21:53,681 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'you', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'ice', 'ice', 'be@@', 'h@@', 'in@@', 'd', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'gr@@', 'av@@', 'ity.', '</s>']
2023-06-03 06:21:53,682 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:21:53,682 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:21:53,682 - INFO - joeynmt.training - 	Hypothesis: However, the gravity of the gravity of the problem because it doesn't show you the ice of the ice of the ice of ice ice behind the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice because it doesn't show the gravity.
2023-06-03 06:21:53,682 - INFO - joeynmt.training - Example #2
2023-06-03 06:21:53,682 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:21:53,682 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:21:53,682 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'l', 'ar@@', 'th@@', 'ic', 'cu@@', 'p@@', ',', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system', 'of', 'global', 'clim@@', 'ate', 'system', '</s>']
2023-06-03 06:21:53,683 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:21:53,683 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:21:53,683 - INFO - joeynmt.training - 	Hypothesis: The glacil arthic cup, in a sense, the heart of the global climate system of global climate system
2023-06-03 06:21:53,683 - INFO - joeynmt.training - Example #3
2023-06-03 06:21:53,683 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:21:53,683 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:21:53,683 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'tur@@', 'ns', 'out', 'out', 'to', 'be', 's@@', 'w@@', 'ing', 'and', 's@@', 'ou@@', 'th@@', '.', '</s>']
2023-06-03 06:21:53,684 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:21:53,684 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:21:53,684 - INFO - joeynmt.training - 	Hypothesis: It turns out out to be swing and south.
2023-06-03 06:21:53,684 - INFO - joeynmt.training - Example #4
2023-06-03 06:21:53,684 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:21:53,684 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:21:53,684 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'de@@', 'vic@@', 'es', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'er', 'for', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:21:53,684 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:21:53,685 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:21:53,685 - INFO - joeynmt.training - 	Hypothesis: The next slide devices will be a quick career for the last 25 years.
2023-06-03 06:22:37,314 - INFO - joeynmt.training - Epoch   6, Step:    20100, Batch Loss:     1.500177, Batch Acc: 0.560141, Tokens per Sec:     1577, Lr: 0.000300
2023-06-03 06:23:20,724 - INFO - joeynmt.training - Epoch   6, Step:    20200, Batch Loss:     1.560709, Batch Acc: 0.562484, Tokens per Sec:     1636, Lr: 0.000300
2023-06-03 06:24:03,305 - INFO - joeynmt.training - Epoch   6, Step:    20300, Batch Loss:     1.552308, Batch Acc: 0.560903, Tokens per Sec:     1632, Lr: 0.000300
2023-06-03 06:24:46,979 - INFO - joeynmt.training - Epoch   6, Step:    20400, Batch Loss:     1.478287, Batch Acc: 0.559751, Tokens per Sec:     1605, Lr: 0.000300
2023-06-03 06:25:31,699 - INFO - joeynmt.training - Epoch   6, Step:    20500, Batch Loss:     1.445429, Batch Acc: 0.561370, Tokens per Sec:     1587, Lr: 0.000300
2023-06-03 06:25:31,699 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:29:08,072 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.80, ppl:   6.03, acc:   0.50, generation: 216.3541[sec], evaluation: 0.0000[sec]
2023-06-03 06:29:08,072 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 06:29:08,304 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/18000.ckpt
2023-06-03 06:29:08,307 - INFO - joeynmt.training - Example #0
2023-06-03 06:29:08,307 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:29:08,307 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:29:08,307 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'sli@@', 'de', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ta', 'd@@', 'i@@', 'st@@', 'inc@@', 't', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'dimen@@', 'sion@@', 's,', 'which', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'in', 'the', 'United', 'St@@', 'ates', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'in', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'is', 're@@', 'ar@@', 'm@@', 's.', '</s>']
2023-06-03 06:29:08,308 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:29:08,308 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:29:08,308 - INFO - joeynmt.training - 	Hypothesis: And I showed these slide slide for demonstrate to show that the glacial calotta distinct for almost three million years had the size of 48 dimensions, which is restretched in the United States is restretched in 48 percent of the United States is rearms.
2023-06-03 06:29:08,308 - INFO - joeynmt.training - Example #1
2023-06-03 06:29:08,308 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:29:08,308 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:29:08,308 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'g@@', 'ol@@', 'den', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 06:29:08,309 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:29:08,309 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:29:08,309 - INFO - joeynmt.training - 	Hypothesis: But the gravity of the gravity of the problem because it doesn't show the golden of the ice.
2023-06-03 06:29:08,309 - INFO - joeynmt.training - Example #2
2023-06-03 06:29:08,309 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:29:08,309 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:29:08,309 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'l', 'is', 'gl@@', 'ac@@', 'i@@', 'l', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'pul@@', 'sing', 'system', 'of', 'global', 'cli@@', 'mat@@', 'ic', 'system@@', '.', '</s>']
2023-06-03 06:29:08,310 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:29:08,310 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:29:08,310 - INFO - joeynmt.training - 	Hypothesis: The glacil is glacil is, in a sense, the pulsing system of global climatic system.
2023-06-03 06:29:08,310 - INFO - joeynmt.training - Example #3
2023-06-03 06:29:08,310 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:29:08,310 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:29:08,310 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'tur@@', 'ns', 'out', 'that', 'it', 'was', 'go@@', 'ing', 'to', 'be', 'ext@@', 'rem@@', 'ely', 're@@', 'v@@', 'el@@', 'y.', '</s>']
2023-06-03 06:29:08,311 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:29:08,311 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:29:08,311 - INFO - joeynmt.training - 	Hypothesis: It turns out that it was going to be extremely revely.
2023-06-03 06:29:08,311 - INFO - joeynmt.training - Example #4
2023-06-03 06:29:08,311 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:29:08,311 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:29:08,311 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'r@@', 'y', 'car@@', 'r@@', 'i@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:29:08,312 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:29:08,312 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:29:08,312 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carery carried on the last 25 years.
2023-06-03 06:29:52,919 - INFO - joeynmt.training - Epoch   6, Step:    20600, Batch Loss:     1.553185, Batch Acc: 0.556456, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 06:30:35,751 - INFO - joeynmt.training - Epoch   6, Step:    20700, Batch Loss:     1.542358, Batch Acc: 0.565267, Tokens per Sec:     1632, Lr: 0.000300
2023-06-03 06:31:19,870 - INFO - joeynmt.training - Epoch   6, Step:    20800, Batch Loss:     1.496230, Batch Acc: 0.563529, Tokens per Sec:     1622, Lr: 0.000300
2023-06-03 06:32:02,818 - INFO - joeynmt.training - Epoch   6, Step:    20900, Batch Loss:     1.499233, Batch Acc: 0.555918, Tokens per Sec:     1662, Lr: 0.000300
2023-06-03 06:32:45,883 - INFO - joeynmt.training - Epoch   6, Step:    21000, Batch Loss:     1.550914, Batch Acc: 0.562662, Tokens per Sec:     1648, Lr: 0.000300
2023-06-03 06:32:45,884 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:36:45,654 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.79, ppl:   5.96, acc:   0.50, generation: 239.7526[sec], evaluation: 0.0000[sec]
2023-06-03 06:36:45,655 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 06:36:45,903 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/18500.ckpt
2023-06-03 06:36:45,906 - INFO - joeynmt.training - Example #0
2023-06-03 06:36:45,906 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:36:45,906 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:36:45,906 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'last', 'year', 'I', 'sh@@', 'own', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'c@@', 'al@@', 'c@@', 'ul@@', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'stre@@', 't@@', 'ta', '4@@', '0@@', '-@@', 'percent@@', '.', '</s>']
2023-06-03 06:36:45,907 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:36:45,907 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:36:45,907 - INFO - joeynmt.training - 	Hypothesis: And the last year I shown these slides to show that the glacial calculartica, which is almost three million years had the size 48 percent of the United States, it's restretta 40-percent.
2023-06-03 06:36:45,907 - INFO - joeynmt.training - Example #1
2023-06-03 06:36:45,907 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:36:45,907 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:36:45,907 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'a', 'wh@@', 'ol@@', 'e', 'thing', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 06:36:45,908 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:36:45,908 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:36:45,908 - INFO - joeynmt.training - 	Hypothesis: But this is a whole thing of the gravity of the problem because it doesn't show the ice of the ice.
2023-06-03 06:36:45,908 - INFO - joeynmt.training - Example #2
2023-06-03 06:36:45,908 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:36:45,908 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:36:45,908 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ro@@', 'w', 'is', 'a', 'certa@@', 'in', 'is', 'a', 'certa@@', 'in', 'sen@@', 'se,', 'the', 'd@@', 'i@@', 'st@@', 'ant', 'system', 'of', 'global', 'clim@@', 'ate', 'system', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 06:36:45,908 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:36:45,909 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:36:45,909 - INFO - joeynmt.training - 	Hypothesis: The arthrow is a certain is a certain sense, the distant system of global climate system of global climate system.
2023-06-03 06:36:45,909 - INFO - joeynmt.training - Example #3
2023-06-03 06:36:45,909 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:36:45,909 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:36:45,909 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['and', 'it', 'ex@@', 'p@@', 'and@@', 's', 'and', 'it', 'tur@@', 'ns', 'out', 'it', 'was', 'ext@@', 'rem@@', 'ed.', '</s>']
2023-06-03 06:36:45,909 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:36:45,909 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:36:45,909 - INFO - joeynmt.training - 	Hypothesis: and it expands and it turns out it was extremed.
2023-06-03 06:36:45,910 - INFO - joeynmt.training - Example #4
2023-06-03 06:36:45,910 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:36:45,910 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:36:45,910 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'er', 'for', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:36:45,910 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:36:45,910 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:36:45,910 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick career for the last 25 years.
2023-06-03 06:37:30,110 - INFO - joeynmt.training - Epoch   6, Step:    21100, Batch Loss:     1.534001, Batch Acc: 0.558913, Tokens per Sec:     1590, Lr: 0.000300
2023-06-03 06:38:12,946 - INFO - joeynmt.training - Epoch   6, Step:    21200, Batch Loss:     1.614633, Batch Acc: 0.556002, Tokens per Sec:     1611, Lr: 0.000300
2023-06-03 06:38:55,764 - INFO - joeynmt.training - Epoch   6, Step:    21300, Batch Loss:     1.600840, Batch Acc: 0.561295, Tokens per Sec:     1629, Lr: 0.000300
2023-06-03 06:39:40,416 - INFO - joeynmt.training - Epoch   6, Step:    21400, Batch Loss:     1.483523, Batch Acc: 0.563274, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 06:40:23,322 - INFO - joeynmt.training - Epoch   6, Step:    21500, Batch Loss:     1.499988, Batch Acc: 0.558728, Tokens per Sec:     1626, Lr: 0.000300
2023-06-03 06:40:23,323 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:43:24,895 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.78, ppl:   5.93, acc:   0.51, generation: 181.5557[sec], evaluation: 0.0000[sec]
2023-06-03 06:43:24,896 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 06:43:25,133 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/19500.ckpt
2023-06-03 06:43:25,136 - INFO - joeynmt.training - Example #0
2023-06-03 06:43:25,136 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:43:25,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:43:25,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'th@@', 'ese', 'last', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'd@@', 'y', 'd@@', 'i@@', 'mon@@', 'ous', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'st@@', 'at@@', 'ement@@', ',', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'and', 'it@@', "'s", 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'of', '4@@', '0@@', '.', '</s>']
2023-06-03 06:43:25,137 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:43:25,137 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:43:25,137 - INFO - joeynmt.training - 	Hypothesis: I showed these last year, I showed these slide dy dimonous that the glacial statement, which is almost three million years had the size of the United States continents, and it's restretched of 40.
2023-06-03 06:43:25,137 - INFO - joeynmt.training - Example #1
2023-06-03 06:43:25,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:43:25,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:43:25,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['All', 'of', 'the', 'ice', 'of', 'the', 'problem', 'be@@', 'cause', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'is@@', 'sue', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'sp@@', 'ess@@', 'or', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 06:43:25,138 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:43:25,138 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:43:25,138 - INFO - joeynmt.training - 	Hypothesis: All of the ice of the problem because the gravity of the issue because it doesn't show the spessor of the ice.
2023-06-03 06:43:25,138 - INFO - joeynmt.training - Example #2
2023-06-03 06:43:25,138 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:43:25,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:43:25,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ro@@', 'w', 'gl@@', 'ac@@', 'i@@', 'al', 'is', '--', 'in', 'a', 'sense', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 06:43:25,139 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:43:25,139 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:43:25,139 - INFO - joeynmt.training - 	Hypothesis: The arthrow glacial is -- in a sense of the global climate system.
2023-06-03 06:43:25,139 - INFO - joeynmt.training - Example #3
2023-06-03 06:43:25,139 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:43:25,139 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:43:25,139 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'and', 'it', 'comes', 'u@@', 'p', 'to', 'the', 'st@@', 'at@@', 'e.', '</s>']
2023-06-03 06:43:25,140 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:43:25,140 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:43:25,140 - INFO - joeynmt.training - 	Hypothesis: It expands and it comes up to the state.
2023-06-03 06:43:25,140 - INFO - joeynmt.training - Example #4
2023-06-03 06:43:25,140 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:43:25,140 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:43:25,140 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'er', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:43:25,141 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:43:25,141 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:43:25,141 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick career on the last 25 years.
2023-06-03 06:44:09,721 - INFO - joeynmt.training - Epoch   6, Step:    21600, Batch Loss:     1.653405, Batch Acc: 0.561626, Tokens per Sec:     1568, Lr: 0.000300
2023-06-03 06:44:56,538 - INFO - joeynmt.training - Epoch   6, Step:    21700, Batch Loss:     1.631155, Batch Acc: 0.561734, Tokens per Sec:     1517, Lr: 0.000300
2023-06-03 06:45:44,398 - INFO - joeynmt.training - Epoch   6, Step:    21800, Batch Loss:     1.623450, Batch Acc: 0.562668, Tokens per Sec:     1466, Lr: 0.000300
2023-06-03 06:46:29,558 - INFO - joeynmt.training - Epoch   6, Step:    21900, Batch Loss:     1.503256, Batch Acc: 0.563266, Tokens per Sec:     1587, Lr: 0.000300
2023-06-03 06:47:13,657 - INFO - joeynmt.training - Epoch   6, Step:    22000, Batch Loss:     1.477417, Batch Acc: 0.563547, Tokens per Sec:     1570, Lr: 0.000300
2023-06-03 06:47:13,658 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:50:24,087 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.88, acc:   0.51, generation: 190.4134[sec], evaluation: 0.0000[sec]
2023-06-03 06:50:24,093 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 06:50:24,312 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/19000.ckpt
2023-06-03 06:50:24,314 - INFO - joeynmt.training - Example #0
2023-06-03 06:50:24,315 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:50:24,315 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:50:24,315 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'h', 'h@@', 'ot@@', 'ter', 'of', 'the', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'you', 'know,', 'it@@', "'s", 're@@', 'cor@@', 'ted', 'by', '4@@', '0@@', '.', '</s>']
2023-06-03 06:50:24,315 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:50:24,315 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:50:24,316 - INFO - joeynmt.training - 	Hypothesis: The last year, I showed these slides for demonstrate that the glacial caloth hotter of the artica, which is almost three million years had the size of the United States of the United States of the United States, you know, it's recorted by 40.
2023-06-03 06:50:24,316 - INFO - joeynmt.training - Example #1
2023-06-03 06:50:24,316 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:50:24,316 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:50:24,316 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'under@@', 'l@@', 'ying', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'ice', 'ice', 'ice', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'be@@', 'cause', 'it', 'is@@', "n't", 'the', 'ice', 'the', 'ice', 'the', 'ice', 'of', 'the', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ex@@', 'ex@@', 'hi@@', 'b@@', 're@@', 'ef@@', 'it.', '</s>']
2023-06-03 06:50:24,316 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:50:24,316 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:50:24,316 - INFO - joeynmt.training - 	Hypothesis: But this underlying the gravity of the gravity of the ice of the ice of the ice of ice ice ice ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice of the ice because it isn't the ice the ice the ice of the ice ice ice ice ice ice ice ice ice ice ice ice ice exexhibreefit.
2023-06-03 06:50:24,316 - INFO - joeynmt.training - Example #2
2023-06-03 06:50:24,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:50:24,317 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:50:24,317 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'cal@@', 'ot@@', 'ti@@', 'c', 'cal@@', 'ot@@', 'h', 'is', 'in', 'a', 'way,', 'the', 'd@@', 'i@@', 'st@@', 'ant', 'system@@', '.', '</s>']
2023-06-03 06:50:24,317 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:50:24,317 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:50:24,317 - INFO - joeynmt.training - 	Hypothesis: The arthic calottic caloth is in a way, the distant system.
2023-06-03 06:50:24,317 - INFO - joeynmt.training - Example #3
2023-06-03 06:50:24,317 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:50:24,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:50:24,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'ands', 'and', 'it', 'tur@@', 'ns', 'out', 'of', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 06:50:24,318 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:50:24,318 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:50:24,318 - INFO - joeynmt.training - 	Hypothesis: It expands and it turns out of the summer.
2023-06-03 06:50:24,318 - INFO - joeynmt.training - Example #4
2023-06-03 06:50:24,318 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:50:24,318 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:50:24,318 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'bo@@', 'ard', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:50:24,319 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:50:24,319 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:50:24,319 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick careboard of the last 25 years.
2023-06-03 06:51:08,116 - INFO - joeynmt.training - Epoch   6, Step:    22100, Batch Loss:     1.412597, Batch Acc: 0.565564, Tokens per Sec:     1591, Lr: 0.000300
2023-06-03 06:51:51,881 - INFO - joeynmt.training - Epoch   6, Step:    22200, Batch Loss:     1.312161, Batch Acc: 0.557502, Tokens per Sec:     1593, Lr: 0.000300
2023-06-03 06:52:36,487 - INFO - joeynmt.training - Epoch   6, Step:    22300, Batch Loss:     1.440645, Batch Acc: 0.564222, Tokens per Sec:     1571, Lr: 0.000300
2023-06-03 06:53:20,131 - INFO - joeynmt.training - Epoch   6, Step:    22400, Batch Loss:     1.574252, Batch Acc: 0.562338, Tokens per Sec:     1638, Lr: 0.000300
2023-06-03 06:54:03,442 - INFO - joeynmt.training - Epoch   6, Step:    22500, Batch Loss:     1.430285, Batch Acc: 0.567401, Tokens per Sec:     1640, Lr: 0.000300
2023-06-03 06:54:03,443 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 06:57:08,486 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.87, acc:   0.51, generation: 185.0280[sec], evaluation: 0.0000[sec]
2023-06-03 06:57:08,492 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 06:57:08,746 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/20000.ckpt
2023-06-03 06:57:08,749 - INFO - joeynmt.training - Example #0
2023-06-03 06:57:08,749 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 06:57:08,749 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 06:57:08,749 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'th@@', 'ese', 'ste@@', 'p@@', 'ed', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'l', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'lo@@', 'ts', 'of', 'the', 'U.@@', 'S.', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', 'which', 'is', 're@@', 'stre@@', 't@@', 't@@', 't@@', 't@@', '-@@', 'United', 'St@@', 'ates', 'the', 'U.@@', 'S.', 'percent@@', '.', '</s>']
2023-06-03 06:57:08,750 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 06:57:08,750 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 06:57:08,750 - INFO - joeynmt.training - 	Hypothesis: I showed these steped slides to show that the glacil artica, which is that the glacial lots of the U.S. continental continents, which is restretttt-United States the U.S. percent.
2023-06-03 06:57:08,750 - INFO - joeynmt.training - Example #1
2023-06-03 06:57:08,750 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 06:57:08,750 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 06:57:08,750 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 's@@', 'l@@', 'ate', 'be@@', 'low', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'ex@@', 'hi@@', 'b@@', 'it@@', 'e.', '</s>']
2023-06-03 06:57:08,751 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 06:57:08,751 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 06:57:08,751 - INFO - joeynmt.training - 	Hypothesis: And this slate below the gravity of the problem because it doesn't show the ice because it doesn't show the ice exhibite.
2023-06-03 06:57:08,751 - INFO - joeynmt.training - Example #2
2023-06-03 06:57:08,751 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 06:57:08,751 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 06:57:08,751 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ere', 'is', 'a', 'kind', 'of', 'gl@@', 'ac@@', 'i@@', 'l', 'is,', 'in', 'a', 'sense', 'of', 'the', 'clim@@', 'ate', 'clim@@', 'ate', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 06:57:08,751 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 06:57:08,752 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 06:57:08,752 - INFO - joeynmt.training - 	Hypothesis: The arthere is a kind of glacil is, in a sense of the climate climate climate system.
2023-06-03 06:57:08,752 - INFO - joeynmt.training - Example #3
2023-06-03 06:57:08,752 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 06:57:08,752 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 06:57:08,752 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'the', 'ex@@', 'p@@', 'and@@', 's', 'and', 'it', 'tur@@', 'ns', 'out', 'to', 'the', 'sum@@', 'm@@', 'er', 'and', 'it', 'tur@@', 'ns', 'out', 'to', 'the', 'sum@@', 'm@@', 'it.', '</s>']
2023-06-03 06:57:08,752 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 06:57:08,752 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 06:57:08,752 - INFO - joeynmt.training - 	Hypothesis: It is the expands and it turns out to the summer and it turns out to the summit.
2023-06-03 06:57:08,752 - INFO - joeynmt.training - Example #4
2023-06-03 06:57:08,753 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 06:57:08,753 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 06:57:08,753 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'f@@', 'ast@@', '-@@', 'car@@', 'e@@', 'ful', 'car@@', 'e@@', 'ful', 'for', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 06:57:08,753 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 06:57:08,753 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 06:57:08,753 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly fast-careful careful for the last 25 years.
2023-06-03 06:57:53,667 - INFO - joeynmt.training - Epoch   6, Step:    22600, Batch Loss:     1.562766, Batch Acc: 0.563255, Tokens per Sec:     1525, Lr: 0.000300
2023-06-03 06:58:38,291 - INFO - joeynmt.training - Epoch   6, Step:    22700, Batch Loss:     1.749985, Batch Acc: 0.563290, Tokens per Sec:     1595, Lr: 0.000300
2023-06-03 06:59:05,593 - INFO - joeynmt.training - Epoch   6: total training loss 5805.25
2023-06-03 06:59:05,594 - INFO - joeynmt.training - EPOCH 7
2023-06-03 06:59:21,991 - INFO - joeynmt.training - Epoch   7, Step:    22800, Batch Loss:     1.540158, Batch Acc: 0.581185, Tokens per Sec:     1500, Lr: 0.000300
2023-06-03 07:00:06,448 - INFO - joeynmt.training - Epoch   7, Step:    22900, Batch Loss:     1.346853, Batch Acc: 0.587094, Tokens per Sec:     1568, Lr: 0.000300
2023-06-03 07:00:50,219 - INFO - joeynmt.training - Epoch   7, Step:    23000, Batch Loss:     1.562190, Batch Acc: 0.583715, Tokens per Sec:     1572, Lr: 0.000300
2023-06-03 07:00:50,219 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 07:04:13,852 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.87, acc:   0.51, generation: 203.6167[sec], evaluation: 0.0000[sec]
2023-06-03 07:04:14,107 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/20500.ckpt
2023-06-03 07:04:14,109 - INFO - joeynmt.training - Example #0
2023-06-03 07:04:14,110 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 07:04:14,110 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 07:04:14,110 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'of', 'sli@@', 'de', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'l', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'which', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'the', 'si@@', 'ze', 'of', '4@@', '0@@', '-@@', 'y@@', 'ear@@', '-@@', 'old', 'T@@', 'ur@@', 'y', 'is', 're@@', 'stre@@', 't@@', 't@@', 'ta', 'd@@', 'i@@', 'gg@@', 'ing', '4@@', '0@@', '.', '</s>']
2023-06-03 07:04:14,110 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 07:04:14,110 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 07:04:14,110 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides of slide that the glacil artica, which is about three million years had the size of the United States, which is restretched the size of 40-year-old Tury is restrettta digging 40.
2023-06-03 07:04:14,111 - INFO - joeynmt.training - Example #1
2023-06-03 07:04:14,111 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 07:04:14,111 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 07:04:14,111 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'under@@', 'l@@', 'ying', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', 'show@@', 's', 'not', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 07:04:14,111 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 07:04:14,111 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 07:04:14,111 - INFO - joeynmt.training - 	Hypothesis: But this underlying the gravity of the problem because it shows not the ice of the ice.
2023-06-03 07:04:14,111 - INFO - joeynmt.training - Example #2
2023-06-03 07:04:14,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 07:04:14,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 07:04:14,112 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ere', 'is', 'gl@@', 'ac@@', 'i@@', 'al', 'is,', 'in', 'a', 'sense', 'of', 'the', 'global', 'clim@@', 'ate', 'cu@@', 'st@@', 'om@@', '.', '</s>']
2023-06-03 07:04:14,112 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 07:04:14,112 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 07:04:14,112 - INFO - joeynmt.training - 	Hypothesis: The arthere is glacial is, in a sense of the global climate custom.
2023-06-03 07:04:14,112 - INFO - joeynmt.training - Example #3
2023-06-03 07:04:14,112 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 07:04:14,112 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 07:04:14,113 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'of', 'w@@', 'int@@', 's', 'and', 're@@', 'ally', 'r@@', 'i@@', 'gh@@', 'ts', 'of', 'ext@@', 'rem@@', 'e.', '</s>']
2023-06-03 07:04:14,113 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 07:04:14,113 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 07:04:14,113 - INFO - joeynmt.training - 	Hypothesis: It expands of wints and really rights of extreme.
2023-06-03 07:04:14,113 - INFO - joeynmt.training - Example #4
2023-06-03 07:04:14,113 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 07:04:14,113 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 07:04:14,113 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'full@@', 'y', 'car@@', 'e@@', 'full@@', 'y', 'for', 'the', 'last', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2023-06-03 07:04:14,114 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 07:04:14,114 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 07:04:14,114 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quickly carefully carefully for the last 25 years old.
2023-06-03 07:04:59,201 - INFO - joeynmt.training - Epoch   7, Step:    23100, Batch Loss:     1.367639, Batch Acc: 0.586435, Tokens per Sec:     1523, Lr: 0.000300
2023-06-03 07:05:43,783 - INFO - joeynmt.training - Epoch   7, Step:    23200, Batch Loss:     1.514450, Batch Acc: 0.580606, Tokens per Sec:     1561, Lr: 0.000300
2023-06-03 07:06:27,647 - INFO - joeynmt.training - Epoch   7, Step:    23300, Batch Loss:     1.385718, Batch Acc: 0.581403, Tokens per Sec:     1626, Lr: 0.000300
2023-06-03 07:07:12,687 - INFO - joeynmt.training - Epoch   7, Step:    23400, Batch Loss:     1.504202, Batch Acc: 0.582027, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 07:07:56,407 - INFO - joeynmt.training - Epoch   7, Step:    23500, Batch Loss:     1.312279, Batch Acc: 0.578342, Tokens per Sec:     1598, Lr: 0.000300
2023-06-03 07:07:56,407 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 07:11:24,436 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.86, acc:   0.51, generation: 208.0098[sec], evaluation: 0.0000[sec]
2023-06-03 07:11:24,437 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 07:11:24,701 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/21000.ckpt
2023-06-03 07:11:24,704 - INFO - joeynmt.training - Example #0
2023-06-03 07:11:24,704 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 07:11:24,704 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 07:11:24,704 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'o@@', 'v@@', 'es', 'for', 'd@@', 'i@@', 'show', 'that', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', '4@@', '8', 'to', 'the', 'mi@@', 'dd@@', 'le', 'of', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 07:11:24,705 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 07:11:24,705 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 07:11:24,705 - INFO - joeynmt.training - 	Hypothesis: The year I showed these dioves for dishow that glacial calotta glacial artica, which is about three million years had the size of the 48 to the middle of the 40.
2023-06-03 07:11:24,705 - INFO - joeynmt.training - Example #1
2023-06-03 07:11:24,705 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 07:11:24,705 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 07:11:24,705 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'a', 's@@', 'lo@@', 'pe', 'of', 'the', 'problem', 'be@@', 'cause', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ice', 'of', 'ice', 'ice', 'be@@', 'cause', 'the', 'ice', 'show@@', '.', '</s>']
2023-06-03 07:11:24,706 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 07:11:24,706 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 07:11:24,706 - INFO - joeynmt.training - 	Hypothesis: But this is a slope of the problem because the gravity of the ice of ice ice because the ice show.
2023-06-03 07:11:24,706 - INFO - joeynmt.training - Example #2
2023-06-03 07:11:24,706 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 07:11:24,706 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 07:11:24,706 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'd@@', 'i@@', 'st@@', 'ant', 'system@@', '.', '</s>']
2023-06-03 07:11:24,706 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 07:11:24,707 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 07:11:24,707 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is, in a sense, the distant system.
2023-06-03 07:11:24,707 - INFO - joeynmt.training - Example #3
2023-06-03 07:11:24,707 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 07:11:24,707 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 07:11:24,707 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'and', 'it', 'comes', 'to', 'the', 'ext@@', 'rem@@', 'ely', 're@@', 'ally', 're@@', 'st@@', 'a@@', 'y@@', 'ed.', '</s>']
2023-06-03 07:11:24,707 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 07:11:24,708 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 07:11:24,708 - INFO - joeynmt.training - 	Hypothesis: It expands and it comes to the extremely really restayed.
2023-06-03 07:11:24,708 - INFO - joeynmt.training - Example #4
2023-06-03 07:11:24,708 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 07:11:24,708 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 07:11:24,708 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'ful', 'car@@', 'e@@', 'ful', 'for', 'the', 'last', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2023-06-03 07:11:24,708 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 07:11:24,709 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 07:11:24,709 - INFO - joeynmt.training - 	Hypothesis: The next slide is a quick careful careful for the last 25 years old.
2023-06-03 07:12:09,574 - INFO - joeynmt.training - Epoch   7, Step:    23600, Batch Loss:     1.498544, Batch Acc: 0.575467, Tokens per Sec:     1540, Lr: 0.000300
2023-06-03 07:12:54,047 - INFO - joeynmt.training - Epoch   7, Step:    23700, Batch Loss:     1.391510, Batch Acc: 0.578338, Tokens per Sec:     1592, Lr: 0.000300
2023-06-03 07:13:37,801 - INFO - joeynmt.training - Epoch   7, Step:    23800, Batch Loss:     1.312260, Batch Acc: 0.574709, Tokens per Sec:     1653, Lr: 0.000300
2023-06-03 07:14:22,780 - INFO - joeynmt.training - Epoch   7, Step:    23900, Batch Loss:     1.441551, Batch Acc: 0.581313, Tokens per Sec:     1540, Lr: 0.000300
2023-06-03 07:15:06,300 - INFO - joeynmt.training - Epoch   7, Step:    24000, Batch Loss:     1.416722, Batch Acc: 0.583441, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 07:15:06,301 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 07:18:16,789 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.77, ppl:   5.86, acc:   0.51, generation: 190.4726[sec], evaluation: 0.0000[sec]
2023-06-03 07:18:16,790 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 07:18:17,052 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/21500.ckpt
2023-06-03 07:18:17,055 - INFO - joeynmt.training - Example #0
2023-06-03 07:18:17,056 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 07:18:17,056 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 07:18:17,056 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'last', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'sli@@', 'de', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'lo@@', 'ts', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', '4@@', '8', 'the', 'si@@', 'ze', 'of', '4@@', '0@@', '0', 'percent', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'you', 'know,', '4@@', '0@@', '-@@', 'percent', 'of', 'the', 'United', 'St@@', 'at@@', 'es.', '</s>']
2023-06-03 07:18:17,056 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 07:18:17,056 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 07:18:17,056 - INFO - joeynmt.training - 	Hypothesis: The last year I showed these slides to show that slide that the glacial lots for almost three million years had the size of 48 dimensions of 48 the size of 400 percent of the United States, you know, 40-percent of the United States.
2023-06-03 07:18:17,056 - INFO - joeynmt.training - Example #1
2023-06-03 07:18:17,056 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 07:18:17,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 07:18:17,057 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'that', 'is', 'a', 'wh@@', 'ol@@', 'e', 'thing', 'that', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 07:18:17,057 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 07:18:17,057 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 07:18:17,057 - INFO - joeynmt.training - 	Hypothesis: And that is a whole thing that is the gravity of the problem because it doesn't show the ice of ice because it doesn't show the ice.
2023-06-03 07:18:17,057 - INFO - joeynmt.training - Example #2
2023-06-03 07:18:17,057 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 07:18:17,057 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 07:18:17,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', '--', 'in', 'a', 'sense', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 07:18:17,058 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 07:18:17,058 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 07:18:17,058 - INFO - joeynmt.training - 	Hypothesis: The glacial glacial is -- in a sense of the global climate system.
2023-06-03 07:18:17,058 - INFO - joeynmt.training - Example #3
2023-06-03 07:18:17,058 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 07:18:17,058 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 07:18:17,058 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'was', 'the', 'w@@', 'int@@', 'er', 'and', 'it', 'comes', 'out', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'it@@', "'s", 'ext@@', 'rem@@', 'e.', '</s>']
2023-06-03 07:18:17,059 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 07:18:17,059 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 07:18:17,059 - INFO - joeynmt.training - 	Hypothesis: It was the winter and it comes out of the summer and it's extreme.
2023-06-03 07:18:17,059 - INFO - joeynmt.training - Example #4
2023-06-03 07:18:17,059 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 07:18:17,059 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 07:18:17,059 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'of', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'r@@', 'y', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 07:18:17,060 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 07:18:17,060 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 07:18:17,060 - INFO - joeynmt.training - 	Hypothesis: The next slide of slide will be a quick carery on the last 25 years.
2023-06-03 07:19:01,413 - INFO - joeynmt.training - Epoch   7, Step:    24100, Batch Loss:     1.430788, Batch Acc: 0.581818, Tokens per Sec:     1586, Lr: 0.000300
2023-06-03 07:19:46,452 - INFO - joeynmt.training - Epoch   7, Step:    24200, Batch Loss:     1.450538, Batch Acc: 0.581241, Tokens per Sec:     1565, Lr: 0.000300
2023-06-03 07:20:32,710 - INFO - joeynmt.training - Epoch   7, Step:    24300, Batch Loss:     1.482450, Batch Acc: 0.575142, Tokens per Sec:     1496, Lr: 0.000300
2023-06-03 07:21:16,866 - INFO - joeynmt.training - Epoch   7, Step:    24400, Batch Loss:     1.512187, Batch Acc: 0.572487, Tokens per Sec:     1565, Lr: 0.000300
2023-06-03 07:22:01,499 - INFO - joeynmt.training - Epoch   7, Step:    24500, Batch Loss:     1.382781, Batch Acc: 0.574168, Tokens per Sec:     1605, Lr: 0.000300
2023-06-03 07:22:01,500 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 07:25:53,715 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.76, ppl:   5.82, acc:   0.51, generation: 232.1984[sec], evaluation: 0.0000[sec]
2023-06-03 07:25:53,715 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 07:25:53,962 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/22000.ckpt
2023-06-03 07:25:53,965 - INFO - joeynmt.training - Example #0
2023-06-03 07:25:53,965 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 07:25:53,965 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 07:25:53,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'sli@@', 'de', 'is', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ter', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'about', 'four', 'dimen@@', 'sion@@', 's', 'of', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'is', 're@@', 'st@@', 'ra@@', 'i@@', 'ght', 'in', '4@@', '0@@', '.', '</s>']
2023-06-03 07:25:53,966 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 07:25:53,966 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 07:25:53,966 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that slide is to show that the glacial calotter artica, which is about about four dimensions of the size of 48 United States is restraight in 40.
2023-06-03 07:25:53,966 - INFO - joeynmt.training - Example #1
2023-06-03 07:25:53,966 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 07:25:53,966 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 07:25:53,966 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'ex@@', 'hi@@', 'bit', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 07:25:53,967 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 07:25:53,967 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 07:25:53,967 - INFO - joeynmt.training - 	Hypothesis: However, the gravity of the problem because it doesn't exhibit of the ice.
2023-06-03 07:25:53,967 - INFO - joeynmt.training - Example #2
2023-06-03 07:25:53,967 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 07:25:53,967 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 07:25:53,967 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ro@@', 'w', 'is', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cu@@', 'st@@', 'om@@', 'b@@', '.', '</s>']
2023-06-03 07:25:53,968 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 07:25:53,968 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 07:25:53,968 - INFO - joeynmt.training - 	Hypothesis: The arthrow is the glacial customb.
2023-06-03 07:25:53,968 - INFO - joeynmt.training - Example #3
2023-06-03 07:25:53,968 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 07:25:53,968 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 07:25:53,968 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'of', 'w@@', 'int@@', 'er', 'and', 're@@', 'ally', 'ex@@', 'p@@', 'and@@', 'ing', 'ex@@', 'p@@', 'and@@', 'ing', 'ex@@', 'p@@', 'and@@', 's.', '</s>']
2023-06-03 07:25:53,968 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 07:25:53,969 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 07:25:53,969 - INFO - joeynmt.training - 	Hypothesis: It expands of winter and really expanding expanding expands.
2023-06-03 07:25:53,969 - INFO - joeynmt.training - Example #4
2023-06-03 07:25:53,969 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 07:25:53,969 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 07:25:53,969 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', ',', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 07:25:53,969 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 07:25:53,969 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 07:25:53,970 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a rapid fast, on the last 25 years.
2023-06-03 07:26:38,638 - INFO - joeynmt.training - Epoch   7, Step:    24600, Batch Loss:     1.479909, Batch Acc: 0.572456, Tokens per Sec:     1557, Lr: 0.000300
2023-06-03 07:27:22,852 - INFO - joeynmt.training - Epoch   7, Step:    24700, Batch Loss:     1.471943, Batch Acc: 0.576544, Tokens per Sec:     1587, Lr: 0.000300
2023-06-03 07:28:07,053 - INFO - joeynmt.training - Epoch   7, Step:    24800, Batch Loss:     1.468594, Batch Acc: 0.575133, Tokens per Sec:     1598, Lr: 0.000300
2023-06-03 07:28:51,605 - INFO - joeynmt.training - Epoch   7, Step:    24900, Batch Loss:     1.462652, Batch Acc: 0.576361, Tokens per Sec:     1559, Lr: 0.000300
2023-06-03 07:29:35,372 - INFO - joeynmt.training - Epoch   7, Step:    25000, Batch Loss:     1.654995, Batch Acc: 0.571050, Tokens per Sec:     1613, Lr: 0.000300
2023-06-03 07:29:35,372 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 07:32:23,587 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.75, acc:   0.51, generation: 168.1971[sec], evaluation: 0.0000[sec]
2023-06-03 07:32:23,589 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 07:32:23,857 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/23000.ckpt
2023-06-03 07:32:23,860 - INFO - joeynmt.training - Example #0
2023-06-03 07:32:23,860 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 07:32:23,860 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 07:32:23,860 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ta', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'has', 'been', 'the', 'si@@', 'ze', 'of', '4@@', '0@@', '.', '</s>']
2023-06-03 07:32:23,860 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 07:32:23,860 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 07:32:23,861 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the glacial calotta artica, which is almost three million years had the size of 48 United States has been the size of 40.
2023-06-03 07:32:23,861 - INFO - joeynmt.training - Example #1
2023-06-03 07:32:23,861 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 07:32:23,861 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 07:32:23,861 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', "ere's", 'a', 's@@', 'low', 'of', 'the', 'problem', 'be@@', 'cause', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'you', 'the', 'sp@@', 'ess@@', 'or', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 07:32:23,861 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 07:32:23,861 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 07:32:23,861 - INFO - joeynmt.training - 	Hypothesis: Here's a slow of the problem because the gravity of the problem because it doesn't show you the spessor the ice of the ice.
2023-06-03 07:32:23,862 - INFO - joeynmt.training - Example #2
2023-06-03 07:32:23,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 07:32:23,862 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 07:32:23,862 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'global', 'war@@', 'm@@', 'er', 'system@@', '.', '</s>']
2023-06-03 07:32:23,862 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 07:32:23,862 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 07:32:23,862 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is in a sense of global warmer system.
2023-06-03 07:32:23,862 - INFO - joeynmt.training - Example #3
2023-06-03 07:32:23,862 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 07:32:23,863 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 07:32:23,863 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 'go', 'into', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 07:32:23,863 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 07:32:23,863 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 07:32:23,863 - INFO - joeynmt.training - 	Hypothesis: It expands of the winter and you go into the summer.
2023-06-03 07:32:23,863 - INFO - joeynmt.training - Example #4
2023-06-03 07:32:23,863 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 07:32:23,863 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 07:32:23,863 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'de@@', 'ca@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'e@@', 'r@@', 'y', 'car@@', 'e@@', 'r@@', 'y.', '</s>']
2023-06-03 07:32:23,864 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 07:32:23,864 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 07:32:23,864 - INFO - joeynmt.training - 	Hypothesis: The next slide decade will be a quickly careery carery.
2023-06-03 07:33:08,457 - INFO - joeynmt.training - Epoch   7, Step:    25100, Batch Loss:     1.332067, Batch Acc: 0.575125, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 07:33:52,549 - INFO - joeynmt.training - Epoch   7, Step:    25200, Batch Loss:     1.276690, Batch Acc: 0.575198, Tokens per Sec:     1603, Lr: 0.000300
2023-06-03 07:34:37,282 - INFO - joeynmt.training - Epoch   7, Step:    25300, Batch Loss:     1.501781, Batch Acc: 0.576174, Tokens per Sec:     1578, Lr: 0.000300
2023-06-03 07:35:21,585 - INFO - joeynmt.training - Epoch   7, Step:    25400, Batch Loss:     1.389732, Batch Acc: 0.574195, Tokens per Sec:     1575, Lr: 0.000300
2023-06-03 07:36:05,573 - INFO - joeynmt.training - Epoch   7, Step:    25500, Batch Loss:     1.633724, Batch Acc: 0.574926, Tokens per Sec:     1594, Lr: 0.000300
2023-06-03 07:36:05,573 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 07:39:17,633 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.77, acc:   0.51, generation: 192.0437[sec], evaluation: 0.0000[sec]
2023-06-03 07:39:17,892 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/22500.ckpt
2023-06-03 07:39:17,895 - INFO - joeynmt.training - Example #0
2023-06-03 07:39:17,896 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 07:39:17,896 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 07:39:17,896 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'ar@@', 'tic@@', 'a,', 'almost', 'three', 'million', 'years', 'had', 'been', 'de@@', 'al@@', 'ing', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', '4@@', '0@@', '.', '</s>']
2023-06-03 07:39:17,896 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 07:39:17,896 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 07:39:17,896 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these diapositive for demonstrate that the glacial glacial artica, almost three million years had been dealing the size of 48 percent of the United States is restretched 40.
2023-06-03 07:39:17,896 - INFO - joeynmt.training - Example #1
2023-06-03 07:39:17,897 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 07:39:17,897 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 07:39:17,897 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 07:39:17,897 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 07:39:17,897 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 07:39:17,897 - INFO - joeynmt.training - 	Hypothesis: However, the gravity of the problem because the gravity of the ice.
2023-06-03 07:39:17,897 - INFO - joeynmt.training - Example #2
2023-06-03 07:39:17,898 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 07:39:17,898 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 07:39:17,898 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'global', 'clim@@', 'ate', 'cu@@', 'st@@', 'om@@', 'b', 'of', 'global', 'system@@', '.', '</s>']
2023-06-03 07:39:17,898 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 07:39:17,898 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 07:39:17,898 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is in a sense of global climate customb of global system.
2023-06-03 07:39:17,898 - INFO - joeynmt.training - Example #3
2023-06-03 07:39:17,899 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 07:39:17,899 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 07:39:17,899 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'ex@@', 'p@@', 'and@@', 's', 'and', 'it', 'comes', 'u@@', 'p', 'and', 'it', 'comes', 'to', 'the', 'est@@', 'at@@', 'e.', '</s>']
2023-06-03 07:39:17,899 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 07:39:17,899 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 07:39:17,899 - INFO - joeynmt.training - 	Hypothesis: It is expands and it comes up and it comes to the estate.
2023-06-03 07:39:17,899 - INFO - joeynmt.training - Example #4
2023-06-03 07:39:17,900 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 07:39:17,900 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 07:39:17,900 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'd', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 07:39:17,900 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 07:39:17,900 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 07:39:17,900 - INFO - joeynmt.training - 	Hypothesis: The next slide slide will be a quick card on the last 25 years.
2023-06-03 07:40:02,370 - INFO - joeynmt.training - Epoch   7, Step:    25600, Batch Loss:     1.491330, Batch Acc: 0.576605, Tokens per Sec:     1560, Lr: 0.000300
2023-06-03 07:40:46,664 - INFO - joeynmt.training - Epoch   7, Step:    25700, Batch Loss:     1.577607, Batch Acc: 0.570199, Tokens per Sec:     1569, Lr: 0.000300
2023-06-03 07:41:30,068 - INFO - joeynmt.training - Epoch   7, Step:    25800, Batch Loss:     1.640391, Batch Acc: 0.573834, Tokens per Sec:     1679, Lr: 0.000300
2023-06-03 07:42:14,379 - INFO - joeynmt.training - Epoch   7, Step:    25900, Batch Loss:     1.531283, Batch Acc: 0.574286, Tokens per Sec:     1593, Lr: 0.000300
2023-06-03 07:42:57,782 - INFO - joeynmt.training - Epoch   7, Step:    26000, Batch Loss:     1.243701, Batch Acc: 0.573475, Tokens per Sec:     1633, Lr: 0.000300
2023-06-03 07:42:57,782 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 07:47:12,897 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.74, acc:   0.51, generation: 255.0954[sec], evaluation: 0.0000[sec]
2023-06-03 07:47:12,898 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 07:47:13,144 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/23500.ckpt
2023-06-03 07:47:13,147 - INFO - joeynmt.training - Example #0
2023-06-03 07:47:13,147 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 07:47:13,147 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 07:47:13,147 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'sli@@', 'de', 'for', 'the', 'gl@@', 'ac@@', 'i@@', 'l', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ac@@', 'i@@', 'fic', 'h@@', 'ad@@', ',', 'which', 'is', 'about', '4@@', '8', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'cor@@', 'ded', 'in', 'the', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'in', 'the', 'United', 'St@@', 'at@@', 'es.', '</s>']
2023-06-03 07:47:13,148 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 07:47:13,148 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 07:47:13,148 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slide slide for the glacil glacial calacial calacial calacific had, which is about 48 the size of the United States, it's recorded in the United States, it's restretched in the United States.
2023-06-03 07:47:13,148 - INFO - joeynmt.training - Example #1
2023-06-03 07:47:13,148 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 07:47:13,148 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 07:47:13,148 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'ex@@', 'hi@@', 'b@@', 'ition', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 07:47:13,149 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 07:47:13,149 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 07:47:13,149 - INFO - joeynmt.training - 	Hypothesis: However, this is the gravity of the problem because it doesn't exhibition of the ice.
2023-06-03 07:47:13,149 - INFO - joeynmt.training - Example #2
2023-06-03 07:47:13,149 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 07:47:13,149 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 07:47:13,149 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'l', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 07:47:13,150 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 07:47:13,150 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 07:47:13,150 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial glacil is, in a sense, the heart of global climate system.
2023-06-03 07:47:13,150 - INFO - joeynmt.training - Example #3
2023-06-03 07:47:13,150 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 07:47:13,150 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 07:47:13,150 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'ex@@', 'p@@', 'and@@', 'ing', 'of', 'the', 's@@', 'am@@', 'e.', '</s>']
2023-06-03 07:47:13,151 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 07:47:13,151 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 07:47:13,151 - INFO - joeynmt.training - 	Hypothesis: It's expanding of the same.
2023-06-03 07:47:13,151 - INFO - joeynmt.training - Example #4
2023-06-03 07:47:13,151 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 07:47:13,151 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 07:47:13,151 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 't', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 07:47:13,152 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 07:47:13,152 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 07:47:13,152 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick cart on the last 25 years.
2023-06-03 07:47:58,171 - INFO - joeynmt.training - Epoch   7, Step:    26100, Batch Loss:     1.442076, Batch Acc: 0.571057, Tokens per Sec:     1544, Lr: 0.000300
2023-06-03 07:48:41,860 - INFO - joeynmt.training - Epoch   7, Step:    26200, Batch Loss:     1.453157, Batch Acc: 0.575957, Tokens per Sec:     1591, Lr: 0.000300
2023-06-03 07:49:26,418 - INFO - joeynmt.training - Epoch   7, Step:    26300, Batch Loss:     1.819252, Batch Acc: 0.569106, Tokens per Sec:     1587, Lr: 0.000300
2023-06-03 07:50:09,285 - INFO - joeynmt.training - Epoch   7, Step:    26400, Batch Loss:     1.348928, Batch Acc: 0.573188, Tokens per Sec:     1627, Lr: 0.000300
2023-06-03 07:50:53,963 - INFO - joeynmt.training - Epoch   7, Step:    26500, Batch Loss:     1.313452, Batch Acc: 0.579749, Tokens per Sec:     1590, Lr: 0.000300
2023-06-03 07:50:53,964 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 07:54:02,078 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.68, acc:   0.52, generation: 188.0969[sec], evaluation: 0.0000[sec]
2023-06-03 07:54:02,083 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 07:54:02,322 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/24000.ckpt
2023-06-03 07:54:02,325 - INFO - joeynmt.training - Example #0
2023-06-03 07:54:02,325 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 07:54:02,326 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 07:54:02,326 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'th@@', 'y', 'gl@@', 'ac@@', 'i@@', 'al', 'c@@', 'al@@', 'c@@', 'ul@@', 'us', 'that', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', '4@@', '8', 'the', 'si@@', 'z@@', 'z@@', 'es', 'of', '4@@', '0@@', '-@@', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'is', 're@@', 'ar@@', 'ed', '4@@', '0@@', '.', '</s>']
2023-06-03 07:54:02,326 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 07:54:02,326 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 07:54:02,326 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides to show that the glacial calotthy glacial calculus that for almost three million years had the size 48 the sizzes of 40-percent of the United States is reared 40.
2023-06-03 07:54:02,326 - INFO - joeynmt.training - Example #1
2023-06-03 07:54:02,326 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 07:54:02,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 07:54:02,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'g@@', 'a@@', 'th@@', 'e.', '</s>']
2023-06-03 07:54:02,327 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 07:54:02,327 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 07:54:02,327 - INFO - joeynmt.training - 	Hypothesis: But this is underestimate the gravity of the ice because it doesn't show the gathe.
2023-06-03 07:54:02,327 - INFO - joeynmt.training - Example #2
2023-06-03 07:54:02,327 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 07:54:02,327 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 07:54:02,327 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'gl@@', 'ac@@', 'i@@', 'al', 'war@@', 'm@@', 'er', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'global', 'clim@@', 'ate', 'clim@@', 'ate', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 07:54:02,328 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 07:54:02,328 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 07:54:02,328 - INFO - joeynmt.training - 	Hypothesis: The artical glacial warmer is, in a sense, the global climate climate climate system.
2023-06-03 07:54:02,328 - INFO - joeynmt.training - Example #3
2023-06-03 07:54:02,328 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 07:54:02,328 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 07:54:02,328 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'ex@@', 'p@@', 'and@@', 'ing', 'from', 'the', 'w@@', 'in@@', 'ver@@', '-@@', 'u@@', 'p', 'ex@@', 'ac@@', 'tly', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 07:54:02,329 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 07:54:02,329 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 07:54:02,329 - INFO - joeynmt.training - 	Hypothesis: It is expanding from the winver-up exactly the summer.
2023-06-03 07:54:02,329 - INFO - joeynmt.training - Example #4
2023-06-03 07:54:02,329 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 07:54:02,329 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 07:54:02,329 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'bo@@', 'ard', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 07:54:02,330 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 07:54:02,330 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 07:54:02,330 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick careboard of the last 25 years.
2023-06-03 07:54:29,833 - INFO - joeynmt.training - Epoch   7: total training loss 5610.84
2023-06-03 07:54:29,833 - INFO - joeynmt.training - EPOCH 8
2023-06-03 07:54:47,029 - INFO - joeynmt.training - Epoch   8, Step:    26600, Batch Loss:     1.216116, Batch Acc: 0.598366, Tokens per Sec:     1637, Lr: 0.000300
2023-06-03 07:55:30,274 - INFO - joeynmt.training - Epoch   8, Step:    26700, Batch Loss:     1.361949, Batch Acc: 0.590174, Tokens per Sec:     1613, Lr: 0.000300
2023-06-03 07:56:14,616 - INFO - joeynmt.training - Epoch   8, Step:    26800, Batch Loss:     1.373387, Batch Acc: 0.600513, Tokens per Sec:     1574, Lr: 0.000300
2023-06-03 07:56:58,666 - INFO - joeynmt.training - Epoch   8, Step:    26900, Batch Loss:     1.451134, Batch Acc: 0.594012, Tokens per Sec:     1607, Lr: 0.000300
2023-06-03 07:57:43,725 - INFO - joeynmt.training - Epoch   8, Step:    27000, Batch Loss:     1.355249, Batch Acc: 0.597854, Tokens per Sec:     1562, Lr: 0.000300
2023-06-03 07:57:43,725 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:00:30,685 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.74, acc:   0.51, generation: 166.9440[sec], evaluation: 0.0000[sec]
2023-06-03 08:00:30,927 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/24500.ckpt
2023-06-03 08:00:30,930 - INFO - joeynmt.training - Example #0
2023-06-03 08:00:30,930 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:00:30,930 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:00:30,930 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ta', 'sh@@', 'if@@', 't', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', 'United', 'St@@', 'ates', 'is', 're@@', 'stre@@', 't@@', 't@@', 'ch@@', 'ed', 'in', '4@@', '0@@', '.', '</s>']
2023-06-03 08:00:30,930 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:00:30,931 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:00:30,931 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the glacial calotta shift for almost three million years had the size of 48 dimensions of the United States is restrettched in 40.
2023-06-03 08:00:30,931 - INFO - joeynmt.training - Example #1
2023-06-03 08:00:30,931 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:00:30,931 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:00:30,931 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'show@@', '.', '</s>']
2023-06-03 08:00:30,931 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:00:30,931 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:00:30,932 - INFO - joeynmt.training - 	Hypothesis: But this underestimate the gravity of the problem because it doesn't show the ice show.
2023-06-03 08:00:30,932 - INFO - joeynmt.training - Example #2
2023-06-03 08:00:30,932 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:00:30,932 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:00:30,932 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'global', 'clim@@', 'ate', 'is', 'in', 'a', 'sense', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 08:00:30,932 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:00:30,932 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:00:30,932 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is in a sense of the global climate is in a sense of global climate system.
2023-06-03 08:00:30,932 - INFO - joeynmt.training - Example #3
2023-06-03 08:00:30,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:00:30,933 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:00:30,933 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'tur@@', 'ns', 'out', 'to', 'the', 'w@@', 'int@@', 'er', 'and', 'r@@', 'i@@', 'se', 'and', 'it', 'was', 'ext@@', 'rem@@', 'e.', '</s>']
2023-06-03 08:00:30,933 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:00:30,933 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:00:30,933 - INFO - joeynmt.training - 	Hypothesis: It turns out to the winter and rise and it was extreme.
2023-06-03 08:00:30,933 - INFO - joeynmt.training - Example #4
2023-06-03 08:00:30,933 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:00:30,934 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:00:30,934 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'bo@@', 'ard', 'of', 'over', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:00:30,934 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:00:30,934 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:00:30,934 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick careboard of over the last 25 years.
2023-06-03 08:01:15,705 - INFO - joeynmt.training - Epoch   8, Step:    27100, Batch Loss:     1.422176, Batch Acc: 0.592285, Tokens per Sec:     1565, Lr: 0.000300
2023-06-03 08:01:59,156 - INFO - joeynmt.training - Epoch   8, Step:    27200, Batch Loss:     1.271945, Batch Acc: 0.595829, Tokens per Sec:     1652, Lr: 0.000300
2023-06-03 08:02:43,833 - INFO - joeynmt.training - Epoch   8, Step:    27300, Batch Loss:     1.341731, Batch Acc: 0.589584, Tokens per Sec:     1583, Lr: 0.000300
2023-06-03 08:03:27,527 - INFO - joeynmt.training - Epoch   8, Step:    27400, Batch Loss:     1.611082, Batch Acc: 0.593626, Tokens per Sec:     1599, Lr: 0.000300
2023-06-03 08:04:11,705 - INFO - joeynmt.training - Epoch   8, Step:    27500, Batch Loss:     1.403485, Batch Acc: 0.588282, Tokens per Sec:     1613, Lr: 0.000300
2023-06-03 08:04:11,705 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:07:39,414 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.70, acc:   0.52, generation: 207.6913[sec], evaluation: 0.0000[sec]
2023-06-03 08:07:39,652 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/25500.ckpt
2023-06-03 08:07:39,655 - INFO - joeynmt.training - Example #0
2023-06-03 08:07:39,655 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:07:39,655 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:07:39,655 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de', 'sli@@', 'de', 'de@@', 'mon@@', 'str@@', 'ate', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'is', 'a', 'gl@@', 'ac@@', 'i@@', 'al', 'so@@', 'i@@', 'l', 'fi@@', 'el@@', 'd,', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 's,', 'is', 're@@', 'cor@@', 'ding', 'in', 'the', 'United', 'St@@', 'ates', 'is', 're@@', 'cor@@', 'ded', 'to', '4@@', '0@@', '.', '</s>']
2023-06-03 08:07:39,656 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:07:39,656 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:07:39,656 - INFO - joeynmt.training - 	Hypothesis: I showed these slide slide demonstrate demonstrate that is a glacial soil field, for almost three million years had the size of the United States of the continents, is recording in the United States is recorded to 40.
2023-06-03 08:07:39,656 - INFO - joeynmt.training - Example #1
2023-06-03 08:07:39,656 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:07:39,656 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:07:39,656 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 08:07:39,657 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:07:39,657 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:07:39,657 - INFO - joeynmt.training - 	Hypothesis: But this underestimate the gravity of the problem because it doesn't show the ice of the ice.
2023-06-03 08:07:39,657 - INFO - joeynmt.training - Example #2
2023-06-03 08:07:39,657 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:07:39,657 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:07:39,657 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', '--', 'in', 'a', 'way,', 'the', 'pul@@', 's@@', 'l@@', 'au@@', 'gh@@', 'ing', 'of', 'the', 'global', 'system@@', '.', '</s>']
2023-06-03 08:07:39,657 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:07:39,657 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:07:39,658 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is -- in a way, the pulslaughing of the global system.
2023-06-03 08:07:39,658 - INFO - joeynmt.training - Example #3
2023-06-03 08:07:39,658 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:07:39,658 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:07:39,658 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'ex@@', 'p@@', 'and', 's@@', 'ki@@', 'e', 'ex@@', 'p@@', 'and@@', 's', 'and', 'it', 'comes', 'a@@', 'es@@', 'p@@', 'and@@', '.', '</s>']
2023-06-03 08:07:39,658 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:07:39,658 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:07:39,658 - INFO - joeynmt.training - 	Hypothesis: You expand skie expands and it comes aespand.
2023-06-03 08:07:39,658 - INFO - joeynmt.training - Example #4
2023-06-03 08:07:39,659 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:07:39,659 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:07:39,659 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'e@@', 'ful', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:07:39,659 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:07:39,659 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:07:39,659 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick careeful on the last 25 years.
2023-06-03 08:08:24,322 - INFO - joeynmt.training - Epoch   8, Step:    27600, Batch Loss:     1.497615, Batch Acc: 0.584520, Tokens per Sec:     1531, Lr: 0.000300
2023-06-03 08:09:07,769 - INFO - joeynmt.training - Epoch   8, Step:    27700, Batch Loss:     1.535896, Batch Acc: 0.585930, Tokens per Sec:     1665, Lr: 0.000300
2023-06-03 08:09:51,385 - INFO - joeynmt.training - Epoch   8, Step:    27800, Batch Loss:     1.389243, Batch Acc: 0.589493, Tokens per Sec:     1611, Lr: 0.000300
2023-06-03 08:10:35,278 - INFO - joeynmt.training - Epoch   8, Step:    27900, Batch Loss:     1.404250, Batch Acc: 0.586962, Tokens per Sec:     1602, Lr: 0.000300
2023-06-03 08:11:19,456 - INFO - joeynmt.training - Epoch   8, Step:    28000, Batch Loss:     1.497050, Batch Acc: 0.587931, Tokens per Sec:     1575, Lr: 0.000300
2023-06-03 08:11:19,456 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:14:09,280 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.75, ppl:   5.73, acc:   0.52, generation: 169.8078[sec], evaluation: 0.0000[sec]
2023-06-03 08:14:09,533 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/25000.ckpt
2023-06-03 08:14:09,536 - INFO - joeynmt.training - Example #0
2023-06-03 08:14:09,537 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:14:09,537 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:14:09,537 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'al@@', 'th@@', 'y', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'al@@', 'th@@', 'y', '--', 'that', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', 'which', 'is', 're@@', 'cor@@', 'rec@@', 't@@', 'ed.', '</s>']
2023-06-03 08:14:09,537 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:14:09,537 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:14:09,537 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that that the glacial healthy glacial healthy -- that almost three million years had the size of the United States continental continents, which is recorrected.
2023-06-03 08:14:09,537 - INFO - joeynmt.training - Example #1
2023-06-03 08:14:09,537 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:14:09,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:14:09,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 08:14:09,538 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:14:09,538 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:14:09,538 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't show the gravity of the ice.
2023-06-03 08:14:09,538 - INFO - joeynmt.training - Example #2
2023-06-03 08:14:09,538 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:14:09,538 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:14:09,538 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'gl@@', 'ac@@', 'i@@', 'al', 'is', '--', 'in', 'a', 'sense', 'of', 'the', 'g@@', 'lob@@', 'al', 'clim@@', 'ate', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 08:14:09,539 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:14:09,539 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:14:09,539 - INFO - joeynmt.training - 	Hypothesis: The art glacial is -- in a sense of the global climate of global climate system.
2023-06-03 08:14:09,539 - INFO - joeynmt.training - Example #3
2023-06-03 08:14:09,539 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:14:09,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:14:09,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'f@@', 'el@@', 't', 'u@@', 'p', 'and', 'it', 'w@@', 'ould', 'be', 're@@', 'ally', 'come', 'u@@', 'p', 'to', 'the', 'sum@@', 'm@@', 'er', 'of', 'the', 'sum@@', 'm@@', 'er', 'and', 'it', 'w@@', 'ould', 'be', 'the', 's@@', 'un@@', '.', '</s>']
2023-06-03 08:14:09,540 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:14:09,540 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:14:09,540 - INFO - joeynmt.training - 	Hypothesis: It felt up and it would be really come up to the summer of the summer and it would be the sun.
2023-06-03 08:14:09,540 - INFO - joeynmt.training - Example #4
2023-06-03 08:14:09,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:14:09,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:14:09,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'a', 'very', 'qu@@', 'ick@@', 'ly', 'car@@', 'e@@', 'bo@@', 'ard', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:14:09,541 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:14:09,541 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:14:09,541 - INFO - joeynmt.training - 	Hypothesis: The next slide is a very quickly careboard of the last 25 years.
2023-06-03 08:14:53,075 - INFO - joeynmt.training - Epoch   8, Step:    28100, Batch Loss:     1.472481, Batch Acc: 0.587930, Tokens per Sec:     1606, Lr: 0.000300
2023-06-03 08:15:36,511 - INFO - joeynmt.training - Epoch   8, Step:    28200, Batch Loss:     1.229123, Batch Acc: 0.586982, Tokens per Sec:     1598, Lr: 0.000300
2023-06-03 08:16:22,893 - INFO - joeynmt.training - Epoch   8, Step:    28300, Batch Loss:     1.425590, Batch Acc: 0.579125, Tokens per Sec:     1516, Lr: 0.000300
2023-06-03 08:17:07,346 - INFO - joeynmt.training - Epoch   8, Step:    28400, Batch Loss:     1.511082, Batch Acc: 0.581666, Tokens per Sec:     1553, Lr: 0.000300
2023-06-03 08:17:51,598 - INFO - joeynmt.training - Epoch   8, Step:    28500, Batch Loss:     1.354356, Batch Acc: 0.591462, Tokens per Sec:     1611, Lr: 0.000300
2023-06-03 08:17:51,598 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:21:04,011 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.72, acc:   0.52, generation: 192.3967[sec], evaluation: 0.0000[sec]
2023-06-03 08:21:04,263 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/27000.ckpt
2023-06-03 08:21:04,266 - INFO - joeynmt.training - Example #0
2023-06-03 08:21:04,266 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:21:04,266 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:21:04,266 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'de@@', 'mon@@', 'str@@', 'ate', 'sli@@', 'de', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ter', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 'al', 'contin@@', 'ent@@', 's,', '</s>']
2023-06-03 08:21:04,267 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:21:04,267 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:21:04,267 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these demonstrate slide that the glacial calotter for almost three million years had the size of the United States of the continental continental continental continental continental continental continental continents,
2023-06-03 08:21:04,267 - INFO - joeynmt.training - Example #1
2023-06-03 08:21:04,267 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:21:04,267 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:21:04,267 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'the', 'wh@@', 'ol@@', 'e', 'thing', 'about', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 08:21:04,268 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:21:04,268 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:21:04,268 - INFO - joeynmt.training - 	Hypothesis: And the whole thing about the gravity of the problem because it doesn't show the ice.
2023-06-03 08:21:04,268 - INFO - joeynmt.training - Example #2
2023-06-03 08:21:04,268 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:21:04,268 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:21:04,268 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', '--', 'in', 'a', 'sense', 'of', 'global', 'clim@@', 'ate', 'cu@@', 'st@@', 'om@@', 'in@@', 'al', 'system@@', '.', '</s>']
2023-06-03 08:21:04,269 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:21:04,269 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:21:04,269 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is -- in a sense of global climate custominal system.
2023-06-03 08:21:04,269 - INFO - joeynmt.training - Example #3
2023-06-03 08:21:04,269 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:21:04,269 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:21:04,269 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'ands', 'out', 'of', 'the', 'w@@', 'in@@', 'k', 'and', 'you', 'w@@', 'ould', 'be', 'ex@@', 'p@@', 'lo@@', 'de@@', 'd.', '</s>']
2023-06-03 08:21:04,269 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:21:04,270 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:21:04,270 - INFO - joeynmt.training - 	Hypothesis: It expands out of the wink and you would be exploded.
2023-06-03 08:21:04,270 - INFO - joeynmt.training - Example #4
2023-06-03 08:21:04,270 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:21:04,270 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:21:04,270 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'ful', 'for', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:21:04,270 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:21:04,270 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:21:04,271 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick careful for the last 25 years.
2023-06-03 08:21:49,116 - INFO - joeynmt.training - Epoch   8, Step:    28600, Batch Loss:     1.492674, Batch Acc: 0.589545, Tokens per Sec:     1531, Lr: 0.000300
2023-06-03 08:22:33,559 - INFO - joeynmt.training - Epoch   8, Step:    28700, Batch Loss:     1.479050, Batch Acc: 0.584703, Tokens per Sec:     1587, Lr: 0.000300
2023-06-03 08:23:18,122 - INFO - joeynmt.training - Epoch   8, Step:    28800, Batch Loss:     1.417141, Batch Acc: 0.591261, Tokens per Sec:     1581, Lr: 0.000300
2023-06-03 08:24:01,586 - INFO - joeynmt.training - Epoch   8, Step:    28900, Batch Loss:     1.641667, Batch Acc: 0.581996, Tokens per Sec:     1612, Lr: 0.000300
2023-06-03 08:24:45,846 - INFO - joeynmt.training - Epoch   8, Step:    29000, Batch Loss:     1.493779, Batch Acc: 0.583512, Tokens per Sec:     1589, Lr: 0.000300
2023-06-03 08:24:45,846 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:28:42,789 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.62, acc:   0.52, generation: 236.9259[sec], evaluation: 0.0000[sec]
2023-06-03 08:28:42,795 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 08:28:43,033 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/26000.ckpt
2023-06-03 08:28:43,036 - INFO - joeynmt.training - Example #0
2023-06-03 08:28:43,037 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:28:43,037 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:28:43,037 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'that', 'the', 'c@@', 'al@@', 'o@@', 'ti@@', 'c', 'cal@@', 'ot@@', 'ing', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', 'United', 'St@@', 'ates', 'is', 're@@', 'stre@@', 't@@', 'ch@@', 'ed', '4@@', '8', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 're@@', 'ar@@', 't', 'of', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 08:28:43,037 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:28:43,037 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:28:43,037 - INFO - joeynmt.training - 	Hypothesis: And I showed these slides to show these slides that the calotic caloting artica, which is almost three million years had the size of 48 dimensions of the United States is restretched 48 percent of the United States reart of the 40.
2023-06-03 08:28:43,037 - INFO - joeynmt.training - Example #1
2023-06-03 08:28:43,038 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:28:43,038 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:28:43,038 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'show', 'the', 'gr@@', 'av@@', 'ity', 'show@@', 's', 'the', 'spac@@', 'e.', '</s>']
2023-06-03 08:28:43,038 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:28:43,038 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:28:43,038 - INFO - joeynmt.training - 	Hypothesis: But this is the gravity of the problem because it doesn't show show the gravity shows the space.
2023-06-03 08:28:43,038 - INFO - joeynmt.training - Example #2
2023-06-03 08:28:43,039 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:28:43,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:28:43,039 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't', 'gl@@', 'ac@@', 'i@@', 'al', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'clim@@', 'ate', 'cu@@', 'st@@', 'om@@', 'p@@', 'in@@', 'i@@', 'c', 'system@@', '.', '</s>']
2023-06-03 08:28:43,039 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:28:43,039 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:28:43,039 - INFO - joeynmt.training - 	Hypothesis: The art glacial glacial is in a sense of the climate custompinic system.
2023-06-03 08:28:43,039 - INFO - joeynmt.training - Example #3
2023-06-03 08:28:43,039 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:28:43,039 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:28:43,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 's', 'of', 'the', 'w@@', 'ould', 'be', 'ex@@', 'p@@', 'and', 'it', 'from', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 08:28:43,040 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:28:43,040 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:28:43,040 - INFO - joeynmt.training - 	Hypothesis: It expands of the would be expand it from the summer.
2023-06-03 08:28:43,040 - INFO - joeynmt.training - Example #4
2023-06-03 08:28:43,040 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:28:43,040 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:28:43,040 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'bo@@', 'ard', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:28:43,041 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:28:43,041 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:28:43,041 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick careboard on the last 25 years.
2023-06-03 08:29:27,643 - INFO - joeynmt.training - Epoch   8, Step:    29100, Batch Loss:     1.419056, Batch Acc: 0.584788, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 08:30:11,035 - INFO - joeynmt.training - Epoch   8, Step:    29200, Batch Loss:     1.528272, Batch Acc: 0.585275, Tokens per Sec:     1579, Lr: 0.000300
2023-06-03 08:30:54,360 - INFO - joeynmt.training - Epoch   8, Step:    29300, Batch Loss:     1.311810, Batch Acc: 0.582928, Tokens per Sec:     1656, Lr: 0.000300
2023-06-03 08:31:39,397 - INFO - joeynmt.training - Epoch   8, Step:    29400, Batch Loss:     1.422705, Batch Acc: 0.588299, Tokens per Sec:     1582, Lr: 0.000300
2023-06-03 08:32:23,817 - INFO - joeynmt.training - Epoch   8, Step:    29500, Batch Loss:     1.262184, Batch Acc: 0.586367, Tokens per Sec:     1572, Lr: 0.000300
2023-06-03 08:32:23,817 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:35:39,186 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.61, acc:   0.52, generation: 195.3508[sec], evaluation: 0.0000[sec]
2023-06-03 08:35:39,187 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 08:35:39,441 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/28000.ckpt
2023-06-03 08:35:39,444 - INFO - joeynmt.training - Example #0
2023-06-03 08:35:39,444 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:35:39,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:35:39,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'of', 'this', 'sli@@', 'de', 'for', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'ice', 'as', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'dimen@@', 'sion@@', 's', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 's,', 'and', 'it', 're@@', 'cor@@', 'ded', 'in', 'the', 'si@@', 'ze', 'of', '4@@', '0@@', '.', '</s>']
2023-06-03 08:35:39,445 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:35:39,445 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:35:39,445 - INFO - joeynmt.training - 	Hypothesis: And I showed these slides of this slide for demonstrate that the ice ice ice ice ice ice ice ice as three million years had the size of 48 dimensions of the United States of the continents, and it recorded in the size of 40.
2023-06-03 08:35:39,445 - INFO - joeynmt.training - Example #1
2023-06-03 08:35:39,445 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:35:39,445 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:35:39,445 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'ice', 'be@@', 'cause', 'it', "doesn't", 'show', 'you', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'ice', 'of', 'the', 'g@@', 'a@@', 'i@@', 's.', '</s>']
2023-06-03 08:35:39,446 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:35:39,446 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:35:39,446 - INFO - joeynmt.training - 	Hypothesis: However, the gravity of the problem because it doesn't show the ice of the ice of the ice of ice because it doesn't show you the ice of the ice of the ice of the ice of the ice of the gais.
2023-06-03 08:35:39,446 - INFO - joeynmt.training - Example #2
2023-06-03 08:35:39,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:35:39,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:35:39,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'y', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'd@@', 'i@@', 'st@@', 'ur@@', 'ing', 'the', 'global', 'system@@', '.', '</s>']
2023-06-03 08:35:39,447 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:35:39,447 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:35:39,447 - INFO - joeynmt.training - 	Hypothesis: The arthy glacial is in a sense, the disturing the global system.
2023-06-03 08:35:39,447 - INFO - joeynmt.training - Example #3
2023-06-03 08:35:39,447 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:35:39,447 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:35:39,447 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'is', 'ex@@', 'p@@', 'and@@', 's', 'and', 'it', 'w@@', 'ould', 'be', 'ext@@', 'rem@@', 'e.', '</s>']
2023-06-03 08:35:39,448 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:35:39,448 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:35:39,448 - INFO - joeynmt.training - 	Hypothesis: It is expands and it would be extreme.
2023-06-03 08:35:39,448 - INFO - joeynmt.training - Example #4
2023-06-03 08:35:39,448 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:35:39,448 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:35:39,448 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 't', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:35:39,449 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:35:39,449 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:35:39,449 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick cart on the last 25 years.
2023-06-03 08:36:23,120 - INFO - joeynmt.training - Epoch   8, Step:    29600, Batch Loss:     1.582231, Batch Acc: 0.584037, Tokens per Sec:     1579, Lr: 0.000300
2023-06-03 08:37:06,900 - INFO - joeynmt.training - Epoch   8, Step:    29700, Batch Loss:     1.434081, Batch Acc: 0.585848, Tokens per Sec:     1611, Lr: 0.000300
2023-06-03 08:37:50,649 - INFO - joeynmt.training - Epoch   8, Step:    29800, Batch Loss:     1.547843, Batch Acc: 0.583561, Tokens per Sec:     1643, Lr: 0.000300
2023-06-03 08:38:34,444 - INFO - joeynmt.training - Epoch   8, Step:    29900, Batch Loss:     1.471112, Batch Acc: 0.584436, Tokens per Sec:     1612, Lr: 0.000300
2023-06-03 08:39:18,414 - INFO - joeynmt.training - Epoch   8, Step:    30000, Batch Loss:     1.331788, Batch Acc: 0.582735, Tokens per Sec:     1589, Lr: 0.000300
2023-06-03 08:39:18,415 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:42:53,254 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.57, acc:   0.52, generation: 214.8199[sec], evaluation: 0.0000[sec]
2023-06-03 08:42:53,260 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 08:42:53,493 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/28500.ckpt
2023-06-03 08:42:53,497 - INFO - joeynmt.training - Example #0
2023-06-03 08:42:53,497 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:42:53,497 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:42:53,497 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'sli@@', 'de', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'al@@', 'th@@', 'y', 'gl@@', 'ac@@', 'i@@', 'al', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', '4@@', '8', 'United', 'St@@', 'ates', 'contin@@', 'ent@@', 's,', 'and', 'it', 're@@', 'cor@@', 'ded', 'to', '4@@', '8', 'percent', 'percent', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'si@@', 'ze', 'of', 'th@@', 'ese', '4@@', '0@@', '.', '</s>']
2023-06-03 08:42:53,498 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:42:53,498 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:42:53,498 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that slide that the glacial healthy glacial for almost three million years had the size of 48 United States continents, and it recorded to 48 percent percent of the United States of the United States of the United States of the United States of the size of these 40.
2023-06-03 08:42:53,498 - INFO - joeynmt.training - Example #1
2023-06-03 08:42:53,498 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:42:53,498 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:42:53,498 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'sp@@', 'ess@@', 'or', 'of', 'ic@@', 'e.', '</s>']
2023-06-03 08:42:53,499 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:42:53,499 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:42:53,499 - INFO - joeynmt.training - 	Hypothesis: However, the gravity of the gravity of the problem because it doesn't show the spessor of ice.
2023-06-03 08:42:53,499 - INFO - joeynmt.training - Example #2
2023-06-03 08:42:53,499 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:42:53,499 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:42:53,499 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'global', 'clim@@', 'ate', 'in', 'a', 'sense', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 08:42:53,500 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:42:53,500 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:42:53,500 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is in a sense of global climate in a sense of global climate system.
2023-06-03 08:42:53,500 - INFO - joeynmt.training - Example #3
2023-06-03 08:42:53,500 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:42:53,500 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:42:53,500 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'ands', 'out', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'about', 'the', 'w@@', 'il@@', 'd.', '</s>']
2023-06-03 08:42:53,500 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:42:53,500 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:42:53,501 - INFO - joeynmt.training - 	Hypothesis: It expands out of the winter and really really really about the wild.
2023-06-03 08:42:53,501 - INFO - joeynmt.training - Example #4
2023-06-03 08:42:53,501 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:42:53,501 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:42:53,501 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'will', 'be', 'a', 'f@@', 'ast@@', '-@@', 'car@@', 't', 'car@@', 'e@@', 'bo@@', 'ard', 're@@', 'st', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:42:53,501 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:42:53,501 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:42:53,501 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a fast-cart careboard rest on the last 25 years.
2023-06-03 08:43:37,591 - INFO - joeynmt.training - Epoch   8, Step:    30100, Batch Loss:     1.363332, Batch Acc: 0.584029, Tokens per Sec:     1576, Lr: 0.000300
2023-06-03 08:44:22,113 - INFO - joeynmt.training - Epoch   8, Step:    30200, Batch Loss:     1.520041, Batch Acc: 0.579018, Tokens per Sec:     1528, Lr: 0.000300
2023-06-03 08:45:05,043 - INFO - joeynmt.training - Epoch   8, Step:    30300, Batch Loss:     1.321734, Batch Acc: 0.585634, Tokens per Sec:     1650, Lr: 0.000300
2023-06-03 08:45:29,616 - INFO - joeynmt.training - Epoch   8: total training loss 5440.71
2023-06-03 08:45:29,616 - INFO - joeynmt.training - EPOCH 9
2023-06-03 08:45:48,977 - INFO - joeynmt.training - Epoch   9, Step:    30400, Batch Loss:     1.499156, Batch Acc: 0.615361, Tokens per Sec:     1534, Lr: 0.000300
2023-06-03 08:46:32,891 - INFO - joeynmt.training - Epoch   9, Step:    30500, Batch Loss:     1.480272, Batch Acc: 0.614206, Tokens per Sec:     1611, Lr: 0.000300
2023-06-03 08:46:32,891 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:49:39,202 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.61, acc:   0.52, generation: 186.2942[sec], evaluation: 0.0000[sec]
2023-06-03 08:49:39,447 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/27500.ckpt
2023-06-03 08:49:39,450 - INFO - joeynmt.training - Example #0
2023-06-03 08:49:39,450 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:49:39,450 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:49:39,450 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'for', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ta', 'ar@@', 'tic@@', 'le', 'of', 'the', 'ar@@', 'tic@@', 'le', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'is', 'st@@', 'ate', '4@@', '0@@', '0@@', '.', '</s>']
2023-06-03 08:49:39,451 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:49:39,451 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:49:39,451 - INFO - joeynmt.training - 	Hypothesis: I showed these slides I showed these slides for the glacial calotta article of the article of the United States of the United States of the United States is state 400.
2023-06-03 08:49:39,451 - INFO - joeynmt.training - Example #1
2023-06-03 08:49:39,451 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:49:39,451 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:49:39,451 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 't@@', 'ex@@', 't', 'of', 'the', 'problem', 'be@@', 'cause', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ice', 'ex@@', 'hi@@', 'b@@', 'ition', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 08:49:39,452 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:49:39,452 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:49:39,452 - INFO - joeynmt.training - 	Hypothesis: This is a text of the problem because the gravity of the ice exhibition of the ice.
2023-06-03 08:49:39,452 - INFO - joeynmt.training - Example #2
2023-06-03 08:49:39,452 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:49:39,452 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:49:39,452 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'ad', 'of', 'the', 'ar@@', 'c@@', 'y@@', 'c@@', 'le', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 08:49:39,453 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:49:39,453 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:49:39,453 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial head of the arcycle of the global climate system.
2023-06-03 08:49:39,453 - INFO - joeynmt.training - Example #3
2023-06-03 08:49:39,453 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:49:39,453 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:49:39,453 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'ands', 'out', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'it', 're@@', 'ally', 're@@', 'st@@', 'ra@@', 'i@@', 'ght', 'and', 'it', 're@@', 'ally', 'is.', '</s>']
2023-06-03 08:49:39,454 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:49:39,454 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:49:39,454 - INFO - joeynmt.training - 	Hypothesis: It expands out of the winter and it really restraight and it really is.
2023-06-03 08:49:39,454 - INFO - joeynmt.training - Example #4
2023-06-03 08:49:39,454 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:49:39,454 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:49:39,454 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'f@@', 'ast@@', '-@@', 'f@@', 'ast@@', '-@@', 'f@@', 'ast@@', '-@@', 'b@@', 're@@', 'ak', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:49:39,455 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:49:39,455 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:49:39,455 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a fast-fast-fast-break on the last 25 years.
2023-06-03 08:50:23,949 - INFO - joeynmt.training - Epoch   9, Step:    30600, Batch Loss:     1.304069, Batch Acc: 0.610143, Tokens per Sec:     1571, Lr: 0.000300
2023-06-03 08:51:07,502 - INFO - joeynmt.training - Epoch   9, Step:    30700, Batch Loss:     1.368813, Batch Acc: 0.607800, Tokens per Sec:     1602, Lr: 0.000300
2023-06-03 08:51:51,696 - INFO - joeynmt.training - Epoch   9, Step:    30800, Batch Loss:     1.422354, Batch Acc: 0.603830, Tokens per Sec:     1580, Lr: 0.000300
2023-06-03 08:52:36,949 - INFO - joeynmt.training - Epoch   9, Step:    30900, Batch Loss:     1.403103, Batch Acc: 0.606348, Tokens per Sec:     1537, Lr: 0.000300
2023-06-03 08:53:21,635 - INFO - joeynmt.training - Epoch   9, Step:    31000, Batch Loss:     1.498936, Batch Acc: 0.598655, Tokens per Sec:     1577, Lr: 0.000300
2023-06-03 08:53:21,635 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 08:56:58,806 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.74, ppl:   5.67, acc:   0.52, generation: 217.1546[sec], evaluation: 0.0000[sec]
2023-06-03 08:56:59,055 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/26500.ckpt
2023-06-03 08:56:59,059 - INFO - joeynmt.training - Example #0
2023-06-03 08:56:59,059 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 08:56:59,060 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 08:56:59,060 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'ar@@', 'tic@@', 'le', 'gl@@', 'ac@@', 'i@@', 'l', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', '4@@', '0@@', '.', '</s>']
2023-06-03 08:56:59,060 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 08:56:59,060 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 08:56:59,060 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the article glacil artica, which is about three million years had the size of the United States of the United States of the United States of the United States of the 40.
2023-06-03 08:56:59,060 - INFO - joeynmt.training - Example #1
2023-06-03 08:56:59,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 08:56:59,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 08:56:59,061 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['This', 'is', 'a', 'under@@', 'est@@', 'ab@@', 'li@@', 'sh@@', 'ed', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 08:56:59,061 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 08:56:59,061 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 08:56:59,061 - INFO - joeynmt.training - 	Hypothesis: This is a underestablished the gravity of the problem because it doesn't show the ice.
2023-06-03 08:56:59,061 - INFO - joeynmt.training - Example #2
2023-06-03 08:56:59,061 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 08:56:59,061 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 08:56:59,062 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'global', 'clim@@', 'ate', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 08:56:59,062 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 08:56:59,062 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 08:56:59,062 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is in a sense of the global climate heart of global climate system.
2023-06-03 08:56:59,062 - INFO - joeynmt.training - Example #3
2023-06-03 08:56:59,062 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 08:56:59,062 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 08:56:59,062 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'ands', 'out', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'you', 'go', 'to', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 08:56:59,063 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 08:56:59,063 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 08:56:59,063 - INFO - joeynmt.training - 	Hypothesis: It expands out of the winter and you go to the summer.
2023-06-03 08:56:59,063 - INFO - joeynmt.training - Example #4
2023-06-03 08:56:59,063 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 08:56:59,063 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 08:56:59,063 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'a', 'f@@', 'ast@@', '-@@', 'car@@', 'e@@', 'bo@@', 'ard', 'of', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 08:56:59,064 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 08:56:59,064 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 08:56:59,064 - INFO - joeynmt.training - 	Hypothesis: The next slide is a fast-careboard of the last 25 years.
2023-06-03 08:57:44,101 - INFO - joeynmt.training - Epoch   9, Step:    31100, Batch Loss:     1.368085, Batch Acc: 0.602582, Tokens per Sec:     1541, Lr: 0.000300
2023-06-03 08:58:28,261 - INFO - joeynmt.training - Epoch   9, Step:    31200, Batch Loss:     1.536525, Batch Acc: 0.599094, Tokens per Sec:     1565, Lr: 0.000300
2023-06-03 08:59:11,883 - INFO - joeynmt.training - Epoch   9, Step:    31300, Batch Loss:     1.402261, Batch Acc: 0.596497, Tokens per Sec:     1599, Lr: 0.000300
2023-06-03 08:59:55,575 - INFO - joeynmt.training - Epoch   9, Step:    31400, Batch Loss:     1.264965, Batch Acc: 0.599232, Tokens per Sec:     1597, Lr: 0.000300
2023-06-03 09:00:39,245 - INFO - joeynmt.training - Epoch   9, Step:    31500, Batch Loss:     1.255740, Batch Acc: 0.599980, Tokens per Sec:     1625, Lr: 0.000300
2023-06-03 09:00:39,246 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 09:04:29,111 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.64, acc:   0.52, generation: 229.8487[sec], evaluation: 0.0000[sec]
2023-06-03 09:04:29,480 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/31000.ckpt
2023-06-03 09:04:29,482 - INFO - joeynmt.helpers - delete /Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/31000.ckpt
2023-06-03 09:04:29,482 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/31000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/31000.ckpt')
2023-06-03 09:04:29,483 - INFO - joeynmt.training - Example #0
2023-06-03 09:04:29,483 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 09:04:29,483 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 09:04:29,483 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'you', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ta', 'ar@@', 'tic@@', 'le,', 'for', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', 'it@@', "'s", 're@@', 'stre@@', 't@@', 'ch@@', 'ed', 'by', '4@@', '0@@', 's.', '</s>']
2023-06-03 09:04:29,484 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 09:04:29,484 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 09:04:29,484 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show you that the glacial calotta article, for almost three million years had the size of the United States of the United States of the United States, it's restretched by 40s.
2023-06-03 09:04:29,484 - INFO - joeynmt.training - Example #1
2023-06-03 09:04:29,484 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 09:04:29,484 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 09:04:29,484 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'under@@', 'est@@', 'ab@@', 'li@@', 'sh@@', 'ed', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', '<unk>', 'un@@', 'd@@', 'le', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 09:04:29,485 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 09:04:29,485 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 09:04:29,485 - INFO - joeynmt.training - 	Hypothesis: But this underestablished the gravity of the problem because it doesn't show the <unk> undle of the ice.
2023-06-03 09:04:29,485 - INFO - joeynmt.training - Example #2
2023-06-03 09:04:29,485 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 09:04:29,485 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 09:04:29,485 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'gl@@', 'ac@@', 'i@@', 'al', 'is', '--', 'in', 'a', 'way,', 'the', 'd@@', 'i@@', 'st@@', 'ant', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 09:04:29,486 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 09:04:29,486 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 09:04:29,486 - INFO - joeynmt.training - 	Hypothesis: The artical glacial is -- in a way, the distant of global climate system.
2023-06-03 09:04:29,486 - INFO - joeynmt.training - Example #3
2023-06-03 09:04:29,486 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 09:04:29,486 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 09:04:29,486 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['You', 'can', 'ex@@', 'p@@', 'and', 'out', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'it', 'comes', 'a@@', 'es@@', 'si@@', 've', 'out', 'of', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 09:04:29,486 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 09:04:29,487 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 09:04:29,487 - INFO - joeynmt.training - 	Hypothesis: You can expand out of the winter and it comes aessive out of the summer.
2023-06-03 09:04:29,487 - INFO - joeynmt.training - Example #4
2023-06-03 09:04:29,487 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 09:04:29,487 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 09:04:29,487 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'bo@@', 'ard', 'about', 'the', 'v@@', 'ac@@', 'u@@', 'um@@', '.', '</s>']
2023-06-03 09:04:29,487 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 09:04:29,487 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 09:04:29,487 - INFO - joeynmt.training - 	Hypothesis: The next slide is a quick careboard about the vacuum.
2023-06-03 09:05:14,324 - INFO - joeynmt.training - Epoch   9, Step:    31600, Batch Loss:     1.253842, Batch Acc: 0.599553, Tokens per Sec:     1554, Lr: 0.000300
2023-06-03 09:05:58,331 - INFO - joeynmt.training - Epoch   9, Step:    31700, Batch Loss:     1.416306, Batch Acc: 0.596110, Tokens per Sec:     1577, Lr: 0.000300
2023-06-03 09:06:42,618 - INFO - joeynmt.training - Epoch   9, Step:    31800, Batch Loss:     1.325221, Batch Acc: 0.597325, Tokens per Sec:     1600, Lr: 0.000300
2023-06-03 09:07:26,113 - INFO - joeynmt.training - Epoch   9, Step:    31900, Batch Loss:     1.236752, Batch Acc: 0.597240, Tokens per Sec:     1610, Lr: 0.000300
2023-06-03 09:08:09,137 - INFO - joeynmt.training - Epoch   9, Step:    32000, Batch Loss:     1.410753, Batch Acc: 0.598708, Tokens per Sec:     1626, Lr: 0.000300
2023-06-03 09:08:09,137 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 09:11:22,193 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.73, ppl:   5.63, acc:   0.52, generation: 193.0402[sec], evaluation: 0.0000[sec]
2023-06-03 09:11:22,439 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/31500.ckpt
2023-06-03 09:11:22,441 - INFO - joeynmt.helpers - delete /Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/31500.ckpt
2023-06-03 09:11:22,442 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/31500.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/31500.ckpt')
2023-06-03 09:11:22,442 - INFO - joeynmt.training - Example #0
2023-06-03 09:11:22,442 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 09:11:22,442 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 09:11:22,442 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'sli@@', 'de', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'al@@', 'th@@', 'y', 'gl@@', 'ac@@', 'i@@', 'l', 'c@@', 'al@@', 'c@@', 'ul@@', 'ar@@', ',', 'that', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'of', 'the', 'United', 'St@@', 'at@@', 'es,', '</s>']
2023-06-03 09:11:22,443 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 09:11:22,443 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 09:11:22,443 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that slide to show that the glacial healthy glacil calcular, that almost three million years had the size of the United States of the United States of the continental of the United States,
2023-06-03 09:11:22,443 - INFO - joeynmt.training - Example #1
2023-06-03 09:11:22,443 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 09:11:22,443 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 09:11:22,443 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'problem', 'be@@', 'cause', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 09:11:22,444 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 09:11:22,444 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 09:11:22,444 - INFO - joeynmt.training - 	Hypothesis: The problem because the gravity of the problem because the gravity of the ice.
2023-06-03 09:11:22,444 - INFO - joeynmt.training - Example #2
2023-06-03 09:11:22,444 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 09:11:22,444 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 09:11:22,444 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'hear@@', 't', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 09:11:22,445 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 09:11:22,445 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 09:11:22,445 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is in a sense, the heart of global climate system.
2023-06-03 09:11:22,445 - INFO - joeynmt.training - Example #3
2023-06-03 09:11:22,445 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 09:11:22,445 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 09:11:22,445 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'ands', 'out', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'it', 'comes', 'ab@@', 'out.', '</s>']
2023-06-03 09:11:22,446 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 09:11:22,446 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 09:11:22,446 - INFO - joeynmt.training - 	Hypothesis: It expands out of the winter and it comes about.
2023-06-03 09:11:22,446 - INFO - joeynmt.training - Example #4
2023-06-03 09:11:22,446 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 09:11:22,446 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 09:11:22,446 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'bo@@', 'ard', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 09:11:22,447 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 09:11:22,447 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 09:11:22,447 - INFO - joeynmt.training - 	Hypothesis: The next slide slide will be a quick careboard on the last 25 years.
2023-06-03 09:12:06,091 - INFO - joeynmt.training - Epoch   9, Step:    32100, Batch Loss:     1.318971, Batch Acc: 0.598469, Tokens per Sec:     1589, Lr: 0.000300
2023-06-03 09:12:50,340 - INFO - joeynmt.training - Epoch   9, Step:    32200, Batch Loss:     1.453733, Batch Acc: 0.598980, Tokens per Sec:     1591, Lr: 0.000300
2023-06-03 09:13:33,699 - INFO - joeynmt.training - Epoch   9, Step:    32300, Batch Loss:     1.388238, Batch Acc: 0.597331, Tokens per Sec:     1611, Lr: 0.000300
2023-06-03 09:14:18,658 - INFO - joeynmt.training - Epoch   9, Step:    32400, Batch Loss:     1.566889, Batch Acc: 0.595956, Tokens per Sec:     1577, Lr: 0.000300
2023-06-03 09:15:03,153 - INFO - joeynmt.training - Epoch   9, Step:    32500, Batch Loss:     1.310963, Batch Acc: 0.597614, Tokens per Sec:     1612, Lr: 0.000300
2023-06-03 09:15:03,153 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 09:18:27,569 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.60, acc:   0.52, generation: 204.3985[sec], evaluation: 0.0000[sec]
2023-06-03 09:18:27,785 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/32000.ckpt
2023-06-03 09:18:27,788 - INFO - joeynmt.helpers - delete /Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/32000.ckpt
2023-06-03 09:18:27,788 - WARNING - joeynmt.helpers - Wanted to delete old checkpoint /Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/32000.ckpt but file does not exist. ([Errno 2] No such file or directory: '/Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/32000.ckpt')
2023-06-03 09:18:27,788 - INFO - joeynmt.training - Example #0
2023-06-03 09:18:27,789 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 09:18:27,789 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 09:18:27,789 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'sli@@', 'de', 'for', 'a', 'few', 'years', 'years', 'from', 'now,', 'which', 'is', 'almost', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'si@@', 'ze', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'of', 'the', 'United', 'St@@', 'ates', 'of', 'the', 'contin@@', 'ent@@', 'al', 'United', 'St@@', 'at@@', 'es,', 'it', 're@@', 'cor@@', 'ded', '4@@', '0@@', '.', '</s>']
2023-06-03 09:18:27,789 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 09:18:27,789 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 09:18:27,789 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides slide for a few years years from now, which is almost three million years had the size of the United States of the continental size of the United States of the continental of the United States of the continental United States, it recorded 40.
2023-06-03 09:18:27,790 - INFO - joeynmt.training - Example #1
2023-06-03 09:18:27,790 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 09:18:27,790 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 09:18:27,790 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'under@@', 'est@@', 'ab@@', 'li@@', 'sh@@', 'ed', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'the', 'sp@@', 'ess@@', 'or', 'of', 'ice', 'ice', 'the', 'ic@@', 'e.', '</s>']
2023-06-03 09:18:27,790 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 09:18:27,790 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 09:18:27,790 - INFO - joeynmt.training - 	Hypothesis: But this underestablished the gravity of the problem because the spessor of ice ice the ice.
2023-06-03 09:18:27,790 - INFO - joeynmt.training - Example #2
2023-06-03 09:18:27,791 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 09:18:27,791 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 09:18:27,791 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'he@@', 'ad', 'is', 'in', 'a', 'sense', 'of', 'the', 'ar@@', 'c@@', 'tic@@', 'al', 'he@@', 'ad', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 09:18:27,791 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 09:18:27,791 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 09:18:27,791 - INFO - joeynmt.training - 	Hypothesis: The arthic head is in a sense of the arctical head of global climate system.
2023-06-03 09:18:27,791 - INFO - joeynmt.training - Example #3
2023-06-03 09:18:27,791 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 09:18:27,791 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 09:18:27,792 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'the', 'w@@', 'inter@@', 's', 'and', 'the', 'w@@', 'inter@@', 's', 'and', 'it', 'comes', 'u@@', 'p', 'to', 'the', 'sum@@', 'mer@@', '.', '</s>']
2023-06-03 09:18:27,792 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 09:18:27,792 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 09:18:27,792 - INFO - joeynmt.training - 	Hypothesis: It's the winters and the winters and it comes up to the summer.
2023-06-03 09:18:27,792 - INFO - joeynmt.training - Example #4
2023-06-03 09:18:27,792 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 09:18:27,792 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 09:18:27,792 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'e@@', 'bo@@', 'x@@', 'ed', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 09:18:27,793 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 09:18:27,793 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 09:18:27,793 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick careeboxed on the last 25 years.
2023-06-03 09:19:12,721 - INFO - joeynmt.training - Epoch   9, Step:    32600, Batch Loss:     1.530756, Batch Acc: 0.593323, Tokens per Sec:     1548, Lr: 0.000300
2023-06-03 09:19:56,199 - INFO - joeynmt.training - Epoch   9, Step:    32700, Batch Loss:     1.242945, Batch Acc: 0.590820, Tokens per Sec:     1609, Lr: 0.000300
2023-06-03 09:20:42,133 - INFO - joeynmt.training - Epoch   9, Step:    32800, Batch Loss:     1.480137, Batch Acc: 0.595874, Tokens per Sec:     1521, Lr: 0.000300
2023-06-03 09:21:25,788 - INFO - joeynmt.training - Epoch   9, Step:    32900, Batch Loss:     1.265031, Batch Acc: 0.592416, Tokens per Sec:     1615, Lr: 0.000300
2023-06-03 09:22:10,224 - INFO - joeynmt.training - Epoch   9, Step:    33000, Batch Loss:     1.294934, Batch Acc: 0.591287, Tokens per Sec:     1594, Lr: 0.000300
2023-06-03 09:22:10,225 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 09:25:40,874 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.56, acc:   0.52, generation: 210.6306[sec], evaluation: 0.0000[sec]
2023-06-03 09:25:40,875 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 09:25:41,130 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/29000.ckpt
2023-06-03 09:25:41,133 - INFO - joeynmt.training - Example #0
2023-06-03 09:25:41,134 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 09:25:41,134 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 09:25:41,134 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'of', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'he@@', 'al@@', 'th@@', 'y', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'al@@', 'th@@', 'y', 'for', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', '4@@', '8', 'the', 'si@@', 'ze', '4@@', '8', 'the', 'si@@', 'ze', '4@@', '8', 'the', 'si@@', 'ze', '4@@', '0@@', '-@@', 'y@@', 'ear@@', '-@@', 'ol@@', 'd.', '</s>']
2023-06-03 09:25:41,134 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 09:25:41,134 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 09:25:41,134 - INFO - joeynmt.training - 	Hypothesis: Last year I showed these slides of slides to show that the healthy glacial healthy for three million years had the size 48 the size 48 the size 48 the size 40-year-old.
2023-06-03 09:25:41,135 - INFO - joeynmt.training - Example #1
2023-06-03 09:25:41,135 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 09:25:41,135 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 09:25:41,135 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', 'o@@', 'we@@', 'ver@@', ',', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 's@@', 'am@@', 'm@@', 'it@@', 'e.', '</s>']
2023-06-03 09:25:41,135 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 09:25:41,135 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 09:25:41,136 - INFO - joeynmt.training - 	Hypothesis: However, the gravity of the problem because it doesn't show the sammite.
2023-06-03 09:25:41,136 - INFO - joeynmt.training - Example #2
2023-06-03 09:25:41,136 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 09:25:41,136 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 09:25:41,136 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'gl@@', 'ac@@', 'i@@', 'al', 'is', 'in', 'a', 'sense', 'of', 'the', 'global', 'clim@@', 'ate', 'hear@@', 't', 'of', 'global', 'system@@', '.', '</s>']
2023-06-03 09:25:41,136 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 09:25:41,136 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 09:25:41,136 - INFO - joeynmt.training - 	Hypothesis: The arthic glacial is in a sense of the global climate heart of global system.
2023-06-03 09:25:41,137 - INFO - joeynmt.training - Example #3
2023-06-03 09:25:41,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 09:25:41,137 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 09:25:41,137 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'and@@', 'es', 'out', 'of', 'the', 'w@@', 'int@@', 'er', 'and', 'it', 'was', 'ext@@', 'rem@@', 'e.', '</s>']
2023-06-03 09:25:41,137 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 09:25:41,137 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 09:25:41,137 - INFO - joeynmt.training - 	Hypothesis: It expandes out of the winter and it was extreme.
2023-06-03 09:25:41,137 - INFO - joeynmt.training - Example #4
2023-06-03 09:25:41,137 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 09:25:41,138 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 09:25:41,138 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'ful', 'car@@', 'to@@', 'on', 'for', 'the', 'last', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2023-06-03 09:25:41,138 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 09:25:41,138 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 09:25:41,138 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick careful cartoon for the last 25 years old.
2023-06-03 09:26:25,785 - INFO - joeynmt.training - Epoch   9, Step:    33100, Batch Loss:     1.297609, Batch Acc: 0.591218, Tokens per Sec:     1594, Lr: 0.000300
2023-06-03 09:27:09,302 - INFO - joeynmt.training - Epoch   9, Step:    33200, Batch Loss:     1.386680, Batch Acc: 0.591506, Tokens per Sec:     1620, Lr: 0.000300
2023-06-03 09:27:54,250 - INFO - joeynmt.training - Epoch   9, Step:    33300, Batch Loss:     1.370448, Batch Acc: 0.596598, Tokens per Sec:     1555, Lr: 0.000300
2023-06-03 09:28:38,884 - INFO - joeynmt.training - Epoch   9, Step:    33400, Batch Loss:     1.435524, Batch Acc: 0.592119, Tokens per Sec:     1532, Lr: 0.000300
2023-06-03 09:29:23,886 - INFO - joeynmt.training - Epoch   9, Step:    33500, Batch Loss:     1.358791, Batch Acc: 0.588833, Tokens per Sec:     1557, Lr: 0.000300
2023-06-03 09:29:23,887 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 09:32:59,004 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.72, ppl:   5.56, acc:   0.52, generation: 215.1015[sec], evaluation: 0.0000[sec]
2023-06-03 09:32:59,234 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/30500.ckpt
2023-06-03 09:32:59,238 - INFO - joeynmt.training - Example #0
2023-06-03 09:32:59,238 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 09:32:59,238 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 09:32:59,238 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'gl@@', 'ac@@', 'i@@', 'al', 'he@@', 'ad', 'of', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', 'of', 'three', 'million', 'years', 'had', 'the', 'si@@', 'ze', '4@@', '8', 'the', 'si@@', 'ze', '4@@', '8', 'the', 'si@@', 'ze', '4@@', '0@@', '.', '</s>']
2023-06-03 09:32:59,239 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 09:32:59,239 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 09:32:59,239 - INFO - joeynmt.training - 	Hypothesis: The year I showed these slides to show that the glacial head of artica, which is about three million years had the size of three million years had the size 48 the size 48 the size 40.
2023-06-03 09:32:59,239 - INFO - joeynmt.training - Example #1
2023-06-03 09:32:59,239 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 09:32:59,239 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 09:32:59,239 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['H@@', "e's", 'able', 'this', 'under@@', 'est@@', 'im@@', 'ate', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'gl@@', 'ac@@', 'i@@', 'er.', '</s>']
2023-06-03 09:32:59,240 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 09:32:59,240 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 09:32:59,240 - INFO - joeynmt.training - 	Hypothesis: He's able this underestimate the gravity of the problem because it doesn't show the glacier.
2023-06-03 09:32:59,240 - INFO - joeynmt.training - Example #2
2023-06-03 09:32:59,240 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 09:32:59,240 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 09:32:59,240 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 'th@@', 'ic', 'he@@', 'ad', 'is', 'in', 'a', 'sense', 'of', 'the', 'ar@@', 'c@@', 'y@@', 'c@@', 'le', 'of', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 09:32:59,241 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 09:32:59,241 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 09:32:59,241 - INFO - joeynmt.training - 	Hypothesis: The arthic head is in a sense of the arcycle of global climate system.
2023-06-03 09:32:59,241 - INFO - joeynmt.training - Example #3
2023-06-03 09:32:59,241 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 09:32:59,241 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 09:32:59,241 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['and', 'the', 's@@', 'ets', 'of', 'w@@', 'int@@', 'er', 'and', 'it', 'comes', 'u@@', 'p', 'to', 'sum@@', 'm@@', 'er', 'and', 'it', 'was', 'ext@@', 'rem@@', 'ely', 're@@', 'cor@@', 'd', 'of', 'the', 'w@@', 'il@@', 'd@@', 'life', 'and', 's@@', 'et.', '</s>']
2023-06-03 09:32:59,241 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 09:32:59,241 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 09:32:59,242 - INFO - joeynmt.training - 	Hypothesis: and the sets of winter and it comes up to summer and it was extremely record of the wildlife and set.
2023-06-03 09:32:59,242 - INFO - joeynmt.training - Example #4
2023-06-03 09:32:59,242 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 09:32:59,242 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 09:32:59,242 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'is', 'go@@', 'ing', 'to', 'be', 'a', 'qu@@', 'ick', 'car@@', 're@@', 'ally', 'on', 'the', 'last', '2@@', '5', 'years.', '</s>']
2023-06-03 09:32:59,242 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 09:32:59,242 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 09:32:59,242 - INFO - joeynmt.training - 	Hypothesis: The next slide is going to be a quick carreally on the last 25 years.
2023-06-03 09:33:42,646 - INFO - joeynmt.training - Epoch   9, Step:    33600, Batch Loss:     1.396292, Batch Acc: 0.596183, Tokens per Sec:     1626, Lr: 0.000300
2023-06-03 09:34:27,648 - INFO - joeynmt.training - Epoch   9, Step:    33700, Batch Loss:     1.349422, Batch Acc: 0.590264, Tokens per Sec:     1557, Lr: 0.000300
2023-06-03 09:35:09,911 - INFO - joeynmt.training - Epoch   9, Step:    33800, Batch Loss:     1.583627, Batch Acc: 0.592761, Tokens per Sec:     1622, Lr: 0.000300
2023-06-03 09:35:54,662 - INFO - joeynmt.training - Epoch   9, Step:    33900, Batch Loss:     1.514906, Batch Acc: 0.593491, Tokens per Sec:     1588, Lr: 0.000300
2023-06-03 09:36:38,656 - INFO - joeynmt.training - Epoch   9, Step:    34000, Batch Loss:     1.596789, Batch Acc: 0.592202, Tokens per Sec:     1578, Lr: 0.000300
2023-06-03 09:36:38,656 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-03 09:39:46,193 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   1.71, ppl:   5.53, acc:   0.52, generation: 187.5209[sec], evaluation: 0.0000[sec]
2023-06-03 09:39:46,199 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-03 09:39:46,428 - INFO - joeynmt.helpers - delete models/bpe_level_transformer2/29500.ckpt
2023-06-03 09:39:46,431 - INFO - joeynmt.training - Example #0
2023-06-03 09:39:46,432 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'or@@', 'so', 'ho', 'mo@@', 'str@@', 'at@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mo@@', 'str@@', 'are', 'ch@@', 'e', 'l@@', 'a', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'asi', 't@@', 're', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'le', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'Stati', 'U@@', 'n@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'de@@', 'l', '4@@', '0@@', '%@@', '.']
2023-06-03 09:39:46,432 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 'sli@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'last', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'at@@', 'es,', 'has', 'sh@@', 'run@@', 'k', 'by', '40', 'percent@@', '.']
2023-06-03 09:39:46,432 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['L@@', 'ast', 'y@@', 'ear@@', ',', 'I', 'show@@', 'ed', 'th@@', 'ese', 'sli@@', 'de@@', 's', 'to', 'show', 'that', 'the', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'al', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'al', 'of', 'the', 'ar@@', 'tic@@', 'a,', 'which', 'is', 'about', 'three', 'million', 'years', 'of', 'the', 'United', 'St@@', 'ates', 'N@@', 'et@@', 's,', 'it', 're@@', 'cor@@', 'ds', 'in', '4@@', '8', 'percent@@', '.', '</s>']
2023-06-03 09:39:46,432 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-03 09:39:46,432 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-03 09:39:46,432 - INFO - joeynmt.training - 	Hypothesis: Last year, I showed these slides to show that the calotta glacial calotta glacial of the artica, which is about three million years of the United States Nets, it records in 48 percent.
2023-06-03 09:39:46,432 - INFO - joeynmt.training - Example #1
2023-06-03 09:39:46,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 'tav@@', 'i@@', 'a', 'qu@@', 'est@@', 'o', 's@@', 'ot@@', 'to@@', 'val@@', 'ut@@', 'a', 'l@@', 'a', 'gr@@', 'av@@', 'ità', 'de@@', 'l', 'problema', 'perché', 'no@@', 'n', 'mostra', 'lo', 'sp@@', 'ess@@', 'ore', 'de@@', 'l', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-03 09:39:46,433 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'ness', 'of', 'this', 'partic@@', 'ular', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'ness', 'of', 'the', 'ic@@', 'e.']
2023-06-03 09:39:46,433 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'this', 'is', 'a', 'very', 's@@', 'we@@', 'at', 'the', 'gr@@', 'av@@', 'ity', 'of', 'the', 'problem', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'gl@@', 'ac@@', 'i@@', 'er.', '</s>']
2023-06-03 09:39:46,433 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-03 09:39:46,433 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-03 09:39:46,433 - INFO - joeynmt.training - 	Hypothesis: But this is a very sweat the gravity of the problem because it doesn't show the glacier.
2023-06-03 09:39:46,433 - INFO - joeynmt.training - Example #2
2023-06-03 09:39:46,433 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'certo', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'de@@', 'l', 'sistema', 'cli@@', 'mat@@', 'ico', 'g@@', 'lob@@', 'ale.']
2023-06-03 09:39:46,434 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'hear@@', 't', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.']
2023-06-03 09:39:46,434 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'ar@@', 't@@', 'ical', 'ar@@', 'c@@', 'ti@@', 'c', 'is', 'in', 'a', 'sen@@', 'se,', 'the', 'd@@', 'i@@', 'st@@', 'inc@@', 'tion', 'of', 'the', 'clim@@', 'ate', 'clim@@', 'ate', 'clim@@', 'ate', 'of', 'the', 'global', 'clim@@', 'ate', 'system@@', '.', '</s>']
2023-06-03 09:39:46,434 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-03 09:39:46,434 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-03 09:39:46,434 - INFO - joeynmt.training - 	Hypothesis: The artical arctic is in a sense, the distinction of the climate climate climate of the global climate system.
2023-06-03 09:39:46,434 - INFO - joeynmt.training - Example #3
2023-06-03 09:39:46,434 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'es@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'r@@', 'a', 'd@@', "'@@", 'est@@', 'at@@', 'e.']
2023-06-03 09:39:46,434 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-03 09:39:46,434 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['It', 'ex@@', 'p@@', 'ands', 'out', 'of', 'the', 'w@@', 'inter@@', 's', 'and', 'it', 're@@', 'ally', 're@@', 'duc@@', 'ed.', '</s>']
2023-06-03 09:39:46,435 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-03 09:39:46,435 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-03 09:39:46,435 - INFO - joeynmt.training - 	Hypothesis: It expands out of the winters and it really reduced.
2023-06-03 09:39:46,435 - INFO - joeynmt.training - Example #4
2023-06-03 09:39:46,435 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 's@@', 'si@@', 'm@@', 'a', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'rapi@@', 'd@@', 'a', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ultimi', '2@@', '5', 'anni.']
2023-06-03 09:39:46,435 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 'sli@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'rapi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happened', 'over', 'the', 'last', '2@@', '5', 'years.']
2023-06-03 09:39:46,435 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'nex@@', 't', 'sli@@', 'de', 'will', 'be', 'a', 'qu@@', 'ick', 'car@@', 'e@@', 'bo@@', 'ard', 'on', 'the', 'last', '2@@', '5', 'years', 'ol@@', 'd.', '</s>']
2023-06-03 09:39:46,436 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-03 09:39:46,436 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-03 09:39:46,436 - INFO - joeynmt.training - 	Hypothesis: The next slide will be a quick careboard on the last 25 years old.
2023-06-03 09:40:30,768 - INFO - joeynmt.training - Epoch   9, Step:    34100, Batch Loss:     1.572518, Batch Acc: 0.594940, Tokens per Sec:     1612, Lr: 0.000300
2023-06-03 09:40:56,591 - INFO - joeynmt.training - Epoch   9: total training loss 5311.76
2023-06-03 09:40:56,591 - INFO - joeynmt.training - EPOCH 10
2023-06-03 09:41:15,396 - INFO - joeynmt.training - Epoch  10, Step:    34200, Batch Loss:     1.366391, Batch Acc: 0.617658, Tokens per Sec:     1497, Lr: 0.000300
2023-06-03 09:41:36,664 - INFO - joeynmt.model - Building an encoder-decoder model...
2023-06-03 09:41:36,737 - INFO - joeynmt.model - Enc-dec model built.
2023-06-03 09:41:36,767 - INFO - joeynmt.helpers - Load model from /Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer2/34000.ckpt.
2023-06-03 09:41:36,773 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4552),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4552),
	loss_function=None)
2023-06-03 09:41:36,774 - INFO - joeynmt.prediction - Decoding on dev set...
2023-06-03 09:41:36,774 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
