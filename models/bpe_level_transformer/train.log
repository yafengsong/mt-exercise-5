2023-06-02 22:49:02,626 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2023-06-02 22:49:02,626 - INFO - joeynmt.helpers -                           cfg.name : bpe_level_transformer
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -                     cfg.data.train : data/train
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -                      cfg.data.test : data/test
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -                  cfg.data.src.lang : it
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : shared_models/joint-vocab.txt
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 2000
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data/bpe.codes.2000
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100
2023-06-02 22:49:02,627 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : shared_models/joint-vocab.txt
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 2000
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data/bpe.codes.2000
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2023-06-02 22:49:02,628 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -           cfg.training.eval_metric : bleu
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/bpe_level_transformer
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2023-06-02 22:49:02,629 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2023-06-02 22:49:02,630 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2023-06-02 22:49:02,631 - INFO - joeynmt.data - Building tokenizer...
2023-06-02 22:49:02,636 - INFO - joeynmt.tokenizers - it tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2023-06-02 22:49:02,636 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2023-06-02 22:49:02,636 - INFO - joeynmt.data - Loading train set...
2023-06-02 22:49:02,794 - INFO - joeynmt.data - Building vocabulary...
2023-06-02 22:49:02,906 - INFO - joeynmt.data - Loading dev set...
2023-06-02 22:49:02,908 - INFO - joeynmt.data - Loading test set...
2023-06-02 22:49:02,911 - INFO - joeynmt.data - Data loaded.
2023-06-02 22:49:02,911 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=100000, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2023-06-02 22:49:02,912 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=929, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2023-06-02 22:49:02,912 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=1566, src_lang=it, trg_lang=en, has_trg=True, random_subset=-1)
2023-06-02 22:49:02,912 - INFO - joeynmt.data - First training example:
	[SRC] A@@ l G@@ o@@ r@@ e: ar@@ rest@@ iamo i@@ l r@@ i@@ sc@@ al@@ d@@ amento glob@@ ale
	[TRG] A@@ l G@@ o@@ r@@ e: A@@ ver@@ ting the c@@ lim@@ ate cri@@ si@@ s
2023-06-02 22:49:02,912 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) of (7) a (8) and (9) in
2023-06-02 22:49:02,912 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) to (6) of (7) a (8) and (9) in
2023-06-02 22:49:02,912 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 3100
2023-06-02 22:49:02,912 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 3100
2023-06-02 22:49:02,914 - INFO - joeynmt.model - Building an encoder-decoder model...
2023-06-02 22:49:02,993 - INFO - joeynmt.model - Enc-dec model built.
2023-06-02 22:49:02,996 - INFO - joeynmt.model - Total params: 3692800
2023-06-02 22:49:02,997 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2023-06-02 22:49:02,997 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=3100),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=3100),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2023-06-02 22:49:02,997 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2023-06-02 22:49:02,998 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2023-06-02 22:49:02,998 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2023-06-02 22:49:02,998 - INFO - joeynmt.training - EPOCH 1
2023-06-02 22:49:36,533 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     4.004734, Batch Acc: 0.039102, Tokens per Sec:     2225, Lr: 0.000300
2023-06-02 22:50:13,593 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     3.856936, Batch Acc: 0.061635, Tokens per Sec:     1963, Lr: 0.000300
2023-06-02 22:50:54,470 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     3.726069, Batch Acc: 0.077194, Tokens per Sec:     1812, Lr: 0.000300
2023-06-02 22:51:34,881 - INFO - joeynmt.training - Epoch   1, Step:      400, Batch Loss:     3.654638, Batch Acc: 0.091638, Tokens per Sec:     1808, Lr: 0.000300
2023-06-02 22:52:15,118 - INFO - joeynmt.training - Epoch   1, Step:      500, Batch Loss:     3.489328, Batch Acc: 0.105873, Tokens per Sec:     1831, Lr: 0.000300
2023-06-02 22:52:15,118 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-02 22:57:13,486 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.53, ppl:  34.17, acc:   0.11, generation: 298.3317[sec], evaluation: 0.0000[sec]
2023-06-02 22:57:13,487 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-02 22:57:13,754 - INFO - joeynmt.training - Example #0
2023-06-02 22:57:13,755 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'o@@', 'r@@', 'so', 'ho', 'mostr@@', 'a@@', 't@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mostr@@', 'are', 'ch@@', 'e', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'as@@', 'i', 'tre', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'l@@', 'e', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2023-06-02 22:57:13,755 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'las@@', 't', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'a@@', 't@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2023-06-02 22:57:13,755 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'have', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'to', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'to', 'the', 'cause', 'the', 'cause', 'to', 'the', 'cause', 'to', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'to', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'the', 'f@@', '.', '</s>']
2023-06-02 22:57:13,755 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-02 22:57:13,755 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-02 22:57:13,756 - INFO - joeynmt.training - 	Hypothesis: And I have the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause to the cause the cause the cause to the cause the cause to the cause to the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause to the cause the cause the cause the the f.
2023-06-02 22:57:13,756 - INFO - joeynmt.training - Example #1
2023-06-02 22:57:13,756 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'quest@@', 'o', 's@@', 'ot@@', 't@@', 'o@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'no@@', 'n', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-02 22:57:13,756 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'u@@', 'l@@', 'ar', 'prob@@', 'le@@', 'm', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2023-06-02 22:57:13,756 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'have', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'and', 'I', 'have', 'the', 'cause', 'the', 'the', 'the', 'f@@', '.', '</s>']
2023-06-02 22:57:13,756 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-02 22:57:13,757 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-02 22:57:13,757 - INFO - joeynmt.training - 	Hypothesis: And I have the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause and I have the cause the the the f.
2023-06-02 22:57:13,757 - INFO - joeynmt.training - Example #2
2023-06-02 22:57:13,757 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2023-06-02 22:57:13,757 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'h@@', 'ear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2023-06-02 22:57:13,757 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'have', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'f@@', '.', '</s>']
2023-06-02 22:57:13,757 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-02 22:57:13,757 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-02 22:57:13,757 - INFO - joeynmt.training - 	Hypothesis: And I have the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the f.
2023-06-02 22:57:13,758 - INFO - joeynmt.training - Example #3
2023-06-02 22:57:13,758 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'e@@', 's@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'ra', 'd@@', "'@@", 'est@@', 'ate.']
2023-06-02 22:57:13,758 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-02 22:57:13,758 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'have', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'f@@', '.', '</s>']
2023-06-02 22:57:13,758 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-02 22:57:13,758 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-02 22:57:13,758 - INFO - joeynmt.training - 	Hypothesis: And I have the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the f.
2023-06-02 22:57:13,758 - INFO - joeynmt.training - Example #4
2023-06-02 22:57:13,759 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2023-06-02 22:57:13,759 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happen@@', 'ed', 'over', 'the', 'las@@', 't', '2@@', '5', 'year@@', 's.']
2023-06-02 22:57:13,759 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'have', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'cause', 'the', 'f@@', '.', '</s>']
2023-06-02 22:57:13,759 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-02 22:57:13,759 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-02 22:57:13,759 - INFO - joeynmt.training - 	Hypothesis: And I have the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the cause the f.
2023-06-02 22:57:56,422 - INFO - joeynmt.training - Epoch   1, Step:      600, Batch Loss:     3.551007, Batch Acc: 0.117336, Tokens per Sec:     1769, Lr: 0.000300
2023-06-02 22:58:37,898 - INFO - joeynmt.training - Epoch   1, Step:      700, Batch Loss:     3.315895, Batch Acc: 0.132179, Tokens per Sec:     1813, Lr: 0.000300
2023-06-02 22:59:20,660 - INFO - joeynmt.training - Epoch   1, Step:      800, Batch Loss:     3.401470, Batch Acc: 0.137682, Tokens per Sec:     1744, Lr: 0.000300
2023-06-02 23:00:00,109 - INFO - joeynmt.training - Epoch   1, Step:      900, Batch Loss:     3.215209, Batch Acc: 0.147748, Tokens per Sec:     1879, Lr: 0.000300
2023-06-02 23:00:36,943 - INFO - joeynmt.training - Epoch   1, Step:     1000, Batch Loss:     3.159062, Batch Acc: 0.160984, Tokens per Sec:     1987, Lr: 0.000300
2023-06-02 23:00:36,943 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-02 23:05:54,161 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   3.26, ppl:  25.98, acc:   0.16, generation: 317.1760[sec], evaluation: 0.0000[sec]
2023-06-02 23:05:54,163 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-02 23:05:54,402 - INFO - joeynmt.training - Example #0
2023-06-02 23:05:54,402 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'o@@', 'r@@', 'so', 'ho', 'mostr@@', 'a@@', 't@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mostr@@', 'are', 'ch@@', 'e', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'as@@', 'i', 'tre', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'l@@', 'e', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2023-06-02 23:05:54,402 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'las@@', 't', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'a@@', 't@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2023-06-02 23:05:54,402 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'I', 'can', 'be', 'a', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'little', 'of', 'the', 's@@', 'e', 'of', 'the', 'b@@', 'ut', 'the', 's@@', 'e', 'of', 'the', 's@@', 'e', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', '<unk>', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 't', 'of', 'the', 'the', 'the', 'the', 're@@', 'ally', 're@@', 'ally', 'can', 'have', 'a', 'little', 'ople', 'that', 'we', 'can', 'have', 'have', 'have', 'have', 'have', 'have', 'have', 'a', 'a', 'a', 'a']
2023-06-02 23:05:54,402 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-02 23:05:54,403 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-02 23:05:54,403 - INFO - joeynmt.training - 	Hypothesis: And I can be a little little little little little little little little of the se of the but the se of the se of the <unk> ididididididididididididididididididididididididi<unk> idididididididididit of the the the the really really can have a little ople that we can have have have have have have have a a a a
2023-06-02 23:05:54,403 - INFO - joeynmt.training - Example #1
2023-06-02 23:05:54,403 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'quest@@', 'o', 's@@', 'ot@@', 't@@', 'o@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'no@@', 'n', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-02 23:05:54,403 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'u@@', 'l@@', 'ar', 'prob@@', 'le@@', 'm', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2023-06-02 23:05:54,403 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['B@@', 'ut', 'the', 'first', 'thing', 'to', 'be', 'a', 'lot', 'of', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'it', 'was', 'to', 'be', 'to', 'be', 'to', 'be', 'a', 'of', 'the', 'b@@', 'ut', 'you', 'can', 'have', 'a', 'of', 'the', 'b@@', 'b@@', 'o@@', '<unk>', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 'i@@', 't', 'a', 'a']
2023-06-02 23:05:54,404 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-02 23:05:54,404 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-02 23:05:54,404 - INFO - joeynmt.training - 	Hypothesis: But the first thing to be a lot of the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but it was to be to be to be a of the but you can have a of the bbo<unk> iiiiiiiiiiiiiiit a a
2023-06-02 23:05:54,404 - INFO - joeynmt.training - Example #2
2023-06-02 23:05:54,404 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2023-06-02 23:05:54,404 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'h@@', 'ear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2023-06-02 23:05:54,404 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'little', 'little', 'little', 'of', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', '<unk>', 'i@@', '<unk>', 'i@@', '<unk>', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 't', 'of', 'the', 'b@@', 'ut', 'the', 's@@', 'e', 'of', 'the', 'b@@', 'u@@', 'l@@', 'e', 'of', 'the', 's@@', '.', '</s>']
2023-06-02 23:05:54,405 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-02 23:05:54,405 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-02 23:05:54,405 - INFO - joeynmt.training - 	Hypothesis: The little little little of the but the but the but the but the but the but the but the but the but the but the but the but the but the <unk> i<unk> i<unk> idididit of the but the se of the bule of the s.
2023-06-02 23:05:54,405 - INFO - joeynmt.training - Example #3
2023-06-02 23:05:54,405 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'e@@', 's@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'ra', 'd@@', "'@@", 'est@@', 'ate.']
2023-06-02 23:05:54,405 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-02 23:05:54,405 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['We', 'can', 'have', 'a', 'f@@', 'o@@', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 'd@@', 'i@@', 's.', '</s>']
2023-06-02 23:05:54,406 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-02 23:05:54,406 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-02 23:05:54,406 - INFO - joeynmt.training - 	Hypothesis: We can have a fo<unk> <unk> <unk> <unk> <unk> <unk> <unk> idididididididididididididididis.
2023-06-02 23:05:54,406 - INFO - joeynmt.training - Example #4
2023-06-02 23:05:54,406 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2023-06-02 23:05:54,406 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happen@@', 'ed', 'over', 'the', 'las@@', 't', '2@@', '5', 'year@@', 's.']
2023-06-02 23:05:54,406 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'of', 'the', 'first', 'ople', 'of', 'the', 'first', 'ople', 'of', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', 'the', 'b@@', 'ut', '</s>']
2023-06-02 23:05:54,407 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-02 23:05:54,407 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-02 23:05:54,407 - INFO - joeynmt.training - 	Hypothesis: The first of the first ople of the first ople of the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but the but
2023-06-02 23:06:42,596 - INFO - joeynmt.training - Epoch   1, Step:     1100, Batch Loss:     3.100284, Batch Acc: 0.173611, Tokens per Sec:     1511, Lr: 0.000300
2023-06-02 23:07:30,104 - INFO - joeynmt.training - Epoch   1, Step:     1200, Batch Loss:     3.050502, Batch Acc: 0.184979, Tokens per Sec:     1541, Lr: 0.000300
2023-06-02 23:08:14,123 - INFO - joeynmt.training - Epoch   1, Step:     1300, Batch Loss:     3.099843, Batch Acc: 0.195288, Tokens per Sec:     1676, Lr: 0.000300
2023-06-02 23:08:56,011 - INFO - joeynmt.training - Epoch   1, Step:     1400, Batch Loss:     3.049290, Batch Acc: 0.204583, Tokens per Sec:     1802, Lr: 0.000300
2023-06-02 23:09:38,948 - INFO - joeynmt.training - Epoch   1, Step:     1500, Batch Loss:     2.862834, Batch Acc: 0.215511, Tokens per Sec:     1749, Lr: 0.000300
2023-06-02 23:09:38,948 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-02 23:14:37,321 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.97, ppl:  19.57, acc:   0.20, generation: 298.3380[sec], evaluation: 0.0000[sec]
2023-06-02 23:14:37,322 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-02 23:14:37,539 - INFO - joeynmt.training - Example #0
2023-06-02 23:14:37,539 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'o@@', 'r@@', 'so', 'ho', 'mostr@@', 'a@@', 't@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mostr@@', 'are', 'ch@@', 'e', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'as@@', 'i', 'tre', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'l@@', 'e', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2023-06-02 23:14:37,539 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'las@@', 't', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'a@@', 't@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2023-06-02 23:14:37,539 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'that', 'I', 'thin@@', 'k', 'in', 'the', 'first', 'time', 'to', 'be', 're@@', 'ally', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 'pe@@', 'ople', 'in', 'the', 'pe@@', 'ople', 'in', 'the', 'h@@', 'o@@', 'n@@', 's', 'of', 'the', 'pe@@', 'ople', 'in', 'the', 'h@@', 'o@@', 'n@@', 's', 'of', 'the', 'pe@@', 'ople', 'in', 'the', '<unk>', 'i@@', 'l', 'of', 'the', 'h@@', 'o@@', 'n@@', '.', '</s>']
2023-06-02 23:14:37,540 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-02 23:14:37,540 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-02 23:14:37,540 - INFO - joeynmt.training - 	Hypothesis: The first thing that I think in the first time to be really the really of the really of the really of the really of the people in the people in the hons of the people in the hons of the people in the <unk> il of the hon.
2023-06-02 23:14:37,540 - INFO - joeynmt.training - Example #1
2023-06-02 23:14:37,540 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'quest@@', 'o', 's@@', 'ot@@', 't@@', 'o@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'no@@', 'n', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-02 23:14:37,540 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'u@@', 'l@@', 'ar', 'prob@@', 'le@@', 'm', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2023-06-02 23:14:37,540 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'that', 'the', 'first', 'first', 'thing', 'that', 'the', 'ide@@', 'a', 'a', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'the', 'a@@', 'n@@', '.', '</s>']
2023-06-02 23:14:37,541 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-02 23:14:37,541 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-02 23:14:37,541 - INFO - joeynmt.training - 	Hypothesis: The first thing that the first first thing that the idea a of the really of the really of the really really really really really really really really really really really really really really really really really the an.
2023-06-02 23:14:37,541 - INFO - joeynmt.training - Example #2
2023-06-02 23:14:37,541 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2023-06-02 23:14:37,541 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'h@@', 'ear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2023-06-02 23:14:37,541 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'is', 'a', 'lot', 'of', 'the', 'first', 'of', 'the', 'ide@@', 'a', 'a', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', '<unk>', 'i@@', 'co@@', 'p@@', 'e.', '</s>']
2023-06-02 23:14:37,542 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-02 23:14:37,542 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-02 23:14:37,542 - INFO - joeynmt.training - 	Hypothesis: The first is a lot of the first of the idea a of the really of the really of the really of the really of the really of the <unk> icope.
2023-06-02 23:14:37,542 - INFO - joeynmt.training - Example #3
2023-06-02 23:14:37,542 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'e@@', 's@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'ra', 'd@@', "'@@", 'est@@', 'ate.']
2023-06-02 23:14:37,542 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-02 23:14:37,542 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ["It's", 'a', 'lot', 'of', 'the', '<unk>', 'i@@', 'r', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'es', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'es', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'es', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'es', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'es', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'es', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'es', 'of', 'the', '<unk>', 'i@@', 'd@@', 'i@@', 'd@@', 's.', '</s>']
2023-06-02 23:14:37,542 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-02 23:14:37,542 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-02 23:14:37,542 - INFO - joeynmt.training - 	Hypothesis: It's a lot of the <unk> ir of the <unk> idies of the <unk> idies of the <unk> idies of the <unk> idies of the <unk> idies of the <unk> idies of the <unk> idies of the <unk> idids.
2023-06-02 23:14:37,543 - INFO - joeynmt.training - Example #4
2023-06-02 23:14:37,543 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2023-06-02 23:14:37,543 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happen@@', 'ed', 'over', 'the', 'las@@', 't', '2@@', '5', 'year@@', 's.']
2023-06-02 23:14:37,543 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'first', 'to', 'the', 'first', 'first', 'first', 'to', 'be', 'a', 'lot', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 'pe@@', 'ople', 'to', 'be', 'a', 'lot', 'of', 'the', 'h@@', 'o@@', 'n@@', 's.', '</s>']
2023-06-02 23:14:37,543 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-02 23:14:37,543 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-02 23:14:37,543 - INFO - joeynmt.training - 	Hypothesis: The first first to the first first first to be a lot of the really of the really of the really of the really of the really of the people to be a lot of the hons.
2023-06-02 23:15:16,544 - INFO - joeynmt.training - Epoch   1, Step:     1600, Batch Loss:     2.981267, Batch Acc: 0.224947, Tokens per Sec:     1895, Lr: 0.000300
2023-06-02 23:15:56,473 - INFO - joeynmt.training - Epoch   1, Step:     1700, Batch Loss:     2.786468, Batch Acc: 0.225867, Tokens per Sec:     1886, Lr: 0.000300
2023-06-02 23:16:39,892 - INFO - joeynmt.training - Epoch   1, Step:     1800, Batch Loss:     2.851750, Batch Acc: 0.233208, Tokens per Sec:     1712, Lr: 0.000300
2023-06-02 23:17:20,323 - INFO - joeynmt.training - Epoch   1, Step:     1900, Batch Loss:     2.812877, Batch Acc: 0.237129, Tokens per Sec:     1826, Lr: 0.000300
2023-06-02 23:18:02,380 - INFO - joeynmt.training - Epoch   1, Step:     2000, Batch Loss:     2.715025, Batch Acc: 0.245190, Tokens per Sec:     1752, Lr: 0.000300
2023-06-02 23:18:02,380 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Greedy decoding with min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2023-06-02 23:22:54,725 - INFO - joeynmt.prediction - Evaluation result (greedy) loss:   2.82, ppl:  16.72, acc:   0.23, generation: 292.3106[sec], evaluation: 0.0000[sec]
2023-06-02 23:22:54,726 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!
2023-06-02 23:22:54,956 - INFO - joeynmt.training - Example #0
2023-06-02 23:22:54,956 - DEBUG - joeynmt.training - 	Tokenized source:     ["L'@@", 'anno', 'sc@@', 'o@@', 'r@@', 'so', 'ho', 'mostr@@', 'a@@', 't@@', 'o', 'queste', 'd@@', 'i@@', 'a@@', 'positi@@', 've', 'per', 'd@@', 'i@@', 'mostr@@', 'are', 'ch@@', 'e', 'la', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 'tic@@', 'a,', 'ch@@', 'e', 'per', 'qu@@', 'as@@', 'i', 'tre', 'milioni', 'd@@', 'i', 'anni', 'ha', 'av@@', 'uto', 'l@@', 'e', 'dimen@@', 'sioni', 'dei', '4@@', '8', 'St@@', 'ati', 'Un@@', 'iti', 'contin@@', 'ent@@', 'ali,', 's@@', 'i', 'è', 'r@@', 'i@@', 'stre@@', 't@@', 'ta', 'del', '4@@', '0@@', '%@@', '.']
2023-06-02 23:22:54,956 - DEBUG - joeynmt.training - 	Tokenized reference:  ['L@@', 'ast', 'year', 'I', 'show@@', 'ed', 'th@@', 'ese', 'two', 's@@', 'li@@', 'de@@', 's', 'so', 'that', 'de@@', 'mon@@', 'str@@', 'ate', 'that', 'the', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'cap@@', ',', 'which', 'for', 'most', 'of', 'the', 'las@@', 't', 'three', 'million', 'years', 'has', 'been', 'the', 'si@@', 'ze', 'of', 'the', 'low@@', 'er', '4@@', '8', 'st@@', 'a@@', 't@@', 'es,', 'has', 'sh@@', 'r@@', 'un@@', 'k', 'by', '4@@', '0', 'per@@', 'cent@@', '.']
2023-06-02 23:22:54,956 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'that', 'I', 'was', 'a', 'lot', 'of', 'pe@@', 'ople', 'are', 'go@@', 'ing', 'to', 'the', 're@@', 'ally', 'that', 'I', 'had', 'to', 'the', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'pe@@', 'ople', 'to', 'be', 'the', '1@@', '0@@', 's,', 'and', 'the', '1@@', '0@@', 's,', 'and', 'the', 'other', 'other', 'other', 'other', 'pe@@', 'ople', 'to', 'be', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'have', 'to', 'the', 'pe@@', 'ople', 'to', 'be', 'the', 're@@', 'ally', 're@@', 'ally', 're@@', 'ally', 'the', 'the', 're@@', 'ally', 're@@', 'ally', 'the', 'the', 'the', 'the', 'the', 'pe@@', 'pe@@', 'pe@@', 'pe@@', 'opl@@', 'e.', '</s>']
2023-06-02 23:22:54,957 - INFO - joeynmt.training - 	Source:     L'anno scorso ho mostrato queste diapositive  per dimostrare che la calotta glaciale artica,  che per quasi tre milioni di anni ha avuto  le dimensioni dei 48 Stati Uniti continentali,  si è ristretta del 40%.
2023-06-02 23:22:54,957 - INFO - joeynmt.training - 	Reference:  Last year I showed these two slides so that demonstrate that the arctic ice cap, which for most of the last three million years has been the size of the lower 48 states, has shrunk by 40 percent.
2023-06-02 23:22:54,957 - INFO - joeynmt.training - 	Hypothesis: The first thing that I was a lot of people are going to the really that I had to the other other other other other other other other other people to be the 10s, and the 10s, and the other other other other people to be the really really really really really really have to the people to be the really really really the the really really the the the the the pepepepeople.
2023-06-02 23:22:54,957 - INFO - joeynmt.training - Example #1
2023-06-02 23:22:54,957 - DEBUG - joeynmt.training - 	Tokenized source:     ['Tut@@', 't@@', 'av@@', 'ia', 'quest@@', 'o', 's@@', 'ot@@', 't@@', 'o@@', 'val@@', 'ut@@', 'a', 'la', 'gr@@', 'av@@', 'ità', 'del', 'problema', 'perché', 'no@@', 'n', 'mostr@@', 'a', 'lo', 'sp@@', 'ess@@', 'ore', 'del', 'gh@@', 'i@@', 'ac@@', 'ci@@', 'o.']
2023-06-02 23:22:54,957 - DEBUG - joeynmt.training - 	Tokenized reference:  ['B@@', 'ut', 'this', 'underst@@', 'ates', 'the', 'seri@@', 'ous@@', 'n@@', 'ess', 'of', 'this', 'partic@@', 'u@@', 'l@@', 'ar', 'prob@@', 'le@@', 'm', 'be@@', 'cause', 'it', "doesn't", 'show', 'the', 'th@@', 'ick@@', 'n@@', 'ess', 'of', 'the', 'ic@@', 'e.']
2023-06-02 23:22:54,957 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['And', 'this', 'is', 'the', 'first', 'thing', 'is', 'that', 'that', 'the', 're@@', 'ally', 'is', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not', 'not']
2023-06-02 23:22:54,958 - INFO - joeynmt.training - 	Source:     Tuttavia questo sottovaluta la gravità del problema  perché non mostra lo spessore del ghiaccio.
2023-06-02 23:22:54,958 - INFO - joeynmt.training - 	Reference:  But this understates the seriousness of this particular problem because it doesn't show the thickness of the ice.
2023-06-02 23:22:54,958 - INFO - joeynmt.training - 	Hypothesis: And this is the first thing is that that the really is not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not not
2023-06-02 23:22:54,958 - INFO - joeynmt.training - Example #2
2023-06-02 23:22:54,958 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'cal@@', 'ot@@', 'ta', 'gl@@', 'ac@@', 'i@@', 'ale', 'ar@@', 't@@', 'ica', 'è@@', ',', 'in', 'un', 'cer@@', 'to', 'sen@@', 'so,', 'i@@', 'l', 'cu@@', 'ore', 'pul@@', 's@@', 'ante', 'del', 'sistema', 'cli@@', 'mat@@', 'ico', 'glob@@', 'ale.']
2023-06-02 23:22:54,958 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'ar@@', 'c@@', 'ti@@', 'c', 'ice', 'ca@@', 'p', 'is,', 'in', 'a', 'sen@@', 'se,', 'the', 'be@@', 'ating', 'h@@', 'ear@@', 't', 'of', 'the', 'glob@@', 'al', 'c@@', 'lim@@', 'ate', 'system@@', '.']
2023-06-02 23:22:54,958 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'first', 'is', 'a', 're@@', 'ally', 're@@', 'ally', 'is', 'a', 're@@', 'ally', 're@@', 'ally', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'of', 'the', 're@@', 'ally', 'is', 'the', 're@@', 'ally', 'is', 'the', 'b@@', 'u@@', 'e.', '</s>']
2023-06-02 23:22:54,959 - INFO - joeynmt.training - 	Source:     La calotta glaciale artica è, in un certo senso,  il cuore pulsante del sistema climatico globale.
2023-06-02 23:22:54,959 - INFO - joeynmt.training - 	Reference:  The arctic ice cap is, in a sense, the beating heart of the global climate system.
2023-06-02 23:22:54,959 - INFO - joeynmt.training - 	Hypothesis: The first first is a really really is a really really the really of the really of the really is the really is the bue.
2023-06-02 23:22:54,959 - INFO - joeynmt.training - Example #3
2023-06-02 23:22:54,959 - DEBUG - joeynmt.training - 	Tokenized source:     ['S@@', 'i', 'e@@', 's@@', 'p@@', 'and@@', 'e', 'd@@', "'@@", 'in@@', 'ver@@', 'no', 'e', 's@@', 'i', 'rit@@', 'i@@', 'ra', 'd@@', "'@@", 'est@@', 'ate.']
2023-06-02 23:22:54,959 - DEBUG - joeynmt.training - 	Tokenized reference:  ['It', 'ex@@', 'p@@', 'ands', 'in', 'w@@', 'int@@', 'er', 'and', 'con@@', 'tr@@', 'ac@@', 'ts', 'in', 'sum@@', 'mer@@', '.']
2023-06-02 23:22:54,959 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['They', 'have', 'a', 'lot', 'of', 'the', '<unk>', 'i@@', 'ght@@', 'ly', 'of', 'the', '<unk>', 'i@@', 'ght@@', '?', '</s>']
2023-06-02 23:22:54,960 - INFO - joeynmt.training - 	Source:     Si espande d'inverno e si ritira d'estate.
2023-06-02 23:22:54,960 - INFO - joeynmt.training - 	Reference:  It expands in winter and contracts in summer.
2023-06-02 23:22:54,960 - INFO - joeynmt.training - 	Hypothesis: They have a lot of the <unk> ightly of the <unk> ight?
2023-06-02 23:22:54,960 - INFO - joeynmt.training - Example #4
2023-06-02 23:22:54,960 - DEBUG - joeynmt.training - 	Tokenized source:     ['La', 'pro@@', 'ssi@@', 'ma', 'd@@', 'i@@', 'a@@', 'positi@@', 'va', 'sarà', 'un@@', 'a', 'ra@@', 'pi@@', 'da', 'car@@', 're@@', 'll@@', 'ata', 'su@@', 'gli', 'av@@', 'ven@@', 'im@@', 'enti', 'degli', 'ulti@@', 'mi', '2@@', '5', 'anni.']
2023-06-02 23:22:54,960 - DEBUG - joeynmt.training - 	Tokenized reference:  ['The', 'nex@@', 't', 's@@', 'li@@', 'de', 'I', 'show', 'you', 'will', 'be', 'a', 'ra@@', 'pi@@', 'd', 'f@@', 'ast@@', '-@@', 'for@@', 'war@@', 'd', 'of', "what's", 'happen@@', 'ed', 'over', 'the', 'las@@', 't', '2@@', '5', 'year@@', 's.']
2023-06-02 23:22:54,960 - DEBUG - joeynmt.training - 	Tokenized hypothesis: ['The', 'first', 'thing', 'is', 'a', 'lot', 'of', 'the', 'first', 'of', 'the', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other', 'other']
2023-06-02 23:22:54,960 - INFO - joeynmt.training - 	Source:     La prossima diapositiva sarà una rapida  carrellata sugli avvenimenti degli ultimi 25 anni.
2023-06-02 23:22:54,960 - INFO - joeynmt.training - 	Reference:  The next slide I show you will be a rapid fast-forward of what's happened over the last 25 years.
2023-06-02 23:22:54,961 - INFO - joeynmt.training - 	Hypothesis: The first thing is a lot of the first of the other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other other
2023-06-02 23:23:35,308 - INFO - joeynmt.training - Epoch   1, Step:     2100, Batch Loss:     2.657745, Batch Acc: 0.248370, Tokens per Sec:     1814, Lr: 0.000300
2023-06-02 23:23:42,912 - INFO - joeynmt.model - Building an encoder-decoder model...
2023-06-02 23:23:42,988 - INFO - joeynmt.model - Enc-dec model built.
2023-06-02 23:23:43,024 - INFO - joeynmt.helpers - Load model from /Users/songyafeng/6/Machine_Translation/Ex5/mt-exercise-5/models/bpe_level_transformer/2000.ckpt.
2023-06-02 23:23:43,030 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=3100),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=3100),
	loss_function=None)
2023-06-02 23:23:43,031 - INFO - joeynmt.prediction - Decoding on dev set...
2023-06-02 23:23:43,031 - INFO - joeynmt.prediction - Predicting 929 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
